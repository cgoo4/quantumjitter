[
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2023 quantumjitter Carl Goodwin\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Carl Goodwin",
    "section": "",
    "text": "I live in London and post-graduated with an MBA from Hult International Business School as valedictorian in 2001.\nI served for 36 years at Big Blue, performing roles combining business (strategy and growth) leadership with data science and machine learning.\nMy passion for data science and machine learning extends beyond work. The tidyverse and tidymodels make this a real joy to do.\nAdobe Fresco does the same for my little digital artworks  to bring it all to life."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Usedthese\n\n\n2 min\n\n\nHexcited to unveil usedthese: an R package that tells you what you just did in case you can’t recall!\n\n\n\nJan 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNight Mode\n\n\n2 min\n\n\nMaking full use of Quarto and sprucing up an oft-visited 404 page\n\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPainting Tails\n\n\n2 min\n\n\nIf you’re a cat, go find the nearest open pot of paint. But if you’re a data scientist, what to do?\n\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Fresh Lick of Paint\n\n\n5 min\n\n\nStaying in Blogdown and renovating with the Hugo Apéro theme\n\n\n\nApr 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoving House\n\n\n3 min\n\n\nLeaving Wordpress for a quieter life in Blogdown with the Hugo Academic theme\n\n\n\nJul 26, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/tail/index.html",
    "href": "blog/tail/index.html",
    "title": "Painting Tails",
    "section": "",
    "text": "There are techniques for painting a region under a curve. But the experimental ggfx package offers an interesting alternative solution based on the blending modes familiar to users of Photoshop.\n\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(scales)\nlibrary(ggfx)\nlibrary(patchwork)\nlibrary(wesanderson)\nlibrary(clock)\nlibrary(tidyquant)\n\n\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(\"Royal1\"))\n\n\n\n\nThe advantage here is that the tail-painting aesthetic needs no information about the shape of the curve; only the limits on the x-axis.\nThe left plot shows the raw components without blending. The right plot is only retaining the red where there is a layer below.\n\np0 &lt;- tibble(outcome = rnorm(10000, 20, 2)) |&gt;\n  ggplot(aes(outcome)) +\n  scale_y_continuous(labels = label_percent())\n\np1 &lt;- p0 +\n  geom_density(adjust = 2, fill = cols[3]) +\n  annotate(\"rect\",\n    xmin = 15, xmax = 18, ymin = -Inf, ymax = Inf,\n    fill = cols[2]\n  ) + \n  labs(title = \"Without Blending\", y = \"Density\")\n\np2 &lt;- p0 +\n  as_reference(geom_density(adjust = 2, fill = cols[3]), id = \"density\") +\n  with_blend(annotate(\"rect\",\n    xmin = 15, xmax = 18, ymin = -Inf, ymax = Inf,\n    fill = cols[2]\n  ), bg_layer = \"density\", blend_type = \"atop\") + \n  labs(title = \"With Blending\", y = NULL)\n\np1 + p2\n\n\n\n\nOf course the red box could also be layered behind a density curve with alpha applied so it shows through. But if the preference is tail-only colouring, it’s a neat solution.\nBlending is actually a handy solution for any awkward shape. The same technique is used here with a time series ribbon summarising the median, lower and upper quartiles of a set of closing stock prices.\n\n\n\n\n\n\nNote\n\n\n\nTry this patch if having problems with tq_get\nThis chunk is using the development version of dplyr which introduces temporary grouping with .by.\n\n\n\ntickrs &lt;- c(\"AAPL\", \"NFLX\", \"TSLA\", \"ADBE\", \"META\", \"GOOG\", \"MSFT\")\n\np0 &lt;- tq_get(tickrs, get = \"stock.prices\", from = \"2022-01-01\") |&gt;\n  filter(!is.na(close)) |&gt; \n  reframe(\n    close = quantile(close, c(0.25, 0.5, 0.75)),\n    quantile = c(\"lower\", \"median\", \"upper\") |&gt; factor(),\n    .by = date\n  ) |&gt;\n  pivot_wider(names_from = quantile, values_from = close) |&gt;\n  ggplot(aes(date, median)) +\n  annotate(\"text\",\n    x = ymd(\"2022-03-16\"), y = 100,\n    label = \"Helpful\\nAnnotation\", colour = \"black\"\n  ) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = NULL)\n\np1 &lt;- p0 +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = cols[1]) +\n  geom_line(colour = cols[3]) +\n  annotate(\"rect\",\n    xmin = ymd(\"2022-03-01\"), xmax = ymd(\"2022-03-31\"),\n    ymin = -Inf, ymax = Inf, \n    fill = cols[4], colour = \"black\", linetype = \"dashed\"\n  ) + \n  labs(title = \"Without Blending\", y = \"Closing Price\")\n\np2 &lt;- p0 +\n  as_reference(geom_ribbon(aes(ymin = lower, ymax = upper), \n                           fill = cols[1]), id = \"ribbon\") +\n  with_blend(\n    annotate(\n      \"rect\",\n      xmin = ymd(\"2022-03-01\"), xmax = ymd(\"2022-03-31\"),\n      ymin = -Inf, ymax = Inf, \n      fill = cols[4], colour = \"black\", linetype = \"dashed\"\n      ),\n    bg_layer = \"ribbon\", blend_type = \"atop\"\n    ) +\n  geom_line(colour = cols[3]) + \n  labs(title = \"With Blending\", y = NULL)\n\np1 + p2 +\n  plot_annotation(title = \"Median Price Bounded by Upper & Lower Quartiles\")"
  },
  {
    "objectID": "blog/usedthese/index.html",
    "href": "blog/usedthese/index.html",
    "title": "Usedthese",
    "section": "",
    "text": "Who ordered that? You may be wondering."
  },
  {
    "objectID": "blog/usedthese/index.html#motivation",
    "href": "blog/usedthese/index.html#motivation",
    "title": "Usedthese",
    "section": "Motivation",
    "text": "Motivation\nUsedthese was born of a selfish desire to better understand my personal package & function usage. I find it makes it easier for me to check for consistency of usage, acknowledge the packages I most need to keep abreast of (or go deeper on), and spot opportunities for updates to the latest and greatest.\nIt started as a chunk of code that varied from project to project. Evolved into a common piece of code included in all projects. And finally, with the help and guidance of the R Packages (2e) book, has entered the hallowed halls of CRAN."
  },
  {
    "objectID": "blog/usedthese/index.html#example-use-case",
    "href": "blog/usedthese/index.html#example-use-case",
    "title": "Usedthese",
    "section": "Example Use Case",
    "text": "Example Use Case\nEach “little project” on this Quarto site focuses on a data science technique or machine learning model to analyse real-world data. At the foot of each of these is a quantified view of the R packages and functions used in the associated code. This is achieved by including used_here() in each Quarto document.\nused_there() scrapes all the tables created by used_here() to enable a Favourite Things article on how R packages and functions are used across the website.\nWhen the Tidyverse blog announced changes to dplyr and purrr a quick review of my Favourite Things identified a number of opportunities to try out the exciting new features. For example, dplyr introduced temporary grouping with the .by argument for mutate() and amigos. group_by() and ungroup() had been used many times and most of these occurrences could be replaced with the new more concise approach."
  },
  {
    "objectID": "blog/usedthese/index.html#conflicted",
    "href": "blog/usedthese/index.html#conflicted",
    "title": "Usedthese",
    "section": "Conflicted?",
    "text": "Conflicted?\nUsethese is designed to work in conjunction with the conflicted package. Whilst library() supports exclusion and include.only arguments, it can feel a little awkward when dealing with pre-loaded base packages or meta-packages such as the tidyverse, tidymodels and fpp3. Conflicted tells you when you try to use a function whose name is shared by two or more loaded packages. And it offers fine-grained conflict resolution based on your preferences."
  },
  {
    "objectID": "blog/usedthese/index.html#multi-site-usage-analysis",
    "href": "blog/usedthese/index.html#multi-site-usage-analysis",
    "title": "Usedthese",
    "section": "Multi-site Usage Analysis",
    "text": "Multi-site Usage Analysis\nI’m considering adding a function that would enable analysis of usage across multiple opted-in sites. If you do deploy usedthese within your Quarto website and would be willing to have the site included, then please raise an issue with your listing page as the title and select the “usedthese” label. The listing URL should link to one or more posts which include used_here() in the code."
  },
  {
    "objectID": "blog/dark/index.html",
    "href": "blog/dark/index.html",
    "title": "Night Mode",
    "section": "",
    "text": "It was only last April that I renovated my blog as described in A Fresh Lick of Paint. Following the launch of a new open-source scientific and technical publishing system, it’s time to get the paint brush out again.\nI initially started to convert some of my projects and posts from Rmarkdown to qmd using format: hugo-md. The thought was to keep my existing site aesthetic, with its beautiful Hugo Apéro theme, unchanged. I found though that it wasn’t possible to make full use of the myriad features offered by Quarto. For example code-link: true and date-modified: last-modified. This is because the document metadata is preserved as-is for formats like Hugo.\nSo, I decided to start building a “full-on Quarto” version, i.e. format: html, with the intent of switching the website over only if I preferred the new versus the old when running the two side-by-side.\nI’ve switched over.\nI’ve given the site a unique look-and-feel by customising the flatly and darkly themes with a number of SASS variables in two theme.scss files, for example, to adopt my own reversible colour scheme.\nThe landing page switches image based on the dark-mode setting by making the class of the first image .dark-mode and adding these two lines in the dark theme’s custom scss file:\n.dark-mode { display: block; }\n.light-mode { display: none; }\nThen the second image has the class .light-mode and uses the mirror css code in the light theme’s custom scss file:\n.light-mode { display: block; }\n.dark-mode { display: none; }\nFor the navbar logo, my initial idea was to have a mid-grey logo which darkens and lightens by adjusting the brightness based on the mode:\n.navbar-logo {\n    filter: brightness(2);\n    max-height: 30px;\n}\n\n.navbar-logo {\n    filter: brightness(20%);\n    max-height: 30px;\n}\nThis worked nicely for some browsers, e.g. firefox, but not others, e.g. safari; perhaps because the cache is cleared for some but not all. I’ve instead implemented a dark-mode logo using a background image in dark.scss1.\nAs for other features, not only does the site now have the code-link, date-modified, and dark-mode, but it also restores the grid-based listing pages given up when moving from Hugo Academic. Many other Quarto features such as call-outs, citations, footnotes and freeze are also now deployed.\nLua Filters are a powerful tool. The _quarto.yml file includes one to check my current installed version of Quarto and then insert that into the website’s page-footer2.\nWhilst refreshing the site, I took the opportunity to make my 404 page a little more welcoming as it seemed to be one of my more popular pages. Feel free to try it by visiting an imaginatively made-up page of your choice.\nThe updated repo is public on github.\n\n\n\n\nFootnotes\n\n\nAs suggested in Quarto Discussions↩︎\nSuggested solution in Stack Overflow↩︎"
  },
  {
    "objectID": "blog/renovate/index.html#motivation",
    "href": "blog/renovate/index.html#motivation",
    "title": "A Fresh Lick of Paint",
    "section": "Motivation",
    "text": "Motivation\nA couple of years ago I moved house from Wordpress to Blogdown. It’s proved to be a much less stressful life and I plan to stay. Hugo Academic served me well, but sometimes you just need a fresh coat of paint. I liked the look of Hugo Apéro.\nApéro feels simpler and has an elegant design with well-chosen themes and fonts.\nI like to add my own digital art to both the site and Rmarkdown projects, and Apéro gives me more flexibility here. GIF animations, for example, on my home page and in my project and blog lists just work.\nThe dark mode I had with Academic would be a nice-to-have, but not essential."
  },
  {
    "objectID": "blog/renovate/index.html#plan-of-attack",
    "href": "blog/renovate/index.html#plan-of-attack",
    "title": "A Fresh Lick of Paint",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nThe upgrade approach I took was to create a brand new blogdown project in RStudio with the Apéro theme and then copy over and re-knit my projects one by one. This worked well because every project needed at least one change as a direct consequence of the move and re-opening each project also prompted other beneficial updates.\nI focused first on manual deployment, i.e. dragging the Public folder to Netlify, rather than going straight to continuous deployment via Github. Doing it this way would narrow the potential cause of any problems when doing the latter. I also initially deployed to one of Netlify’s auto-generated site names, so my live manually-deployed Academic blog remained unaffected whilst preparing the new site."
  },
  {
    "objectID": "blog/renovate/index.html#set-up",
    "href": "blog/renovate/index.html#set-up",
    "title": "A Fresh Lick of Paint",
    "section": "Set-up",
    "text": "Set-up\nThere’s a very helpful get started authored by the theme owner Alison Hill, so I’ll comment here only on the personal touches I wanted to add.\nIn Hugo Academic, each project’s (or post’s) feature image rendered automatically in both the project list page and in the individual project. In Apéro, I needed to add ![](pathname) to the Rmarkdown file to render the image in an individual project or post. I actually prefer this approach because it means the image then also appears when re-publishing to a blog aggregator which frustratingly was not the case with Academic.\nGiven the taxonomy differences, I created a static/_redirects file so that bookmarks for, say, category/r or tag/statistical-inference (under Academic) would go to categories/r or tags/statistical-inference.\n\n\n/index.xml                          /project/index-R.xml\n/categories/r/index.xml             /project/index-R.xml\n\n\nI had customised my Academic site to show the updated, as well as posted, date for each project and post. So to get the same in Apéro, I copied the themes &gt; hugo-apero &gt; layouts &gt; partials &gt; shared &gt; post-details.html file to layouts &gt; partials &gt; shared &gt; post-details.html, duplicated lines 2-5 below and changed .PublishDate to .Lastmod. As my YAML header for all projects and posts already included lastmod:, the details twistie at the foot of each project (and post) now shows both dates.\n&lt;details {{ .Scratch.Get \"details\" }} class=\"f6 fw7 input-reset\"&gt;\n  &lt;dl class=\"f6 lh-copy\"&gt;\n    &lt;dt class=\"fw7\"&gt;Posted:&lt;/dt&gt;\n    &lt;dd class=\"fw5 ml0\"&gt;{{ .PublishDate.Format \"January 2, 2006\" }}&lt;/dd&gt;\n  &lt;/dl&gt;\n  &lt;dl class=\"f6 lh-copy\"&gt;\n    &lt;dt class=\"fw7\"&gt;Updated:&lt;/dt&gt;\n    &lt;dd class=\"fw5 ml0\"&gt;{{ .Lastmod.Format \"January 2, 2006\" }}&lt;/dd&gt;\n  &lt;/dl&gt;\nI used a tag cloud in Academic and wanted to replicate this too. To do so, I also copied the themes &gt; hugo-apero &gt; layouts &gt; partials &gt; shared &gt; summary-li.html file to layouts &gt; partials &gt; shared and changed the last section to refer to tags rather than categories. I removed most of the other code to simplify the About page, so my customised summary-li.html contained only the code below. This change also required a tweak to the content &gt; about &gt; main &gt; index.md to replace number_categories: with a number_tags: parameter.\n&lt;section class=\"featured-content\"&gt;\n{{ $page := . }} &lt;!--save current page--&gt;\n\n{{ $number_tags := $page.Params.number_tags | default 0 }}\n{{ if ge $number_tags 1 }}\n  &lt;article{{ if .Params.show_outro }} class=\"bb pb5\"{{ end }}&gt;\n  &lt;h5 class=\"f4 mv4 ttu tracked lh-title bt pv3\"&gt;Themes&lt;/h5&gt;\n  {{ range first $number_tags site.Taxonomies.tags.ByCount }}\n      &lt;a class=\"f6 link dim ba ph3 pv2 mb2 dib mr2\" href=\"{{ .Page.RelPermalink }}\"&gt;{{ .Page.Title }} ({{ .Count }})&lt;/a&gt;\n  {{ end }}\n  &lt;/article&gt;\n{{ end }}\n&lt;/section&gt;\nFormspree is removing support for email-based forms, so my contact.md required a randomly-generated formspree_form_id: rather than an email address."
  },
  {
    "objectID": "blog/renovate/index.html#deployment",
    "href": "blog/renovate/index.html#deployment",
    "title": "A Fresh Lick of Paint",
    "section": "Deployment",
    "text": "Deployment\nManual\nInitially a few things did not render correctly, e.g. syntax highlighting, which it turned out required renaming the index.Rmd files to index.Rmarkdown. And when the manual deployment to Netlify got stuck uploading, I realised I also needed to change the .Rprofile to blogdown.method = 'markdown' rather than blogdown.method = 'html'.\nContinuous\nOnce the manual deployment to Netlify was working, I then moved on to continuous deployment via Github. I wanted to switch the commenting engine from Disqus to utterance.es and, as is often the case, wanting one thing results in the need for a bunch of other things; in this case, a public repo on Github. Installing the latter provides a more elegant fit with the Apéro design and has some nice advantages.\nAnd because I wanted to deploy a pre-existing RStudio project to Github, rather than following the usual Github-first practice, I found this guidance helpful.\nI played around a bit with the .gitignore file and found I could exclude quite a lot of stuff that Netlify would not need to do the Hugo build.\nThe Netlify deployment via Github did initially fail with a “Base directory does not exist” message. The fix there was to leave the base directory in Netlify’s build settings blank rather than using the repo URL (which it already had under current repository).\n\nThen finally I could flip my live site over to continuous deployment, pack away my paint pots, paint roller and step ladder, put my feet up in front of a roaring fire and bask in the warmth of my newly-renovated blogdown home.\nPost-deployment there was initially an issue with the RSS feed showing only the summary. Adding a layouts/_default/rss.xml file using the Hugo default with .Summary changed to .Content fixed that."
  },
  {
    "objectID": "blog/plunge/index.html",
    "href": "blog/plunge/index.html",
    "title": "Moving House",
    "section": "",
    "text": "After reading up on Blogdown, I decided to take the plunge and leave Wordpress for a quieter life in Blogdown."
  },
  {
    "objectID": "blog/plunge/index.html#motivation",
    "href": "blog/plunge/index.html#motivation",
    "title": "Moving House",
    "section": "Motivation",
    "text": "Motivation\nMy former site looked pretty good. But it was expensive to maintain.\nI was spending more than I wished to get a performant site. I could have spent less, and perhaps I’m easily seduced by “bells & whistles”, e.g. CloudFlare Plus and “GoGeek” hosting. But a non-speedy site is a bit of a turn-off.\nAnd it wasn’t just cost. It also took a lot of non-R effort to publish a post with Rmarkdown in the way I wanted. My main interest is in writing R code. Not wrestling Wordpress and multiple plugins into submission.\nA reboot was also a chance to re-brand. When I originally set up thinkr.biz I was initially unaware of a similarly-named site in France. Although my personal blog posed no threat across the Channel, and we co-existed for a few years, I anyway prefer having something a little more unique."
  },
  {
    "objectID": "blog/plunge/index.html#why-blogdown",
    "href": "blog/plunge/index.html#why-blogdown",
    "title": "Moving House",
    "section": "Why Blogdown?",
    "text": "Why Blogdown?\nI like Yihui Xie’s Blogdown primarily because it simplifies the path from Rmarkdown to blog. No more WWE-style detour. I can tweak a line of code in Rmarkdown, serve_site, and immediately see the updated blog locally. When I’m ready to publish, I just drag the public folder into Netlify, and voilà it’s live."
  },
  {
    "objectID": "blog/plunge/index.html#my-personal-roadmap",
    "href": "blog/plunge/index.html#my-personal-roadmap",
    "title": "Moving House",
    "section": "My personal roadmap",
    "text": "My personal roadmap\nThere are different routes one can take. Here’s mine.\n\nChoose a theme\nIn his book, Yihui advises asking yourself: “Do I like this fancy theme so much that I will definitely not change it in the next couple of years?” It’s very sound advice. Nonetheless, I’m easily seduced, so explored all possible fancy themes. In part because I like creating my own graphic art, so I wanted something that could help these little creations shine.\nOddly, I started by looking at one of Yihui’s recommended themes and discarded it, only to return to it again much later after an exhaustive exploration of other themes. There are many superficially nice Hugo themes. But when you actually play with them, there’s little below the surface and/or an absence of serious upkeep.\nHugo Academic is not the most appealing in the “shop window”. But when you take it for a test spin, and really take it through its paces, it offers a richness, flexibility and investment that reeled me in. After customising it to my taste, and paring back optional bits I do not need, simply by switching them off, it gave me something I feel very happy with.\n\n\nMigrate\nThere are assisted migration paths, e.g. from Wordpress, discussed in the Blogdown book. However I wanted to review and upgrade the R code in my original posts (only a dozen or so at the time). So, one-by-one, I copied each Rmd file into the projects folder of my new site, tweaked the code, and used serve_site to see the end product.\nI took this approach because R, especially the tidyverse and its ecosystem, is rapidly evolving. For example, the latest release of dplyr has some great new column-wise and row-wise functions. And spread and gather have been superseded by the more capable pivot_wider and pivot_longer. So it was a chance to upgrade my code.\nFor one or two of the more processing-intensive projects, I used either cache = TRUE in the code chunk, or saveRDS and readRDS to load data prepared earlier.\n\n\nBuild\nAn option I haven’t yet pursued is to host all my website source files in a GIT repository. Then Netlify could call Hugo to render my website automatically. Right now, my site content is simple enough to be able to use the Build Website button in RStudio.\nThe web-site is a static build, so it’s fast out-of-the-box, i.e. no need for speed-inducing wallet-slimming plugins.\n\n\nDeploy\nNetlify is recommended by bookdown.org. And it’s free for personal projects. The only small annual cost is my domain name.\nChoosing a domain name, which one can do via Netflify, is a little tricky. Many of the ideas one might have, have already occurred to someone else. And when you do find something available, there’s always that niggling feeling there may be something better out there.\nI chose Quantum Jitter for several reasons:\n\nI often use ggplot2’s geom_jitter\nLike a Quant, I have an interest in using machine learning to assess stock fundamentals\nLike the quantum world, my work features statistics and randomness\nIt was available\n\nAfter running the build in RStudio, which for my site only takes a few minutes, I can simply drag my newly-created public folder into Netlify’s Deploys page and bingo, the site’s live in a jiffy.\nSo, if you are toying with the idea of moving house, I can recommend a quieter life in Blogdown."
  },
  {
    "objectID": "project/forecast/index.html",
    "href": "project/forecast/index.html",
    "title": "Can Ravens Forecast?",
    "section": "",
    "text": "Humans have the magical ability to plan for future events, for future gain. It’s not quite a uniquely human trait. Because apparently ravens can match a four-year-old.\nAn abundance of data, and some very nice R packages, make our ability to plan all the more powerful.\nIn the Spring of 2018 I looked at sales from an historical perspective in Six Months Later.. Here I’ll use the data to model a time-series forecast for the year ahead. The techniques apply to any time series with characteristics of trend, seasonality or longer-term cycles.\nWhy forecast sales? Business plans require a budget, e.g. for resources, marketing and office space. A good projection of revenue provides the foundation for the budget. And, for an established business, with historical data, time-series forecasting is one way to deliver a robust projection.\nWithout exogenous data, the forecast assumes one continues to do what one’s doing. So, it provides a good starting-point. Then one might, for example, add assumptions about new products or services. And, if there is forward-looking data available, for example, market size projections (with past projections to train the model), then one could feed this into the forecast modelling too.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(wesanderson)\nlibrary(fpp3)\nlibrary(scales)\nlibrary(clock)\nlibrary(usedthese)\n\nconflict_scout()\n\n7 conflicts:\n* `as_date`    : clock, lubridate\n* `col_factor` : scales, readr\n* `date_format`: clock, scales\n* `discard`    : scales, purrr\n* `filter`     : [dplyr]\n* `interval`   : tsibble, lubridate\n* `lag`        : [dplyr]\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(name = \"IsleofDogs2\"))\nFirst I’ll check the encoding of the data.\nurl &lt;-\n  \"https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/\"\n\ngcloud_csv &lt;- str_c(url, \"703943/G-Cloud_spend_data_to_end_March_2018.csv\")\n\ndos_csv &lt;- str_c(url, \"703952/DOS_spend_data_to_end_March_2018.csv\")\n\nnames &lt;- c(gcloud_csv, dos_csv)\n\n# Use walk to suppress the printing of list element numbers\n\nwalk(names, \\(x) {\n  p &lt;- guess_encoding(x)\n  print(p)\n})\n\n# A tibble: 2 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.4 \n2 ISO-8859-2       0.22\n# A tibble: 2 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.36\n2 ISO-8859-2       0.24\nNext I’ll set up a vector of column names to apply consistently to both files, and import the data with the suggested encoding.\ncolnam &lt;- \n  c(\"sector\",\n    \"lot\",\n    \"date\",\n    \"spend\",\n    \"status\",\n    \"supplier\",\n    \"customer\",\n    \"framework\")\n\nread_dm &lt;- \\(x){\n  read_csv(\n    x,\n    col_names = colnam,\n    skip = 1,\n    locale = locale(encoding = \"ISO-8859-1\"),\n    show_col_types = FALSE)\n}\n\nraw &lt;- map(names, read_dm) |&gt; \n  set_names(c(\"gcloud\", \"dos\")) |&gt; \n  bind_rows() |&gt; \n  mutate(framework = if_else(is.na(framework), \"DOS\", framework))\nI’d like to create some new features: Month-end dates, something to distinguish between the two frameworks (G-Cloud or DOS). The spend has a messy format and needs a bit of cleaning too.\nThe lot structure for G-Cloud has evolved over time, but fortunately, there is a simple mapping, i.e. PaaS and IaaS became Cloud Hosting, SaaS became Cloud Software, and Specialist Cloud Services became Cloud Support, so I’ll standardise on the latter.\nboth &lt;- raw |&gt;\n  mutate(\n    month_end = date_parse(str_c(date, \"01\", sep = \"-\"), \n                           format = \"%b-%y-%d\") |&gt; \n      add_months(1) |&gt; add_days(-1),\n    date = yearmonth(month_end),\n    framework = str_extract(framework, \".{3,7}\"),\n    spend = str_remove(spend, coll(\"£\")),\n    spend = str_replace(spend, \"^\\\\(\", \"-\"),\n    spend = parse_number(spend) / 1000000,\n    lot = recode(\n      lot,\n      \"Software as a Service (SaaS)\" = \"Cloud Software\",\n      \"Infrastructure as a Service (IaaS)\" = \"Cloud Hosting\",\n      \"Platform as a Service (PaaS)\" = \"Cloud Hosting\",\n      \"Specialist Cloud Services\" = \"Cloud Support\"\n      )\n)\nThe tidied data now needs to be converted to a tsibble(Wang, Cook, and Hyndman 2020), the temporal equivalent of a tibble(Müller and Wickham 2022).\nR has evolved since I first wrote this post. At that time, it was necessary to either split the data into the two frameworks (G-Cloud and DOS) and forecast them separately. Or, as I did with the three G-Cloud lots, use the purrr package to iterate through a forecast.\nThe tsibble package combined with the newer fable(O’Hara-Wild, Hyndman, and Wang 2022a) and feasts(O’Hara-Wild, Hyndman, and Wang 2022b) packages, make this easier. One of the defining feature of the tsibble is the key. I want a model for each framework, so I’m setting this as the tsibble key (and the temporal variable as the tsibble index).\nboth_ts &lt;- both |&gt;\n  summarise(spend = sum(spend), .by = c(date, framework)) |&gt; \n  as_tsibble(key = framework, index = date)\n\nboth_ts |&gt; \n  ggplot(aes(date, spend, colour = framework)) +\n  geom_line(key_glyph = \"timeseries\") +\n  scale_y_continuous(labels = label_dollar(prefix = \"£\", suffix = \"m\")) +\n  scale_colour_manual(values = cols[c(3, 4)]) +\n  labs(x = NULL, y = NULL, title = \"Monthly Digital Marketplace Sales\")\nBy decomposing the historical data we can tease out the underlying trend and seasonality:\nboth_ts |&gt;\n  model(stl = STL(spend ~ trend(window = 7) + season(window = \"periodic\"))) |&gt;\n  components() |&gt;\n  autoplot() +\n  scale_colour_manual(values = cols[c(3, 4)]) +\n  labs(x = NULL, title = \"Time Series Decomposition\")\nI’ll use auto.arima: AutoRegressive Integrated Moving Average modelling which aims to describe the autocorrelations in the data.\nBy setting stepwise and approximation to FALSE, auto.arima will explore a wider range of potential models.\nI’ll forecast with the default 80% and 95% prediction intervals. This means the darker-shaded 80% range should include the future sales value with an 80% probability. Likewise with a 95% probability when adding the wider and lighter-shaded area.\nUse of autoplot would simplify the code, but personally I like to expose all the data, for example unpacking the prediction intervals, and have finer control over the visualisation.\nmod_ts &lt;- both_ts |&gt;\n  model(ARIMA = ARIMA(spend, stepwise = TRUE, approximation = FALSE))\n\nmod_ts |&gt; \n  glance() |&gt;\n  select(-ar_roots, -ma_roots)\n\n\n\n\nframework\n.model\nsigma2\nlog_lik\nAIC\nAICc\nBIC\n\n\n\nDOS\nARIMA\n23.60154\n-59.93305\n123.8661\n124.5720\n125.8576\n\n\nG-Cloud\nARIMA\n34.75275\n-191.61547\n393.2309\n394.3421\n403.7027\n\n\n\n\n\nmod_ts |&gt; \n  tidy()\n\n\n\n\n\n\n\n\n\n\n\n\n\nframework\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\nDOS\nARIMA\nma1\n-0.7725018\n0.1718541\n-4.495102\n0.0002213\n\n\nG-Cloud\nARIMA\nar1\n0.9390150\n0.0595354\n15.772391\n0.0000000\n\n\nG-Cloud\nARIMA\nma1\n-0.5777210\n0.1094066\n-5.280494\n0.0000019\n\n\nG-Cloud\nARIMA\nsar1\n-0.5124417\n0.1142670\n-4.484597\n0.0000336\n\n\nG-Cloud\nARIMA\nconstant\n1.4084703\n0.3168155\n4.445712\n0.0000385\n\n\n\n\n\nfcast_ts &lt;- mod_ts |&gt;\n  forecast(h = \"2 years\") |&gt; \n  mutate(`95%` = hilo(spend, 95), `80%` = hilo(spend, 80)) |&gt; \n  unpack_hilo(c(\"95%\", \"80%\")) |&gt;\n  rename(fc_spend = spend) |&gt; \n  bind_rows(both_ts)\n\nfcast_ts |&gt;\n  ggplot(aes(date, fill = framework)) +\n  geom_line(aes(y = spend), colour = cols[5]) +\n  geom_ribbon(aes(ymin = `95%_lower`, ymax = `95%_upper`),\n    fill = cols[1], colour = NA\n  ) +\n  geom_ribbon(aes(ymin = `80%_lower`, ymax = `80%_upper`),\n    fill = cols[2], colour = NA\n  ) +\n  geom_line(aes(y = .mean), colour = \"white\") +\n  scale_y_continuous(labels = label_dollar(prefix = \"£\", suffix = \"m\")) +\n  facet_wrap(~framework) +\n  labs(\n    title = \"Digital Marketplace Sales Forecast by Framework\",\n    x = NULL, y = \"Spend\",\n    subtitle = \"80 & 95% Prediction Intervals\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\nThe G-Cloud framework compromises three lots: Cloud Hosting, Cloud Software and Cloud Support.\nI previously combined auto.arima (from the forecast package) with functions from the sweep package, to create multiple forecasts in one shot. tsibble coupled fabletools handle this with the key set to the lot variable.\nAn alternative option is hierarchical time-series forecasting which models bottom-up, top-down or middle-out, and ensures the sum of the forecasts at the lower level sum to the top-level forecast. This approach has pros and cons and is not considered here.\ngcloud_ts &lt;- both |&gt;\n  filter(framework == \"G-Cloud\") |&gt; \n  summarise(spend = sum(spend), .by = c(date, lot)) |&gt; \n  as_tsibble(key = lot, index = date)\n\ngc_ts &lt;- gcloud_ts |&gt;\n  model(ARIMA = ARIMA(spend, stepwise = TRUE, approximation = FALSE))\n\ngc_ts |&gt; \n  glance() |&gt;\n  select(-ar_roots, -ma_roots)\n\n\n\n\n\n\n\n\n\n\n\n\n\nlot\n.model\nsigma2\nlog_lik\nAIC\nAICc\nBIC\n\n\n\nCloud Hosting\nARIMA\n1.347179\n-109.3173\n224.6346\n224.9982\n231.3801\n\n\nCloud Software\nARIMA\n2.275664\n-107.5551\n221.1102\n221.5465\n227.3428\n\n\nCloud Support\nARIMA\n18.606529\n-212.6556\n435.3112\n436.2342\n446.6246\n\n\n\n\n\ngc_ts |&gt; tidy()\n\n\n\n\n\n\n\n\n\n\n\n\n\nlot\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\nCloud Hosting\nARIMA\nma1\n-0.8269314\n0.0765747\n-10.799011\n0.0000000\n\n\nCloud Hosting\nARIMA\nconstant\n0.1713346\n0.0258894\n6.617935\n0.0000000\n\n\nCloud Software\nARIMA\nma1\n-0.9981867\n0.1304426\n-7.652304\n0.0000000\n\n\nCloud Software\nARIMA\nma2\n0.2242994\n0.1224834\n1.831264\n0.0721116\n\n\nCloud Support\nARIMA\nma1\n-0.7044948\n0.0743797\n-9.471597\n0.0000000\n\n\nCloud Support\nARIMA\nsma1\n0.3878660\n0.1458579\n2.659205\n0.0096735\n\n\nCloud Support\nARIMA\nsma2\n0.7866476\n0.4284425\n1.836064\n0.0705352\n\n\nCloud Support\nARIMA\nconstant\n0.7601155\n0.2889763\n2.630373\n0.0104520\n\n\n\n\n\nfcgc_ts &lt;- gc_ts |&gt;\n  forecast(h = \"2 years\") |&gt; \n  mutate(`95%` = hilo(spend, 95), `80%` = hilo(spend, 80)) |&gt; \n  unpack_hilo(c(\"95%\", \"80%\")) |&gt; \n  rename(fc_spend = spend) |&gt; \n  bind_rows(gcloud_ts)\n\nfcgc_ts |&gt;\n  ggplot(aes(date, fill = lot)) +\n  geom_line(aes(y = spend), colour = cols[5]) +\n  geom_ribbon(aes(ymin = `95%_lower`, ymax = `95%_upper`),\n    fill = cols[1], colour = NA\n  ) +\n  geom_ribbon(aes(ymin = `80%_lower`, ymax = `80%_upper`),\n    fill = cols[2], colour = NA\n  ) +\n  geom_line(aes(y = .mean), colour = \"white\") +\n  scale_y_continuous(labels = label_dollar(prefix = \"£\", suffix = \"m\")) +\n  facet_wrap(~lot) +\n  labs(\n    title = \"G-Cloud Sales Forecast by Lot\",\n    x = NULL, y = \"Spend\",\n    subtitle = \"80 & 95% Prediction Intervals\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\nSo ravens are not yet ready for forecasting with R. But then neither are 4-year-olds, are they?"
  },
  {
    "objectID": "project/forecast/index.html#r-toolbox",
    "href": "project/forecast/index.html#r-toolbox",
    "title": "Can Ravens Forecast?",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[9], is.na[1], library[7], print[1], sum[2]\n\n\nclock\nadd_days[1], add_months[1], date_parse[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nbind_rows[3], filter[1], if_else[1], mutate[4], recode[1], rename[2], select[2], summarise[2]\n\n\nfable\nARIMA[2]\n\n\nfabletools\ncomponents[1], forecast[2], glance[2], hilo[4], model[3], tidy[2], unpack_hilo[2]\n\n\nfeasts\nSTL[1]\n\n\nggplot2\naes[11], autoplot[1], element_text[2], facet_wrap[2], geom_line[5], geom_ribbon[4], ggplot[3], labs[4], scale_colour_manual[2], scale_y_continuous[3], theme[2], theme_bw[1], theme_set[1]\n\n\npurrr\nmap[1], walk[1]\n\n\nreadr\nguess_encoding[1], locale[1], parse_number[1], read_csv[1]\n\n\nrlang\nset_names[1]\n\n\nscales\nlabel_dollar[3]\n\n\nstringr\ncoll[1], str_c[3], str_extract[1], str_remove[1], str_replace[1]\n\n\ntsibble\nas_tsibble[2], yearmonth[1]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/wood/index.html",
    "href": "project/wood/index.html",
    "title": "Seeing the Wood for the Trees",
    "section": "",
    "text": "In Criminal Goings-on faceting offered a way to get a sense of the data. This is a great visualisation tool building on the principle of small multiples. There may come a point though where the sheer volume of small multiples make it harder to “see the wood for the trees”. What’s an alternative strategy?\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nlibrary(trelliscopejs)\nlibrary(rbokeh)\nlibrary(janitor)\nlibrary(vangogh)\nlibrary(usedthese)\n\nconflict_scout()\nThis time I’ll use Van Gogh’s “The Starry Night” palette for the feature image and plots. And there are 9 types of criminal offence, so colorRampPalette will enable the interpolation of an extended set.\ntheme_set(theme_bw())\n\n(cols &lt;- vangogh_palette(\"StarryNight\"))\n\n\n\ncols9 &lt;- colorRampPalette(cols)(9)\nThe data need a little tidy-up.\nurl &lt;- str_c(\n  \"https://data.london.gov.uk/\",\n  \"download/recorded_crime_rates/\",\n  \"c051c7ec-c3ad-4534-bbfe-6bdfee2ef6bb/\",\n  \"crime%20rates.csv\"\n)\n\ncrime_df &lt;-\n  read_csv(url, col_types = \"cfcfdn\") |&gt;\n  clean_names() |&gt;\n  mutate(\n    year = str_extract(year, \"(?:1999|200[0-9]|201[0-7])\"),\n    year = as.numeric(year)\n  ) |&gt;\n  summarise(number_of_offences = sum(number_of_offences),\n            .by = c(year, borough, offences)) |&gt;\n  filter(\n    offences != \"All recorded offences\",\n    !borough %in% c(\n      \"England and Wales\",\n      \"Met Police Area\",\n      \"Inner London\",\n      \"Outer London\"\n    )\n  )\nThis was the original visualisation in Criminal Goings-on using ggplot’s facet_wrap.\ncrime_df |&gt;\n  mutate(borough = str_wrap(borough, 11)) |&gt;\n  ggplot(aes(year, number_of_offences, colour = offences, group = offences)) +\n  geom_line() +\n  facet_wrap(~borough, scales = \"free_y\", ncol = 4) +\n  labs(\n    x = NULL, y = NULL, title = \"London Crime by Borough\",\n    colour = \"Offence\", caption = \"Source: data.gov.uk\"\n  ) +\n  scale_colour_manual(values = cols9) +\n  guides(colour = guide_legend(nrow = 3)) +\n  theme(\n    strip.background = element_rect(fill = cols[4]),\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\nThere are some nice alternatives which allow one to go deeper into the data whilst making the whole experience more consumable and engaging.\nSwitching facet_wrap for facet_trelliscope is a simple option. Or trelliscope(Hafen and Schloerke 2021) may be used in combination with the rbokeh (Hafen 2021) (or plotly) packages. Irrespective of the option chosen, one can more flexibly display the several hundred “small multiple” panels required to go deeper into the crime data.\nPairing trelliscope with rbokeh permits the addition of some custom cognostics and additional interactivity. The slope cognostic, for example, enables filtering on the boroughs and types of offence exhibiting the steepest upward or downward trends.\nslope &lt;- \\(x, y) coef(lm(y ~ x))[2]\n\nplot_data &lt;- crime_df |&gt;\n  nest(.by = c(borough, offences)) |&gt;\n  mutate(\n    additional_cogs = map_cog(\n      data,\n      ~ tibble(\n        slope = cog(slope(.x$year, .x$number_of_offences),\n          desc = \"Steepness of the trend\"\n        ) |&gt;\n          round(2),\n        mean_count = cog(mean(.x$number_of_offences),\n          desc = \"Average count\"\n        ),\n        iqr_count = cog(IQR(.x$number_of_offences),\n          desc = \"Interquartile range\"\n        )\n      )\n    ),\n    panel = map_plot(\n      data,\n      ~ figure(xlab = \"Date\", ylab = \"Count\") |&gt;\n        ly_lines(year, number_of_offences, color = cols[5], \n                 width = 2, data = .x) |&gt;\n        ly_points(year, number_of_offences,\n          size = 10,\n          fill_color = cols[9],\n          hover = number_of_offences, data = .x\n        ) |&gt;\n        theme_plot(\n          background_fill_color = cols[2],\n          background_fill_alpha = 0.5\n        )\n    )\n  )\nplot_data |&gt;\n  trelliscope(\n    name = \"London Crime\",\n    desc = \"Source: data.gov.uk\",\n    nrow = 2,\n    ncol = 3,\n    state = list(\n      sort = list(sort_spec(\"slope\", dir = \"desc\")),\n      labels = c(\"borough\", \"offences\", \"slope\")\n    ),\n    path = \"appfiles\"\n  )"
  },
  {
    "objectID": "project/wood/index.html#r-toolbox",
    "href": "project/wood/index.html#r-toolbox",
    "title": "Seeing the Wood for the Trees",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.numeric[1], c[4], library[7], list[2], mean[1], round[1], sum[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nfilter[1], mutate[3], summarise[1]\n\n\nggplot2\naes[1], element_rect[1], element_text[1], facet_wrap[1], geom_line[1], ggplot[1], guide_legend[1], guides[1], labs[1], scale_colour_manual[1], theme[1], theme_bw[1], theme_set[1]\n\n\ngrDevices\ncolorRampPalette[1]\n\n\njanitor\nclean_names[1]\n\n\nrbokeh\nfigure[1], ly_lines[1], ly_points[1], theme_plot[1]\n\n\nreadr\nread_csv[1]\n\n\nstats\nIQR[1], coef[1], lm[1]\n\n\nstringr\nstr_c[1], str_extract[1], str_wrap[1]\n\n\ntibble\ntibble[1]\n\n\ntidyr\nnest[1]\n\n\ntrelliscopejs\ncog[3], map_cog[1], map_plot[1], sort_spec[1], trelliscope[1]\n\n\nusedthese\nused_here[1]\n\n\nvangogh\nvangogh_palette[1]"
  },
  {
    "objectID": "project/hansard/index.html",
    "href": "project/hansard/index.html",
    "title": "Cluster of Six",
    "section": "",
    "text": "Before each vote, the Speaker of the House yells “Division! Clear the Lobby”. I’d like to find which cluster of MPs (Members of Parliament) may be exiting the lobby and going their own way.\nHansard reports what’s said in the UK Parliament, sets out details of divisions, and records decisions taken during a sitting. The R package hansard package (Odell 2017) provides access to the data.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nlibrary(clock)\nlibrary(wesanderson)\nlibrary(hansard)\nlibrary(dendextend)\nlibrary(corrplot)\nlibrary(broom)\nlibrary(factoextra)\nlibrary(glue)\nlibrary(ggrepel)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(name = \"Moonrise2\"))\nI’ll start by building a list of all Labour Party MPs.\nurl_prefix &lt;- \"http://data.parliament.uk/members/\"\n\nmps &lt;- commons_members() |&gt;\n  filter(party_value == \"Labour\" | about == str_c(url_prefix, \"478\")) |&gt;\n  mutate(ID = str_replace(about, url_prefix, \"\"))\n\nsaveRDS(mps, file = \"mps.rds\")\nCreating a function will enable me to iterate through the MP list to extract their voting records.\nstart_date &lt;- \"2017-06-08\"\nend_date &lt;- \"2018-01-28\"\n\npull_votes &lt;- \\(x) {\n  mp_vote_record(x,\n    start_date = start_date,\n    end_date = end_date,\n    verbose = FALSE\n  ) |&gt;\n    mutate(mp = x)\n}\nI’ll use it to extract the “aye” and “no” votes. Use of possibly prevents the code from stopping when it encounters former MPs for whom no data is returned.\nvotes &lt;-\n  map(mps$ID, possibly(pull_votes, NULL)) |&gt;\n  compact() |&gt;\n  map(simplify, \"tibbles\") |&gt;\n  list_rbind() |&gt; \n  rename(\"lobby\" = \"vote\")\n\nsaveRDS(votes, file = \"votes.rds\")\nVoting the opposite way to the majority of the party, as well as non-votes, will both be of interest when assessing which MPs are “most distant” from the wider party.\nvotes_df &lt;- votes |&gt;\n  left_join(mps, by = join_by(mp == ID)) |&gt;\n  select(about = about.x, title, date_value, \n         lobby, mp, name = full_name_value) |&gt;\n  transmute(\n    vote = if_else(lobby == \"aye\", 1, -1),\n    mp = str_c(name, \" (\", mp, \")\"),\n    about = str_replace(about, \"http://data.parliament.uk/resources/\", \"\"),\n    title = str_c(title, \" (\", about, \")\")\n  ) |&gt; \n  select(-about) |&gt; \n  pivot_wider(names_from = title, values_from = vote, values_fill = 0)\nThe data are standardised (i.e. scaled) to ensure comparability. This is verified by ensuring the mean and standard deviation are close to zero and one respectively.\nscaled_df &lt;-\n  votes_df |&gt;\n  mutate(across(-mp, scale))\n\nscaled_df |&gt;\n  summarise(across(-mp, list(mean = mean, sd = sd))) |&gt; \n  summarise(\n    sd_min = min(pick(ends_with(\"_sd\"))),\n    sd_max = max(pick(ends_with(\"_sd\"))),\n    mean_min = min(pick(ends_with(\"_mean\"))) |&gt; round(1),\n    mean_max = max(pick(ends_with(\"_mean\"))) |&gt; round(1)\n  )\n\n\n\n\nsd_min\nsd_max\nmean_min\nmean_max\n\n\n1\n1\n0\n0\nI’d like to assess whether the data contain meaningful clusters rather than random noise. This is achieved quantitatively by calculating the Hopkins statistic, and visually by inspection.\nIf the Hopkins statistic is closer to 1 than 0, then we have data which may be clustered.\nscaled_df |&gt;\n  select(-mp) |&gt;\n  get_clust_tendency(nrow(votes_df) - 1) |&gt;\n  pluck(\"hopkins_stat\")\n\n[1] 0.783357\nA visual assessment of clustering tendency reveals distance data exhibiting a visible structure.\nscaled_df |&gt;\n  select(-mp) |&gt;\n  dist() |&gt;\n  fviz_dist(\n    show_labels = FALSE,\n    gradient = list(\n      low = cols[1],\n      mid = cols[3],\n      high = cols[4]\n    )\n  )\nThere are eight methods I could use for hierarchical clustering, and I’ll need to determine which will yield results that best fit the data.\nThe correlation plot below shows that the median and ward methods have a weaker correlation with the other five methods.\norig_dist &lt;- scaled_df |&gt;\n  select(-mp) |&gt;\n  dist()\n\ndend_meths &lt;-\n  c(\n    \"complete\",\n    \"average\",\n    \"single\",\n    \"ward.D\",\n    \"ward.D2\",\n    \"mcquitty\",\n    \"median\",\n    \"centroid\"\n  )\n\ndend_list &lt;-\n  map(dend_meths, \\(x) {\n    orig_dist |&gt;\n      hclust(x) |&gt;\n      as.dendrogram()\n  })\n\ndend_list |&gt;\n  reduce(dendlist) |&gt;\n  set_names(dend_meths) |&gt;\n  cor.dendlist() |&gt;\n  corrplot(\n    \"pie\",\n    \"lower\",\n    col = cols[1],\n    mar = c(1, 0.5, 4, 0.5),\n    order = \"AOE\",\n    tl.cex = 0.8,\n    tl.col = \"black\",\n    cl.cex = 0.7\n  )\nThe above plot does not tell us which method is optimal. For that, I’ll take each of the cluster agglomeration methods and calculate their cophenetic distances. I can then correlate these with the original distance to see which offers the best fit.\nmethods &lt;- list(\n  \"complete\",\n  \"average\",\n  \"single\",\n  \"ward.D\",\n  \"ward.D2\",\n  \"mcquitty\",\n  \"median\",\n  \"centroid\"\n)\n\nbest_method &lt;- map(methods, \\(x) {\n  co_comp &lt;-\n    orig_dist |&gt;\n    hclust(x) |&gt;\n    cophenetic()\n  tibble(\n    correlation = cor(orig_dist, co_comp),\n    method = x\n  )\n}) |&gt; \n  list_rbind()\nThe plot below confirms the ward and median methods having a weaker fit. Average produces the strongest correlation coefficient of 0.98.\nbest_method |&gt;\n  ggplot(aes(reorder(method, correlation), correlation)) +\n  geom_col(fill = cols[1], width = 0.8) +\n  geom_text(aes(label = str_c(method, \"  \", round(correlation, 2))),\n    hjust = 1.3, colour = \"white\"\n  ) +\n  coord_flip() +\n  labs(\n    x = \"Method\", y = \"Correlation\",\n    title = \"Cluster Method Correlation Coefficients\",\n    caption = \"Source: Hansard\"\n  )\nI can now plot the full Labour Party dendrogram using the average method. This shows a “cluster of six” MPs which is the last to merge with the rest of the party based on their voting pattern.\ndend_avg &lt;- orig_dist |&gt;\n  hclust(\"average\") |&gt;\n  as.dendrogram()\n\nlabels(dend_avg) &lt;- scaled_df$mp[order.dendrogram(dend_avg)]\n\ndend &lt;- dend_avg |&gt;\n  color_branches(k = 2, col = cols[4]) |&gt;\n  set(\"labels_cex\", 0.4)\n\nstart_formatted &lt;- date_parse(start_date, format = \"%Y-%m-%d\") |&gt; \n  date_format(format = \"%b %d, %Y\")\n\nend_formatted &lt;- date_parse(end_date, format = \"%Y-%m-%d\") |&gt; \n  date_format(format = \"%b %d, %Y\")\n\nggplot(rev(dend), horiz = TRUE, offset_labels = -0.2) +\n  labs(\n    y = \"\\nDistance\", title = \"Hierarchical Clustering of Labour MPs\",\n    subtitle = \"Based on House of Commons Divisions Since the 2017 Election\",\n    caption = glue(\n      \"Source: Hansard ({start_formatted} to {end_formatted})\")\n  ) +\n  theme(panel.border = element_blank())\nI’ll zoom in on the “cluster of six”.\ndend_cuts &lt;- dend |&gt;\n  assign_values_to_leaves_nodePar(19, \"pch\") |&gt;\n  assign_values_to_leaves_nodePar(5, \"cex\") |&gt;\n  assign_values_to_leaves_nodePar(cols[1], \"col\") |&gt;\n  set(\"labels_cex\", 0.4) |&gt;\n  set(\"branches_lwd\", 2.5) |&gt;\n  color_branches(k = 2, col = cols[1]) |&gt;\n  cut(h = 50)\n\nggplot(rev(dend_cuts$lower[[1]]),\n  horiz = TRUE,\n  nodePar = nodePar,\n  offset_labels = -0.5\n) +\n  labs(\n    title = \"Cluster of Six\",\n    subtitle = \"MPs who Branch off First in the Dendrogram\"\n  ) +\n  theme_void() +\n  theme(plot.margin = unit(c(1, 1, 1, 1), \"cm\"))\nSummarising and sorting the total votes by MP tells me that the “cluster of six” MPs are among the eight MPs voting the fewest times. And I can, for example, verify the record for Emma Reynolds directly via Hansard.\nfewest_votes &lt;- votes |&gt;\n  left_join(mps, by = join_by(mp == ID)) |&gt;\n  summarise(n_lobby = n(), .by = c(full_name_value, lobby)) |&gt;\n  rename(mp = full_name_value) |&gt; \n  pivot_wider(names_from = \"lobby\", values_from = \"n_lobby\") |&gt;\n  mutate(total = aye + no,\n         mp = fct_reorder(mp, total)) |&gt;\n  slice_min(n = 10, order_by = total) |&gt;\n  pivot_longer(cols = -mp) |&gt;\n  filter(name != \"total\")\n\nfewest_votes |&gt;\n  ggplot(aes(mp, value, fill = name)) +\n  geom_col() +\n  geom_label(aes(label = value), position = position_stack()) +\n  scale_fill_manual(values = cols[c(1, 3)]) +\n  coord_flip() +\n  labs(title = \"Labour MPs Voting Fewest Times\",\n       y = \"Votes\", x = NULL, fill = NULL)\nNon-voting will not be the only influencing factor. The “distant cluster” will be particularly influenced by a small minority of MPs voting in the opposite direction to the overwhelming majority.\nCook’s Distance visualises these influential outliers. This shows the voting of three MPs, all on the European Union Withdrawal Bill readings, to be particular outliers. All three MPs are in the “cluster of six”.\ntidy_df &lt;- votes_df |&gt;\n  pivot_longer(cols = -mp, names_to = \"title\", values_to = \"vote\")\n\nmod &lt;- lm(vote ~ ., data = tidy_df)\n\nmod_df &lt;- mod |&gt;\n  augment() |&gt;\n  as_tibble()\n\nggplot(mod_df, aes(title, .cooksd, colour = mp)) +\n  geom_jitter() +\n  geom_label_repel(aes(label = if_else(.cooksd &gt; 0.002, mp, NA)), size = 4) +\n  scale_colour_manual(values = wes_palette(220, name = \"Moonrise2\", type = \"continuous\")) +\n  labs(title = \"Cook's Distance\") +\n  coord_flip() +\n  theme(\n    panel.border = element_blank(),\n    axis.text = element_text(size = 6),\n    legend.position = \"none\"\n  )\nmod_df |&gt;\n  filter(str_detect(title, \"759161|824379|809989\")) |&gt;\n  mutate(title = str_wrap(title, 30)) |&gt; \n  ggplot(aes(title, .cooksd, colour = mp)) +\n  geom_point(size = 4) +\n  geom_label_repel(aes(label = if_else(.cooksd &gt; 0.0015, mp, NA)), size = 4) +\n  ggtitle(\"Cook's Distance\") +\n  theme(\n    axis.line.x = element_line(color = \"grey60\"),\n    axis.text = element_text(size = 8),\n    legend.position = \"none\",\n    axis.title = element_blank()\n  ) +\n  scale_colour_manual(values = wes_palette(\n    210, name = \"Moonrise2\", type = \"continuous\")) +\n  coord_flip()"
  },
  {
    "objectID": "project/hansard/index.html#r-toolbox",
    "href": "project/hansard/index.html#r-toolbox",
    "title": "Cluster of Six",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[5], cut[1], labels[1], library[12], list[3], max[2], min[2], nrow[1], readRDS[2], rev[2], round[3], saveRDS[2]\n\n\nclock\ndate_format[2], date_parse[2]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ncorrplot\ncorrplot[1]\n\n\ndendextend\nassign_values_to_leaves_nodePar[3], color_branches[2], cor.dendlist[1], set[3]\n\n\ndplyr\nacross[2], filter[3], if_else[3], join_by[2], left_join[2], mutate[5], n[1], pick[4], rename[2], select[5], slice_min[1], summarise[3], transmute[1]\n\n\nfactoextra\nfviz_dist[1], get_clust_tendency[1]\n\n\nforcats\nfct_reorder[1]\n\n\ngenerics\naugment[1]\n\n\nggplot2\naes[8], coord_flip[4], element_blank[3], element_line[1], element_text[2], geom_col[2], geom_jitter[1], geom_label[1], geom_point[1], geom_text[1], ggplot[6], ggtitle[1], labs[5], position_stack[1], scale_colour_manual[2], scale_fill_manual[1], theme[4], theme_bw[1], theme_set[1], theme_void[1], unit[1]\n\n\nggrepel\ngeom_label_repel[2]\n\n\nglue\nglue[1]\n\n\nhansard\ncommons_members[1], mp_vote_record[1]\n\n\npurrr\ncompact[1], list_rbind[2], map[4], pluck[1], possibly[1], reduce[1]\n\n\nrlang\nset_names[1]\n\n\nstats\nas.dendrogram[2], cophenetic[1], cor[1], dist[2], hclust[3], lm[1], order.dendrogram[1], reorder[1]\n\n\nstringr\nstr_c[4], str_detect[1], str_replace[2], str_wrap[1]\n\n\ntibble\nas_tibble[1], tibble[1]\n\n\ntidyr\npivot_longer[2], pivot_wider[2]\n\n\ntidyselect\nends_with[4]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[3]"
  },
  {
    "objectID": "project/forest/index.html",
    "href": "project/forest/index.html",
    "title": "Criminal Goings-on in a Random Forest",
    "section": "",
    "text": "When first posted in 2018 this project used the caret package to model crime in London. Since then, the newer tidymodels(Kuhn and Wickham 2020) framework, consistent with tidy data principles, has rapidly evolved.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(scales)\nlibrary(vip)\nlibrary(poissonreg)\nlibrary(usedthese)\n\nconflict_scout()\n\n10 conflicts:\n* `chisq.test` : janitor, stats\n* `col_factor` : scales, readr\n* `discard`    : scales, purrr\n* `filter`     : [dplyr]\n* `fisher.test`: janitor, stats\n* `fixed`      : recipes, stringr\n* `lag`        : [dplyr]\n* `spec`       : yardstick, readr\n* `step`       : recipes, stats\n* `vi`         : vip, utils\nThis custom palette was created in Adobe Colour as the basis for the feature image above and with the hex codes loaded for use in ggplot. colorRampPalette enables interpolation of an extended set of colours to support the number of offence types.\ntheme_set(theme_bw())\n\ncols &lt;- c(\"#798E87\", \"#C27D38\", \"#CCC591\", \"#29211F\") |&gt;\n  fct_inorder()\n\ntibble(x = 1:4, y = 1) |&gt;\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols), size = 4, vjust = 2, fill = \"white\") +\n  annotate(\n    \"label\",\n    x = 2.5, y = 0.5,\n    label = \"Custom Pallette\",\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\ncols10 &lt;- colorRampPalette(cols)(10)\nurl &lt;- str_c(\n  \"https://data.london.gov.uk/\",\n  \"download/recorded_crime_rates/\",\n  \"c051c7ec-c3ad-4534-bbfe-6bdfee2ef6bb/\",\n  \"crime%20rates.csv\"\n)\n\nraw_df &lt;-\n  read_csv(url, col_types = \"cfcfdn\") |&gt;\n  clean_names() |&gt;\n  mutate(\n    year = str_extract(year, \"(?:1999|200[0-9]|201[0-7])\"), # 1999-2007\n    year = as.numeric(year)\n  ) |&gt;\n  summarise(number_of_offences = sum(number_of_offences),\n            .by = c(year, borough, offences))\nA faceted plot is one way to get a sense of the data.\nraw_df |&gt;\n  mutate(borough = str_wrap(borough, 11)) |&gt;\n  ggplot(aes(year, number_of_offences, \n             colour = offences, group = offences)) +\n  geom_line() +\n  facet_wrap(~borough, scales = \"free_y\", ncol = 4) +\n  labs(\n    x = NULL, y = NULL, title = \"London Crime by Borough\",\n    colour = \"Offence\", caption = \"Source: data.gov.uk\"\n  ) +\n  scale_colour_manual(values = cols10) +\n  guides(colour = guide_legend(nrow = 4)) +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\nVisualising data in small multiples using facet_wrap or facet_grid can be a useful way to explore data. When there are a larger number of these however, as we’re starting to see in the example above, there are alternative techniques one can employ. This is explored in Seeing the Wood for the Trees.\nNonetheless, one can anyway see there are data aggregated at multiple levels. So to net these data down to purely borough-level, I’ll filter out the summarised rows, for example, “England and Wales” and “Inner London”.\ncrime_df &lt;- raw_df |&gt;\n  filter(\n    offences != \"All recorded offences\",\n    !borough %in% c(\n      \"England and Wales\",\n      \"Met Police Area\", \n      \"Inner London\", \n      \"Outer London\"\n    )\n  )\nThere are 9 types of offence in 33 boroughs. The dataset covers the period 1999 to 2016.\nThe faceted plot hints at a potential interaction between borough and type of offence. In more affluent boroughs, and/or those attracting greater visitor numbers, e.g. Westminster and Kensington & Chelsea, “theft and handling” is the more dominant category. In Lewisham, for example, “violence against the person” exhibits higher counts. However, for the purpose of this basic model comparison, I’m going to set aside the potential interaction term.\nBefore modelling, I’ll visualise the dependent variable against each independent variable.\ncrime_df |&gt;\n  summarise(number_of_offences = sum(number_of_offences),\n            .by = c(offences, borough)) |&gt;\n  mutate(\n    median_offences = median(number_of_offences),\n    offences = str_wrap(offences, 10),\n    .by = offences\n  ) |&gt;\n  ggplot(aes(fct_reorder(offences, median_offences), number_of_offences)) +\n  geom_boxplot(fill = cols[1]) +\n  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Number of Offences by Type\",\n    caption = \"Source: data.gov.uk\"\n  )\ncrime_df |&gt;\n  summarise(number_of_offences = sum(number_of_offences),\n            .by = c(offences, borough)) |&gt;\n  mutate(\n    median_offences = median(number_of_offences),\n    offences = str_wrap(offences, 10),\n    .by = borough\n  ) |&gt;\n  ggplot(aes(fct_reorder(borough, median_offences), number_of_offences)) +\n  geom_boxplot(fill = cols[1]) +\n  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Number of Offences by Borough\",\n    caption = \"Source: data.gov.uk\"\n  )\nThe offences and borough variables show significant variation in crime counts. And there is also evidence of a change over time.\ncrime_df |&gt;\n  summarise(number_of_offences = sum(number_of_offences), .by = year) |&gt;\n  ggplot(aes(year, number_of_offences)) +\n  geom_line(colour = cols[4], linetype = \"dashed\") +\n  geom_smooth(colour = cols[5], fill = cols[1]) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Number of Offences by Year\",\n    caption = \"Source: data.gov.uk\"\n  )\nI’ll separate out some test data so I can compare the performance of the models on data they have not see during model training.\nset.seed(123)\n\ndata_split &lt;- \n  crime_df |&gt;\n  initial_split(strata = offences)\n\ncrime_train &lt;- data_split |&gt;\n  training()\n\ncrime_test &lt;- data_split |&gt;\n  testing()\nI’m using the recipes package to establish the role of the variables. Alternatively I could have used a formula-based approach, i.e. number_of_offences ~ borough + offences + year.\nWhilst borough and offences are nominal, I’m not creating any dummy variables since I intend to use tree-based models which will anyway branch left and right based on groups of values.\ncrime_recipe &lt;-\n  crime_train |&gt;\n  recipe() |&gt;\n  update_role(number_of_offences, new_role = \"outcome\") |&gt;\n  update_role(-has_role(\"outcome\"), new_role = \"predictor\")\n\nsummary(crime_recipe)\n\n\n\n\n\n\n\n\n\n\nvariable\ntype\nrole\nsource\n\n\n\nyear\ndouble , numeric\npredictor\noriginal\n\n\nborough\nfactor , unordered, nominal\npredictor\noriginal\n\n\noffences\nfactor , unordered, nominal\npredictor\noriginal\n\n\nnumber_of_offences\ndouble , numeric\noutcome\noriginal\nI’ll start with a Recursive Partitioning And Regression Trees (rpart) model. The feature importance plot tells me which variables are having the biggest influence on the model. The type of offence is the most important predictor in the rpart model, followed by the location of the offences. This makes intuitive sense.\nClearly there is a temporal component too otherwise there would be no trend.\nrp_model &lt;- \n  decision_tree() |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\nrp_wflow &lt;- workflow() |&gt;\n  add_recipe(crime_recipe) |&gt;\n  add_model(rp_model)\n\nrp_fit &lt;- rp_wflow |&gt; \n  fit(crime_train)\n\nrp_fit |&gt;\n  extract_fit_parsnip() |&gt; \n  vip(aesthetics = list(fill = cols[1])) +\n  labs(title = \"Feature Importance -- rpart\")\n\n\n\nrp_results &lt;- rp_fit |&gt; \n  augment(crime_test) |&gt; \n  mutate(model = \"rpart\")\nRanger is an implementation of random forests or recursive partitioning that, according to the documentation, is particularly suited to high dimensional data. My data is not high-dimensional, but let’s throw it into the mix.\nranger_model &lt;- \n  rand_forest() |&gt;\n  set_engine(\"ranger\", importance = \"impurity\") |&gt;\n  set_mode(\"regression\")\n\nranger_wflow &lt;- workflow() |&gt;\n  add_recipe(crime_recipe) |&gt;\n  add_model(ranger_model)\n\nranger_fit &lt;- ranger_wflow |&gt; \n  fit(crime_train)\n\nranger_fit |&gt;\n  extract_fit_parsnip() |&gt; \n  vip(aesthetics = list(fill = cols[3])) +\n  labs(title = \"Feature Importance -- Ranger\")\n\n\n\nranger_results &lt;- ranger_fit |&gt; \n  augment(crime_test) |&gt; \n  mutate(model = \"ranger\")\nAnd of course my project title would make little sense without a Random Forest.\nrf_model &lt;- \n  rand_forest() |&gt;\n  set_engine(\"randomForest\") |&gt;\n  set_mode(\"regression\")\n\nrf_wflow &lt;- workflow() |&gt;\n  add_recipe(crime_recipe) |&gt;\n  add_model(rf_model)\n\nrf_fit &lt;- rf_wflow |&gt; \n  fit(crime_train)\n\nrf_fit |&gt;\n  extract_fit_parsnip() |&gt; \n  vip(aesthetics = list(fill = \"grey60\")) +\n  labs(title = \"Feature Importance -- Random Forest\")\n\n\n\nrf_results &lt;- rf_fit |&gt; \n  augment(crime_test) |&gt; \n  mutate(model = \"random forest\")\nFor good measure, I’ll also include a generalized linear model (glm)\npoisson_model &lt;- \n  poisson_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  set_mode(\"regression\")\n\npoisson_wflow &lt;- workflow() |&gt;\n  add_recipe(crime_recipe) |&gt;\n  add_model(poisson_model)\n\npoisson_fit &lt;- poisson_wflow |&gt; \n  fit(crime_train)\n\npoisson_fit |&gt;\n  extract_fit_parsnip() |&gt; \n  vip(aesthetics = list(fill = cols[4])) +\n  labs(title = \"Feature Importance -- glm\")\n\n\n\npoisson_results &lt;- poisson_fit |&gt; \n  augment(crime_test) |&gt; \n  mutate(model = \"glm\")\nThe Random Forest and the glm models performed the best here, with the former edging the Mean Absolute Error and R Squared metrics, and the latter with its nose in front on the Root Mean Squared Error.\nmodel_results &lt;- \n  rp_results |&gt; \n  bind_rows(ranger_results) |&gt; \n  bind_rows(rf_results) |&gt; \n  bind_rows(poisson_results) |&gt; \n  group_by(model) |&gt; \n  metrics(truth = number_of_offences, estimate = .pred)\n\nmodel_results |&gt; \n  ggplot(aes(model, .estimate, fill = model)) +\n  geom_col() +\n  geom_label(aes(label = round(.estimate, 2)), size = 3, fill = \"white\") +\n  facet_wrap(~ .metric, scales = \"free_y\") +\n  scale_fill_manual(values = as.character(cols[c(4, 5, 3, 1)])) +\n  labs(x = NULL, y = NULL, title = \"Comparison of Model Metrics\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\nAnother way of approaching all this would be to use time-series forecasting. This would major on auto-regression, i.e. looking at how the lagged number-of-offences influence future values. And one could further include exogenous data such as, say, the numbers of police. It would be reasonable to expect that increasing police numbers would, in time, lead to decreased crime levels.\nI explored time-series in other posts such as Digging Deep, so I won’t go down that path here.\nWhat I could do though is to strengthen my tree-based models above by engineering some additional temporal features. Let’s try that just with the Random Forest to see if it improves the outcome.\ntemp_df &lt;- \n  crime_df |&gt; \n  mutate(num_lag1 = lag(number_of_offences),\n         num_lag2 = lag(number_of_offences, 2),\n         num_lag3 = lag(number_of_offences, 3)) |&gt; \n  drop_na()\nSo, when predicting the number of offences, the model will now additionally consider, for each borough, type of offence and year, the number of offences in each of the three prior years.\nset.seed(123)\n\ndata_split &lt;- \n  temp_df |&gt;\n  initial_split(strata = offences)\n\ntemp_train &lt;- data_split |&gt;\n  training()\n\ntemp_test &lt;- data_split |&gt;\n  testing()\n\ntemp_recipe &lt;-\n  temp_train |&gt;\n  recipe() |&gt;\n  update_role(number_of_offences, new_role = \"outcome\") |&gt;\n  update_role(-has_role(\"outcome\"), new_role = \"predictor\")\n\nsummary(temp_recipe)\n\n\n\n\n\n\n\n\n\n\nvariable\ntype\nrole\nsource\n\n\n\nyear\ndouble , numeric\npredictor\noriginal\n\n\nborough\nfactor , unordered, nominal\npredictor\noriginal\n\n\noffences\nfactor , unordered, nominal\npredictor\noriginal\n\n\nnumber_of_offences\ndouble , numeric\noutcome\noriginal\n\n\nnum_lag1\ndouble , numeric\npredictor\noriginal\n\n\nnum_lag2\ndouble , numeric\npredictor\noriginal\n\n\nnum_lag3\ndouble , numeric\npredictor\noriginal\n\n\n\n\n\ntemp_model &lt;- \n  rand_forest() |&gt;\n  set_engine(\"randomForest\") |&gt;\n  set_mode(\"regression\")\n\ntemp_wflow &lt;- workflow() |&gt;\n  add_recipe(temp_recipe) |&gt;\n  add_model(temp_model)\n\ntemp_fit &lt;- temp_wflow |&gt; \n  fit(temp_train)\n\ntemp_fit |&gt;\n  extract_fit_parsnip() |&gt; \n  vip(aesthetics = list(fill = cols[2])) +\n  labs(title = \"Feature Importance -- Random Forest with Lags\")\n\n\n\ntemp_results &lt;- temp_fit |&gt; \n  augment(temp_test) |&gt; \n  metrics(truth = number_of_offences, estimate = .pred) |&gt; \n  mutate(model = \"rf with lags\")\nThe recipe summary includes the three new predictors. And the feature importance plot shows the lags playing a larger role in the model than the year variable, so looks like we should anticipate a model improvement.\nupdated_results &lt;- \n  model_results |&gt; \n  bind_rows(temp_results)\n\nupdated_results |&gt; \n  ggplot(aes(model, .estimate, fill = model)) +\n  geom_col() +\n  geom_label(aes(label = round(.estimate, 2)), size = 3, fill = \"white\") +\n  facet_wrap(~ .metric, scales = \"free_y\") +\n  scale_fill_manual(values = as.character(cols[c(4, 5, 3, 2, 1)])) +\n  labs(x = NULL, y = NULL, title = \"Comparison of Model Metrics\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\nThe model metrics bear this out. The mae and rmse are markedly smaller, and the rsq significantly improved. We could have tried further lags. We could have tried tweaking some parameters. We could have tried time-series forecasting with, for example a statistical model like ARIMA, or a Neural Network model such as NNETAR.\nThe best approach would depend upon a more precise definition of the objective. And some trial and error, comparing approaches after more extensive feature-engineering, validation, testing and tuning. For the purposes of this post though I wanted to merely explore some techniques. So I’ll leave it there."
  },
  {
    "objectID": "project/forest/index.html#r-toolbox",
    "href": "project/forest/index.html#r-toolbox",
    "title": "Criminal Goings-on in a Random Forest",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[3], as.numeric[1], c[7], library[8], list[5], round[2], set.seed[2], sum[4], summary[2]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nbind_rows[4], filter[1], group_by[1], lag[3], mutate[10], summarise[4]\n\n\nforcats\nfct_inorder[1], fct_reorder[2]\n\n\ngenerics\naugment[5], fit[5]\n\n\nggplot2\naes[10], annotate[1], coord_flip[1], element_text[3], facet_wrap[3], geom_boxplot[2], geom_col[3], geom_label[3], geom_line[2], geom_smooth[1], ggplot[7], guide_legend[1], guides[1], labs[11], scale_colour_manual[1], scale_fill_manual[3], scale_y_continuous[1], scale_y_log10[2], theme[4], theme_bw[1], theme_set[1], theme_void[1]\n\n\ngrDevices\ncolorRampPalette[1]\n\n\nhardhat\nextract_fit_parsnip[5]\n\n\njanitor\nclean_names[1]\n\n\nparsnip\ndecision_tree[1], poisson_reg[1], rand_forest[3], set_engine[5], set_mode[5]\n\n\nreadr\nread_csv[1]\n\n\nrecipes\nhas_role[2], recipe[2], update_role[4]\n\n\nrsample\ninitial_split[2], testing[2], training[2]\n\n\nscales\ncut_short_scale[3], label_number[3]\n\n\nstats\nmedian[2]\n\n\nstringr\nstr_c[1], str_extract[1], str_wrap[3]\n\n\ntibble\ntibble[1]\n\n\ntidyr\ndrop_na[1]\n\n\nusedthese\nused_here[1]\n\n\nvip\nvip[5]\n\n\nworkflows\nadd_model[5], add_recipe[5], workflow[5]\n\n\nyardstick\nmetrics[2]"
  },
  {
    "objectID": "project/un/index.html",
    "href": "project/un/index.html",
    "title": "East-West Drift",
    "section": "",
    "text": "In Finding Happiness in ‘The Smoke’, dimension reduction and cluster analysis are used to see how different characteristics group London boroughs.\nDimension reduction is used here to visualise the grouping of UN members, for example five of the founding members, based on their General Assembly voting patterns. And by using animation, it’s possible to more easily see changes over time. Are they drifting closer or farther apart?\nGeneral Assembly votes are obtained from unvotes(Robinson 2021) and the visualisation animated using gganimate(Pedersen and Robinson 2022).\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(tidymodels)\nlibrary(tsibble)\nlibrary(gganimate)\nlibrary(clock)\nconflict_prefer(\"date_format\", \"clock\")\nlibrary(unvotes)\nlibrary(patchwork)\nlibrary(wesanderson)\nlibrary(rvest)\nlibrary(glue)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(\"GrandBudapest2\"))\nraw_df &lt;- un_votes |&gt; \n  inner_join(un_roll_calls, by = join_by(rcid)) |&gt; \n  select(vote_date = date, everything()) |&gt; \n  filter(country_code %in% c(\"GB\", \"CN\", \"US\", \"FR\", \"RU\")) |&gt;\n  mutate(\n    country = recode(\n      country_code,\n      GB = \"UK\",\n      CN = \"China\",\n      FR = \"France\",\n      RU = \"Russia\"\n    ),\n    vote_date = date_parse(as.character(vote_date), \n                           format = \"%Y-%m-%d\")\n  )\n\nfrom &lt;- raw_df |&gt;\n  summarise(min(get_year(vote_date))) |&gt;\n  pull()\n\nto &lt;- raw_df |&gt;\n  summarise(max(get_year(vote_date))) |&gt;\n  pull()\nApplying a sliding window to the roll-calls from 1946 to 2019 will make it possible to show the temporal changes.\ntidy_df &lt;- raw_df |&gt;\n  arrange(vote_date, rcid) |&gt;\n  nest(.by = c(vote_date, rcid)) |&gt;\n  mutate(vote_id = row_number(), year = get_year(vote_date)) |&gt;\n  unnest(data) |&gt;\n  complete(country, nesting(vote_id)) |&gt;\n  mutate(vote = replace_na(as.character(vote), \"na\"), value = 1) |&gt;\n  group_by(vote_id) |&gt;\n  fill(year, .direction = \"updown\") |&gt;\n  mutate(variation = n_distinct(vote)) |&gt;\n  ungroup() |&gt;\n  filter(variation != 1) |&gt;\n  select(country, vote_id, year, vote, value)\n\nwdow_df &lt;- tidy_df |&gt;\n  as_tsibble(key = country, index = vote_id) |&gt;\n  nest(.by = vote_id) |&gt;\n  slide_tsibble(.size = 1000, .step = 250, .id = \"slide_id\") |&gt;\n  unnest(data) |&gt;\n  as_tibble() |&gt;\n  arrange(slide_id, vote_id, country)\nDimensionality reduction may be performed on each window. And the voting patterns are then visualised as a two-dimensional animation.\nwdows &lt;- wdow_df |&gt;\n  summarise(max(slide_id)) |&gt;\n  pull()\n\nslide_pca &lt;- \\(x) {\n  wide_df &lt;- wdow_df |&gt;\n    filter(slide_id == x) |&gt;\n    pivot_wider(\n      id_cols = c(country, slide_id),\n      names_from = c(vote_id, vote),\n      values_from = value,\n      values_fill = 0\n    )\n\n  pca_fit &lt;- wide_df |&gt;\n    select(-c(country, slide_id)) |&gt;\n    prcomp(scale = TRUE) |&gt;\n    augment(wide_df) |&gt;\n    select(slide_id, country, .fittedPC1, .fittedPC2)\n}\n\npca_windows &lt;- map(1:wdows, slide_pca) |&gt; list_rbind()\n\np &lt;- pca_windows |&gt;\n  mutate(east_west = if_else(country %in% c(\"China\", \"Russia\"), \n                             \"East\", \"West\")) |&gt;\n  ggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_label(aes(label = country, fill = east_west)) +\n  scale_fill_manual(values = cols[c(1, 3)]) +\n  transition_time(slide_id) +\n  labs(\n    title = glue(\"P5 Distance for the Period {from} to {to}\"),\n    subtitle = \"Frame {frame} of {nframes}\",\n    x = \"Principal Component 1\",\n    y = \"Principal Component 2\",\n    fill = NULL,\n    caption = \"Source: unvotes\"\n  ) +\n  shadow_wake(wake_length = 0.1, wrap = FALSE)\n  \nanimate(p, fps = 5, end_pause = 10)\nFrance and the UK, for example, have remained particularly close given their historical ties and geographical proximity.\nThe UN’s Security Council Veto List provides further insights on the changing profile of P5 voting over the decades.\nurl &lt;- \"https://www.un.org/depts/dhl/resguide/scact_veto_table_en.htm\"\n\nmeeting_df &lt;- url |&gt;\n  read_html() |&gt;\n  html_element(\".tablefont\") |&gt;\n  html_table(fill = TRUE) |&gt;\n  select(vote_date = 1, draft = 2, meeting = 3, agenda = 4, vetoed_by = 5) |&gt;\n  slice(-c(1:2))\nmeeting_df2 &lt;- meeting_df |&gt;\n  mutate(\n    vote_date = str_remove(vote_date, \"-(?:\\\\d{2}|\\\\d)\"),\n    vote_date = date_parse(vote_date, format = \"%d %B %Y\"),\n    vote_date = if_else(get_year(vote_date) == \"86\", \n                        date_build(1986, 01, 01), vote_date),\n    vetoed_by = str_replace(vetoed_by, \"USSR\", \"Russia\"),\n    Russia = if_else(str_detect(vetoed_by, \"Russia\"), 1, 0),\n    China = if_else(str_detect(vetoed_by, \"China\"), 1, 0),\n    France = if_else(str_detect(vetoed_by, \"France\"), 1, 0),\n    US = if_else(str_detect(vetoed_by, \"US\"), 1, 0),\n    UK = if_else(str_detect(vetoed_by, \"UK\"), 1, 0)\n  ) |&gt;\n  pivot_longer(c(Russia:UK), names_to = \"country\", values_to = \"veto\") |&gt;\n  filter(veto == 1)\n\ncountry_df &lt;- meeting_df2 |&gt;\n  count(country) |&gt;\n  mutate(country = fct_reorder(country, n))\ncols2 &lt;- wes_palette(5, name = \"GrandBudapest2\", type = \"continuous\")\n\nlittle_plot &lt;- country_df |&gt;\n  ggplot(aes(country, n, fill = country)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = cols2[c(1:5)]) +\n  geom_label(aes(label = n), colour = \"white\", hjust = \"inward\") +\n  labs(\n    x = NULL, y = NULL, fill = NULL, title = \"Most Vetoes\",\n    caption = \"Source: research.un.org\"\n  )\n\nyear_df &lt;- meeting_df2 |&gt;\n  mutate(year = get_year(vote_date)) |&gt;\n  count(year, country)\n\nto_date &lt;- format(max(meeting_df2$vote_date), \"%b %d, %y\")\n\nbig_plot &lt;- year_df |&gt;\n  ggplot(aes(year, n, fill = country)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_manual(values = cols2[c(1:5)]) +\n  scale_x_continuous(breaks = (seq(1945, 2020, 5))) +\n  labs(\n    x = NULL, y = \"Veto Count\", fill = NULL,\n    title = glue(\"Security Council Vetoes to {to_date}\")\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nlayout &lt;- \"AAB\"\nbig_plot + little_plot + plot_layout(design = layout)"
  },
  {
    "objectID": "project/un/index.html#r-toolbox",
    "href": "project/un/index.html#r-toolbox",
    "title": "East-West Drift",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[2], c[11], format[1], library[12], max[3], min[1], seq[1]\n\n\nclock\ndate_build[1], date_parse[2], get_year[5]\n\n\nconflicted\nconflict_prefer[1], conflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\narrange[2], count[2], filter[4], group_by[1], if_else[7], inner_join[1], join_by[1], mutate[8], n_distinct[1], pull[3], recode[1], row_number[1], select[5], slice[1], summarise[3], ungroup[1]\n\n\nforcats\nfct_reorder[1]\n\n\ngenerics\naugment[1]\n\n\ngganimate\nanimate[1], shadow_wake[1], transition_time[1]\n\n\nggplot2\naes[5], coord_flip[1], element_text[1], geom_col[2], geom_label[2], ggplot[3], labs[3], scale_fill_manual[3], scale_x_continuous[1], theme[1], theme_bw[1], theme_set[1]\n\n\nglue\nglue[2]\n\n\npatchwork\nplot_layout[1]\n\n\npurrr\nlist_rbind[1], map[1]\n\n\nrvest\nhtml_element[1], html_table[1]\n\n\nstats\nprcomp[1]\n\n\nstringr\nstr_detect[5], str_remove[1], str_replace[1]\n\n\ntibble\nas_tibble[1]\n\n\ntidyr\ncomplete[1], fill[1], nest[2], nesting[1], pivot_longer[1], pivot_wider[1], replace_na[1], unnest[2]\n\n\ntidyselect\neverything[1]\n\n\ntsibble\nas_tsibble[1], slide_tsibble[1]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[2]\n\n\nxml2\nread_html[1]"
  },
  {
    "objectID": "project/cetacea/index.html",
    "href": "project/cetacea/index.html",
    "title": "Sea Monsters that Lost their Way",
    "section": "",
    "text": "The Natural History Museum began recording cetacean (whales, dolphins and porpoises) strandings in 1913 (Natural History Museum 2019). Let’s explore this 1913-1989 dataset.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(tidymodels)\nlibrary(probably)\nlibrary(finetune)\nlibrary(textrecipes)\nlibrary(stopwords)\nlibrary(wesanderson)\nlibrary(clock)\nlibrary(glue)\nlibrary(janitor)\nlibrary(vip)\nconflict_prefer(\"vi\", \"vip\")\nlibrary(tictoc)\nlibrary(patchwork)\nlibrary(doParallel)\nlibrary(usedthese)\n\nconflict_scout()\n\nregisterDoParallel(cores = 6)\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(name = \"Darjeeling2\"))\nstrandings_df &lt;- read_csv(\"strandings.csv\", show_col_types = FALSE) |&gt;\n  clean_names() |&gt; \n  mutate(\n    date_rep = date_parse(date, format = \"%d/%m/%Y\"),\n    length = parse_number(length_et),\n    species_lumped = fct_lump_n(species, 20),\n    across(ends_with(\"_val\"), as.integer),\n    rep_comment = comment\n  )\n\n# glimpse(strandings_df)"
  },
  {
    "objectID": "project/cetacea/index.html#exploratory",
    "href": "project/cetacea/index.html#exploratory",
    "title": "Sea Monsters that Lost their Way",
    "section": "Exploratory",
    "text": "Exploratory\nSome of the species labels contain a question mark or forward slash. This indicates uncertainty, so it might be fun to see if a machine learning model (multi-class classification) could learn from the known species and suggest an appropriate species where it’s uncertain.\n\nstrandings_df2 &lt;- \n  strandings_df |&gt; \n  mutate(species_uncertainty =\n      if_else(str_detect(species, \"[?/]\"), \"Uncertain\", \"Known\"))\n\nstrandings_df2 |&gt; \n  filter(species_uncertainty == \"Uncertain\") |&gt; \n  count(species, sort = TRUE, name = \"Count\") |&gt; \n  slice_head(n = 6)\n\n\n\n\nspecies\nCount\n\n\n\ndelphis/coeruleoalba\n48\n\n\nphocoena?\n42\n\n\nmelaena?\n20\n\n\ndelphis?\n18\n\n\ntruncatus?\n18\n\n\nacutorostrata?\n12\n\n\n\n\n\n\nThe date variable has many NA’s. Fortunately, the components to construct many of these are in the year_val, month_val and day_val variables. With a little wrangling and imputation, we can coalesce these variables into a new date. This will be useful since plots of sample species by year, month and week of stranding suggest a de-constructed date could be a useful predictor.\n\nstrandings_df2 |&gt; \n  select(date_rep, year_val, month_val, day_val) |&gt; \n  summary()\n\n    date_rep             year_val      month_val         day_val     \n Min.   :1913-02-13   Min.   :   0   Min.   : 0.000   Min.   : 0.00  \n 1st Qu.:1933-09-09   1st Qu.:1933   1st Qu.: 4.000   1st Qu.: 9.00  \n Median :1954-04-13   Median :1955   Median : 7.000   Median :16.00  \n Mean   :1955-01-08   Mean   :1954   Mean   : 6.766   Mean   :15.66  \n 3rd Qu.:1979-03-21   3rd Qu.:1979   3rd Qu.:10.000   3rd Qu.:22.00  \n Max.   :1989-12-25   Max.   :1989   Max.   :12.000   Max.   :31.00  \n NA's   :121                                                         \n\nstrandings_df3 &lt;- strandings_df2 |&gt;\n  mutate(\n    month_val = if_else(month_val == 0, mean(month_val) |&gt; \n                          as.integer(), month_val),\n    day_val = if_else(day_val == 0, mean(day_val) |&gt; as.integer(), day_val),\n    day_val = if_else(day_val == 0, 1L, day_val),\n    date2 = date_build(year_val, month_val, day_val, invalid = \"NA\"),\n    .by = species\n  ) |&gt; \n  mutate(date3 = coalesce(date_rep, date2),\n         date_rep = if_else(is.na(date_rep), lag(date3), date3)\n         ) |&gt; \n  select(-date2, -date3, -ends_with(\"_val\"))\n\nexample_species &lt;-\n  c(\"musculus\", \"melas\", \"crassidens\", \"borealis\", \"coeruleoalba\")\n\nknown_species &lt;- strandings_df3 |&gt; \n  filter(species_uncertainty == \"Known\")\n\nplot_date_feature &lt;- \\(var) {\n  known_species |&gt;\n    mutate(\n      year = get_year(date_rep),\n      month = get_month(date_rep),\n      week = as_iso_year_week_day(date_rep) |&gt; get_week()\n    ) |&gt;\n    filter(species %in% example_species) |&gt;\n    count(species, {{ var }}) |&gt;\n    ggplot(aes(species, {{ var }})) +\n    geom_violin(\n      alpha = 0.7,\n      fill = cols[3],\n      show.legend = FALSE\n    ) +\n    coord_flip() +\n    labs(\n      title = glue(\"Variation in {str_to_title(as.character(var))}\",\n                   \" of Stranding for Known Species\"),\n      x = NULL, y = glue(\"{str_to_title(as.character(var))}\")\n    )\n}\n\nc(\"year\", \"month\", \"week\") |&gt; \n  map(sym) |&gt; \n  map(plot_date_feature) |&gt; \n  wrap_plots(ncol = 1)\n\n\n\n\nDo latitude and longitude carry useful predictive information?\nA geospatial visualisation of strandings shows some species do gravitate towards particular stretches of coastline, e.g. “acutus” and “albirostris” to the east, and “coeruleoalba” to the south-west.\nSome species may also be more prone to mass stranding, so something that indicates whether a species has such a history (in these data) may be worth including in the mix.\n\nuki &lt;- map_data(\"world\", region = c(\"uk\", \"ireland\"))\n\nlabels &lt;- c(\"Mass\", \"Single\")\n\nuki |&gt; \n  ggplot() +\n  geom_map(aes(long, lat, map_id = region), map = uki, \n           colour = \"black\", fill = \"grey90\", size = 0.1) +\n  geom_jitter(aes(longitude, latitude, colour = mass_single, \n                  size = mass_single), \n              alpha = 0.5, data = known_species) +\n  facet_wrap(~ species_lumped, nrow = 3) +\n  coord_map(\"mollweide\") +\n  scale_size_manual(values = c(1, 0.5), labels = labels) +\n  scale_colour_manual(values = cols[c(3, 2)], labels = labels) +\n  theme_void() +\n  theme(legend.position = \"top\", \n        strip.text = element_text(colour = \"grey50\")) +\n  labs(title = \"Strandings by Species\", \n       colour = NULL, size = NULL)\n\n\n\n# Add history of mass stranding\nstrandings_df4 &lt;- strandings_df3 |&gt; \n  mutate(mass_possible = min(mass_single, na.rm = TRUE),\n         .by = species)\n\nSome records are missing the length measurement of the mammal. Nonetheless, where present, this is likely to be predictive, and may help, for example, separate species labelled as “delphis/coeruleoalba” where the length is at the extreme ends of the “delphis” range as we see below. And the range of length may differ by mammal sex.\n\nknown_species |&gt;\n  mutate(sex = case_match(\n    sex,\n    \"M\" ~ \"Male\",\n    \"F\" ~ \"Female\",\n    .default = \"Unknown\"\n  )) |&gt; \n  filter(species_lumped != \"Other\") |&gt; \n  count(species_lumped, length, sex) |&gt; \n  mutate(species_lumped = fct_reorder(species_lumped, \n                                      desc(length), min, na.rm = TRUE)) |&gt; \n  ggplot(aes(species_lumped, length)) + \n  geom_violin(aes(fill = if_else(str_detect(species_lumped, \"^de|^co\"), \n                                 TRUE, FALSE)), show.legend = FALSE) +\n  facet_wrap(~ sex) +\n  scale_fill_manual(values = cols[c(1, 5)]) +\n  coord_flip() +\n  labs(title = \"Variation in Species Length by Sex\", \n       x = NULL, y = \"Length (metres)\")\n\n\n\n\nWith map coordinates not always available, county could be, with a little string cleaning, a useful additional predictor.\n\nstrandings_df4 |&gt; \n  count(county) |&gt; \n  filter(str_detect(county, \"Shet|Northumberland\")) |&gt; \n  rename(County = county, Count = n)\n\n\n\n\nCounty\nCount\n\n\n\nFair Isle, Shetland Isles\n1\n\n\nNorthumberland\n89\n\n\nNorthumberland.\n1\n\n\nShetland Islands, Scotland\n232\n\n\nShetland Isles, Scotland\n35\n\n\nShetland, Scotland\n1\n\n\nShetlands, Scotland\n1\n\n\n\n\n\nregex_pattern &lt;-\n  c(\"[,/].*$\",\n    \"(?&lt;!Che|Hamp|Lanca|North York)-?shire\",\n    \" Isl.*$\",\n    \" &.*$\",\n    \"[-.]$\")\n\nstrandings_df5 &lt;- strandings_df4 |&gt;\n  mutate(\n    county = str_remove_all(county, str_c(regex_pattern, collapse = \"|\")),\n    county = recode(\n      county,\n      \"Carnarvon\" = \"Caernarvon\",\n      \"E.Lothian\" = \"East Lothian\",\n      \"Shetlands\" = \"Shetland\",\n      \"W.Glamorgan\" = \"West Glamorgan\",\n      \"S.Glamorgan\" = \"South Glamorgan\"\n    )\n  ) \n\nstrandings_df4 |&gt;\n  summarise(counties_before = n_distinct(county))\n\n\n\n\ncounties_before\n\n\n146\n\n\n\n\nstrandings_df5 |&gt;\n  summarise(counties_after = n_distinct(county))\n\n\n\n\ncounties_after\n\n\n109\n\n\n\n\n\nWhilst count also appears to hold, based on the plot pattern below, species-related information, I’m not going to use it as a predictor as we do not know enough about how it was derived, as reflected in these variable descriptions.\n\nstrandings_df5 |&gt;\n  ggplot(aes(species, count, colour = species_uncertainty)) +\n  geom_jitter(alpha = 0.5, size = 2) +\n  coord_flip() +\n  scale_y_log10() +\n  scale_colour_manual(values = cols[c(1, 5)]) +\n  labs(title = \"How 'Count' Relates to Species\", \n       x = NULL, y = \"Count (log10)\", colour = \"Species\") +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "project/cetacea/index.html#modelling",
    "href": "project/cetacea/index.html#modelling",
    "title": "Sea Monsters that Lost their Way",
    "section": "Modelling",
    "text": "Modelling\nSo, I’ll set aside the rows where the species is uncertain (to be used later for new predictions), and I’ll train a model on 75% of known species, and test it on the remaining 25%. I’ll use the following predictors:\n\n\nlatitude and longitude\n\nMammal length and sex\n\n\nmass_possible indicating a species history of mass strandings\n\ndate reported converted into decimal, week, month and year\n\ncounty may be useful, especially where the longitude and latitude are missing\n\nfam_genus which narrows the range of likely species\n\nI’d like to also make use of the textrecipes(Hvitfeldt 2022) package. I can tokenise the textual information in rep_comment and location to see if these add to the predictive power of the model.\nI’ll tune the model using tune_race_anova(Kuhn 2022) which quickly discards hyperparameter combinations showing little early promise.\n\nknown_species &lt;- strandings_df5 |&gt;\n  filter(species_uncertainty == \"Known\") |&gt;\n  mutate(across(\n    c(\n      \"species\",\n      \"mass_single\",\n      \"mass_possible\",\n      \"county\",\n      \"location\",\n      \"sex\",\n      \"fam_genus\"\n    ),\n    factor\n  ))\n\nset.seed(123)\n\ndata_split &lt;-\n  known_species |&gt;\n  mutate(species = fct_drop(species)) |&gt; \n  initial_split(strata = species)\n\ntrain &lt;- data_split |&gt; training()\n\ntest &lt;- data_split |&gt; testing()\n\npredictors &lt;-\n  c(\n    \"latitude\",\n    \"longitude\",\n    \"length\",\n    \"mass_single\",\n    \"mass_possible\",\n    \"county\",\n    \"location\",\n    \"rep_comment\",\n    \"sex\",\n    \"fam_genus\"\n  )\n\nrecipe &lt;-\n  train |&gt;\n  recipe() |&gt;\n  update_role(species, new_role = \"outcome\") |&gt;\n  update_role(all_of(predictors), new_role = \"predictor\") |&gt;\n  update_role(!has_role(\"outcome\") & !has_role(\"predictor\"), \n              new_role = \"id\") |&gt;\n  step_date(date_rep, features = c(\"decimal\", \"week\", \"month\", \"year\"), \n            label = FALSE) |&gt;\n  step_tokenize(location, rep_comment) |&gt;\n  step_stopwords(location, rep_comment) |&gt;\n  step_tokenfilter(location, rep_comment, max_tokens = tune()) |&gt; #100\n  step_tf(location, rep_comment) |&gt;\n  step_zv(all_predictors()) |&gt;\n  step_dummy(all_nominal_predictors())\n\nxgb_model &lt;-\n  boost_tree(trees = tune(), # 440\n             mtry = tune(), # 0.6\n             learn_rate = 0.02) |&gt; \n  set_mode(\"classification\") |&gt;\n  set_engine(\"xgboost\", \n             count = FALSE,\n             verbosity = 0,\n             tree_method = \"hist\")\n\nxgb_wflow &lt;- workflow() |&gt;\n  add_recipe(recipe) |&gt;\n  add_model(xgb_model)\n\nset.seed(9)\n\nfolds &lt;- vfold_cv(train, strata = species)\n\nset.seed(10)\n\ntic()\n\ntune_result &lt;- xgb_wflow |&gt;\n  tune_race_anova(\n    resamples = folds,\n    grid = crossing(\n      trees = seq(200, 520, 40),\n      mtry = seq(0.5, 0.7, 0.1),\n      max_tokens = seq(80, 120, 20)\n      ),\n    control = control_race(),\n    metrics = metric_set(accuracy)\n  )\n\ntoc()\n\n396.407 sec elapsed\n\ntune_result |&gt; \n  plot_race() + \n  labs(title = \"Early Elimination of Parameter Combinations\")\n\n\n\nset.seed(123)\n\nxgb_fit &lt;- xgb_wflow |&gt;\n  finalize_workflow(tune_result |&gt; \n                      select_best(metric = \"accuracy\")) |&gt; \n  fit(train)\n\nHaving fitted the model with the 3080 records in the training data, I’ll test its accuracy on the 1028 records of known species the model has not yet seen.\nWithout spending time on alternative models, we’re getting a reasonable result for the “porpoise” of this post, as reflected in both the accuracy metric and confusion matrix.\n\nxgb_results &lt;- xgb_fit |&gt; \n  augment(new_data = test)\n\n[14:40:54] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nxgb_results |&gt;\n  accuracy(species, .pred_class)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\naccuracy\nmulticlass\n0.9931907\n\n\n\n\nxgb_results |&gt;\n  conf_mat(species, .pred_class) |&gt;\n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient2(\n    mid = \"white\",\n    high = cols[1],\n    midpoint = 0\n  ) +\n  labs(title = \"Confusion Matrix\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nThe top variable importance scores include fam_genus, many of the rep_comment tokens, plus length, mass-possible, date_decimal, date_year, and latitude.\n\nvi(xgb_fit |&gt; extract_fit_parsnip()) |&gt; \n  arrange(desc(Importance)) |&gt; \n  mutate(ranking = row_number()) |&gt; \n  slice_head(n = 40)\n\n\n\n\nVariable\nImportance\nranking\n\n\n\nfam_genus_Phocoena\n0.1215845\n1\n\n\nfam_genus_Globicephala\n0.0816541\n2\n\n\ntf_rep_comment_unidentified\n0.0736307\n3\n\n\nfam_genus_Delphinus\n0.0710860\n4\n\n\nfam_genus_Tursiops\n0.0500141\n5\n\n\ntf_rep_comment_false\n0.0498405\n6\n\n\nfam_genus_Lagenorhynchus\n0.0448997\n7\n\n\ntf_rep_comment_finned\n0.0341306\n8\n\n\ntf_rep_comment_sided\n0.0302409\n9\n\n\ntf_rep_comment_long\n0.0244866\n10\n\n\nfam_genus_Hyperoodon\n0.0240353\n11\n\n\nlength\n0.0230962\n12\n\n\nfam_genus_Grampus\n0.0227513\n13\n\n\ntf_rep_comment_beaked\n0.0219021\n14\n\n\ntf_rep_comment_lesser\n0.0215853\n15\n\n\ntf_rep_comment_rorqual\n0.0199182\n16\n\n\ntf_rep_comment_dolphin\n0.0198597\n17\n\n\nfam_genus_Orcinus\n0.0194480\n18\n\n\ntf_rep_comment_porpoise\n0.0192418\n19\n\n\ntf_rep_comment_bottle\n0.0165892\n20\n\n\nfam_genus_Pseudorca\n0.0159957\n21\n\n\nfam_genus_Physeter\n0.0159090\n22\n\n\ntf_rep_comment_risso’s\n0.0131135\n23\n\n\nmass_possible_S\n0.0127573\n24\n\n\nfam_genus_Mesoplodon\n0.0123743\n25\n\n\ntf_rep_comment_fin\n0.0122737\n26\n\n\nfam_genus_Ziphius\n0.0122521\n27\n\n\nfam_genus_odontocete\n0.0109274\n28\n\n\nfam_genus_cetacean\n0.0104412\n29\n\n\ntf_rep_comment_killer\n0.0099741\n30\n\n\nfam_genus_Stenella\n0.0087018\n31\n\n\ndate_rep_decimal\n0.0081614\n32\n\n\ntf_rep_comment_nosed\n0.0075943\n33\n\n\ndate_rep_year\n0.0072361\n34\n\n\ntf_rep_comment_whale\n0.0071684\n35\n\n\ntf_rep_comment_white\n0.0067214\n36\n\n\nmass_single_S\n0.0041988\n37\n\n\ntf_rep_comment_common\n0.0041767\n38\n\n\ntf_rep_comment_cuvier’s\n0.0036603\n39\n\n\ntf_rep_comment_sowerby’s\n0.0036372\n40\n\n\n\n\n\n\nDo the predictions look reasonable?\nThe class probability is spread across 27 species. I’m going to set a high threshold of 0.9, meaning the predicted species needs to be a pretty confident prediction.\n\nxgb_preds &lt;- xgb_fit |&gt; \n  augment(new_data = strandings_df5 |&gt; \n            filter(species_uncertainty == \"Uncertain\"))\n\nspecies_levels &lt;- xgb_preds |&gt; \n  select(starts_with(\".pred_\"), -.pred_class) |&gt; \n  names() |&gt; \n  as.factor()\n\nsubset_df &lt;- xgb_preds |&gt;\n  mutate(\n    .class_pred = make_class_pred(\n      .pred_acutorostrata,\n      .pred_acutus,\n      .pred_albirostris,\n      .pred_ampullatus,\n      .pred_bidens,\n      .pred_borealis,\n      .pred_breviceps,\n      .pred_cavirostris,\n      .pred_coeruleoalba,\n      .pred_crassidens,\n      .pred_delphis,\n      .pred_electra,\n      .pred_europaeus,\n      .pred_griseus,\n      .pred_leucas,\n      .pred_macrocephalus,\n      .pred_melaena,\n      .pred_melas,\n      .pred_mirus,\n      .pred_monoceros,\n      .pred_musculus,\n      .pred_novaeangliae,\n      .pred_orca,\n      .pred_phocoena,\n      .pred_physalus,\n      .pred_sp.indet.,\n      .pred_truncatus,\n      levels = levels(species_levels),\n      min_prob = .9\n    )\n  )\n\nsubset_df |&gt;\n  summarise(n = n(), .by = c(species, .class_pred)) |&gt; \n  arrange(species, desc(n)) |&gt; \n  rename(\"Actual\" = species, \"Predicted\" = .class_pred, \"Count\" = n)\n\n\n\n\nActual\nPredicted\nCount\n\n\n\nacutorostrata/borealis\n.pred_acutorostrata\n1\n\n\nacutorostrata?\n.pred_acutorostrata\n12\n\n\nacutus?\n.pred_acutus\n3\n\n\nalbirostris?\n.pred_albirostris\n9\n\n\nampullatus?\n.pred_ampullatus\n3\n\n\nbidens?\n.pred_bidens\n2\n\n\nbidens?\n[EQ]\n1\n\n\ncavirostris?\n.pred_cavirostris\n7\n\n\ncoeruleoalba?\n.pred_coeruleoalba\n1\n\n\ndelphis/coeruleoalba\n[EQ]\n48\n\n\ndelphis?\n.pred_delphis\n18\n\n\ngriseus?\n.pred_griseus\n2\n\n\nmacrocephalus?\n.pred_macrocephalus\n2\n\n\nmelaena?\n.pred_melaena\n20\n\n\norca?\n.pred_orca\n4\n\n\nphocoena?\n.pred_phocoena\n42\n\n\nphysalus/acutorostrata\n[EQ]\n1\n\n\nphysalus?\n.pred_physalus\n4\n\n\ntruncatus/albirostris\n[EQ]\n5\n\n\ntruncatus?\n.pred_truncatus\n18\n\n\n\n\n\n\nThe majority of the 203 uncertain records are predicted to be as suspected in the original labelling. The remainder are classed as equivocal as they have not met the high bar of a 0.9-or-above probability for a single species."
  },
  {
    "objectID": "project/cetacea/index.html#r-toolbox",
    "href": "project/cetacea/index.html#r-toolbox",
    "title": "Sea Monsters that Lost their Way",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[2], as.integer[2], c[13], is.na[1], levels[1], library[16], mean[2], min[1], names[1], seq[3], set.seed[4], summary[1]\n\n\nclock\nas_iso_year_week_day[1], date_build[1], date_parse[1], get_month[1], get_week[1], get_year[1]\n\n\nconflicted\nconflict_prefer[1], conflict_prefer_all[1], conflict_scout[1]\n\n\ndoParallel\nregisterDoParallel[1]\n\n\ndplyr\nacross[2], arrange[2], case_match[1], coalesce[1], count[4], desc[3], filter[7], if_else[6], lag[1], mutate[13], n[1], n_distinct[2], recode[1], rename[2], row_number[1], select[3], slice_head[2], starts_with[1], summarise[3]\n\n\nfinetune\ncontrol_race[1], plot_race[1], tune_race_anova[1]\n\n\nforcats\nfct_drop[1], fct_lump_n[1], fct_reorder[1]\n\n\ngenerics\nas.factor[1], augment[2], fit[1]\n\n\nggplot2\naes[6], autoplot[1], coord_flip[3], coord_map[1], element_text[2], facet_wrap[2], geom_jitter[2], geom_map[1], geom_violin[2], ggplot[4], labs[6], map_data[1], scale_colour_manual[2], scale_fill_gradient2[1], scale_fill_manual[1], scale_size_manual[1], scale_y_log10[1], theme[3], theme_bw[1], theme_set[1], theme_void[1]\n\n\nglue\nglue[2]\n\n\nhardhat\nextract_fit_parsnip[1]\n\n\njanitor\nclean_names[1]\n\n\nparsnip\nboost_tree[1], set_engine[1], set_mode[1]\n\n\npatchwork\nwrap_plots[1]\n\n\nprobably\nmake_class_pred[1]\n\n\npurrr\nmap[2]\n\n\nreadr\nparse_number[1], read_csv[1]\n\n\nrecipes\nall_nominal_predictors[1], all_predictors[1], has_role[2], recipe[1], step_date[1], step_dummy[1], step_zv[1], update_role[3]\n\n\nrsample\ninitial_split[1], testing[1], training[1], vfold_cv[1]\n\n\nstringr\nstr_c[1], str_detect[3], str_remove_all[1], str_to_title[2]\n\n\ntextrecipes\nstep_stopwords[1], step_tf[1], step_tokenfilter[1], step_tokenize[1]\n\n\ntictoc\ntic[1], toc[1]\n\n\ntidyr\ncrossing[1]\n\n\ntidyselect\nall_of[1], ends_with[2]\n\n\ntune\nfinalize_workflow[1], select_best[1], tune[3]\n\n\nusedthese\nused_here[1]\n\n\nvip\nvi[1]\n\n\nwesanderson\nwes_palette[1]\n\n\nworkflows\nadd_model[1], add_recipe[1], workflow[1]\n\n\nyardstick\naccuracy[1], conf_mat[1], metric_set[1]"
  },
  {
    "objectID": "project/deal/index.html",
    "href": "project/deal/index.html",
    "title": "A Frosty Deal?",
    "section": "",
    "text": "Before the post-Brexit trade negotiations concluded, what did quantitative textual analysis and word embeddings tell us about the shifting trade-talk sentiment?\nReading news articles on the will-they-won’t-they post-Brexit trade negotiations with the EU sees days of optimism jarred by days of gloom. Do negative news articles, when one wants a positive outcome, leave a deeper impression?\nIs it possible to get a more objective view from quantitative analysis of textual data? To do this, I’m going to look at hundreds of articles published in the Guardian newspaper over the course of the year to see how trade-talk sentiment changed week-to-week.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(paletteer)\nlibrary(guardianapi)\nlibrary(quanteda)\nlibrary(scales)\nlibrary(tictoc)\nlibrary(clock)\nlibrary(patchwork)\nlibrary(text2vec)\nlibrary(topicmodels)\nlibrary(slider)\nlibrary(glue)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\nn &lt;- 4\npalette &lt;- \"wesanderson::Chevalier1\"\n\ncols &lt;- paletteer_d(palette, n = n)\n\ntibble(x = 1:n, y = 1) |&gt;\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols |&gt; str_remove(\"FF$\")), \n             size = 4, vjust = 2, fill = \"white\") +\n  annotate(\n    \"label\",\n    x = (n + 1) / 2, y = 0.5,\n    label = palette,\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\nThe Withdrawal Agreement between the UK and the European Union was signed on the 24th of January 2020. Brexit-related newspaper articles will be imported from that date.\nThe Guardian newspaper asks for requests to span no more than 1 month at a time. Creating a set of monthly date ranges will enable the requests to be chunked.\ndates_df &lt;- tibble(start_date = date_build(2020, 1:11, 25)) |&gt; \n  mutate(end_date = add_months(start_date, 1) |&gt; add_days(-1))\n\ndates_df\n\n\n\n\nstart_date\nend_date\n\n\n\n2020-01-25\n2020-02-24\n\n\n2020-02-25\n2020-03-24\n\n\n2020-03-25\n2020-04-24\n\n\n2020-04-25\n2020-05-24\n\n\n2020-05-25\n2020-06-24\n\n\n2020-06-25\n2020-07-24\n\n\n2020-07-25\n2020-08-24\n\n\n2020-08-25\n2020-09-24\n\n\n2020-09-25\n2020-10-24\n\n\n2020-10-25\n2020-11-24\n\n\n2020-11-25\n2020-12-24\ntic()\n\nread_slowly &lt;- slowly(gu_content)\n\narticle_df &lt;-\n  pmap(dates_df, \\(start_date, end_date) {\n    read_slowly(\n      \"brexit\",\n      from_date = start_date,\n      to_date = end_date\n    )\n  }) |&gt; \n  list_rbind()\n\ntoc()\nThe data need a little cleaning, for example, to remove multi-topic articles, html tags and non-breaking spaces.\ntrade_df &lt;-\n  article_df |&gt;\n  filter(!str_detect(id, \"/live/\"), \n         section_id %in% c(\"world\", \"politics\", \"business\")) |&gt;\n  mutate(\n    body = str_remove_all(body, \"&lt;.*?&gt;\") |&gt; str_to_lower(),\n    body = str_remove_all(body, \"[^a-z0-9 .-]\"),\n    body = str_remove_all(body, \"nbsp\")\n  )\nA corpus then gives me a collection of texts whereby each document is a newspaper article.\ntrade_corp &lt;- trade_df |&gt; \n  corpus(docid_field = \"short_url\", \n         text_field = \"body\", unique_docnames = FALSE)\nAlthough only articles mentioning Brexit have been imported, some of these will not be related to trade negotiations with the EU. For example, there are on-going negotiations with many countries around the world. So, word embeddings(Selivanov, Bickel, and Wang 2022) will help to narrow the focus to the specific context of the UK-EU trade deal.\nThe chief negotiator for the EU is Michel Barnier, so I’ll quantitatively identify words in close proximity to “Barnier” in the context of these Brexit news articles.\nwindow &lt;- 5\n\ntrade_fcm &lt;-\n  trade_corp |&gt;\n  fcm(context = \"window\", window = window, \n      count = \"weighted\", weights = window:1)\n\nglove &lt;- GlobalVectors$new(rank = 60, x_max = 10)\n\nset.seed(42)\n\nwv_main &lt;- glove$fit_transform(trade_fcm, n_iter = 10)\n\nINFO  [10:54:25.762] epoch 1, loss 0.3788\nINFO  [10:54:27.135] epoch 2, loss 0.2570\nINFO  [10:54:28.548] epoch 3, loss 0.2292\nINFO  [10:54:29.944] epoch 4, loss 0.2090\nINFO  [10:54:31.337] epoch 5, loss 0.1921\nINFO  [10:54:32.731] epoch 6, loss 0.1791\nINFO  [10:54:34.125] epoch 7, loss 0.1694\nINFO  [10:54:35.518] epoch 8, loss 0.1619\nINFO  [10:54:36.912] epoch 9, loss 0.1557\nINFO  [10:54:38.306] epoch 10, loss 0.1506\n\nwv_context &lt;- glove$components\nword_vectors &lt;- wv_main + t(wv_context)\n\nsearch_coord &lt;- \n  word_vectors[\"barnier\", , drop = FALSE]\n\nword_vectors |&gt; \n  sim2(search_coord, method = \"cosine\") |&gt; \n  as_tibble(rownames = NA) |&gt; \n  rownames_to_column(\"term\") |&gt; \n  rename(similarity = 2) |&gt; \n  slice_max(similarity, n = 10)\n\n\n\n\nterm\nsimilarity\n\n\n\nbarnier\n1.0000000\n\n\nnegotiator\n0.8060194\n\n\nmichel\n0.8060093\n\n\nfrost\n0.8040999\n\n\nbrussels\n0.7012591\n\n\nnegotiators\n0.6404119\n\n\nchief\n0.6399011\n\n\nfriday\n0.6363004\n\n\nteam\n0.6253523\n\n\nnegotiations\n0.6246642\nWord embedding is a learned modelling technique placing words into a multi-dimensional vector space such that contextually-similar words may be found close by. Not surprisingly, one of the closest words contextually is “Michel”. And as he is the chief negotiator for the EU, we find “negotiator” and “brussels” also in the top most contextually-similar words.\nThe word embeddings algorithm, through word co-occurrence, has identified the name of Michel Barnier’s UK counterpart David Frost. So filtering articles for “Barnier”, “Frost” and “UK-EU” should help narrow the focus.\ncontext_df &lt;- \n  trade_df |&gt; \n  filter(str_detect(body, \"barnier|frost|uk-eu\")) \n\ncontext_corp &lt;- \n  context_df |&gt; \n  corpus(docid_field = \"short_url\", text_field = \"body\")\nQuanteda’s(Benoit et al. 2018) kwic function shows key phrases in context to ensure we’re homing in on the required texts. Short URLs are included below so one can click on any to read the actual article as presented by The Guardian.\nset.seed(123)\n\ncontext_corp |&gt;\n  tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE,\n    remove_numbers = TRUE\n  ) |&gt;\n  kwic(pattern = phrase(c(\"trade negotiation\", \"trade deal\", \"trade talks\")), \n       valuetype = \"regex\", window = 7) |&gt;\n  as_tibble() |&gt;\n  left_join(article_df, by = join_by(docname == short_url)) |&gt; \n  slice_sample(n = 10) |&gt; \n  select(docname, pre, keyword, post, headline)\n\n\n\n\n\n\n\n\n\n\n\ndocname\npre\nkeyword\npost\nheadline\n\n\n\nhttps://www.theguardian.com/p/ep2yb\nput pressure on brussels to agree a\ntrade deal\nand iron out problems with the withdrawal\nBoris Johnson bows to Tory rebels with Brexit bill compromise\n\n\nhttps://www.theguardian.com/p/dag4n\nexpected from parties about to embark on\ntrade talks\neu member states are due to confirm\nBrexit deal ‘a different ball game’ to Canada agreement, warns EU\n\n\nhttps://www.theguardian.com/p/ekz7e\nhas gone down badly in brussels in\ntrade negotiations\nusually both sides work out a consolidated\nBarnier ‘flabbergasted’ at UK attempt to reopen Brexit specialty food debate\n\n\nhttps://www.theguardian.com/p/fptbj\nthe eu followed the usual pattern of\ntrade talks\ndown the ages the negotiations seemed to\nBrexit talks followed common pattern but barrier-raising outcome is unique\n\n\nhttps://www.theguardian.com/p/dq896\ntext contains a cut-and-paste from the eus\ntrade deal\nwith canada stating merely that it would\nBrexit talks: Britain accuses EU of treating UK as ‘unworthy’ partner\n\n\nhttps://www.theguardian.com/p/fmmga\nthe conservative party for years john harris\ntrade deals\nare not meant to assert sovereignty she\nEU leaders stress unity as they welcome Brexit trade talks extension\n\n\nhttps://www.theguardian.com/p/fv2xh\ndemanded a last-minute compromise to reach a\ntrade deal\nand avert chaos at the border as\nFirms plead for Brexit deal as coronavirus leaves industry reeling\n\n\nhttps://www.theguardian.com/p/f6444\ncanada-style trade deal the eu has a\ntrade deal\nwith canada called the comprehensive economic and\nWhat did Boris Johnson mean by an Australia-style system of trade?\n\n\nhttps://www.theguardian.com/p/fk5kt\ncompanies await news of a potential uk-eu\ntrade deal\nabf said our businesses have completed all\nPrimark reports ‘phenomenal’ trading since lockdowns ended\n\n\nhttps://www.theguardian.com/p/evgxe\nin talks trying to thrash out a\ntrade deal\nbefore january but after the chief negotiators\nWednesday briefing: Tory revolt over Cummings piles pressure on PM\nQuanteda provides a sentiment dictionary which, in addition to identifying positive and negative words, also finds negative-negatives and negative-positives such as, for example, “not effective”. For each week’s worth of articles, we can calculate the proportion of positive sentiments.\ntic()\n\nsent_df &lt;- \n  context_corp |&gt; \n  tokens() |&gt; \n  dfm(dictionary = data_dictionary_LSD2015) |&gt; \n  as_tibble() |&gt;\n  left_join(context_df, by = join_by(doc_id == short_url)) |&gt; \n  mutate(\n    pos = positive + neg_negative,\n    neg = negative + neg_positive,\n    web_date = date_ceiling(as_date(web_publication_date), \"week\"),\n    pct_pos = pos / (pos + neg)\n  )\n\nsent_df |&gt; \n  select(Article = doc_id, \"Pos Score\" = pos, \"Neg Score\" = neg) |&gt; \n  slice(1:10)\n\n\n\n\nArticle\nPos Score\nNeg Score\n\n\n\nhttps://www.theguardian.com/p/d6qhb\n40\n22\n\n\nhttps://www.theguardian.com/p/d9e9j\n27\n15\n\n\nhttps://www.theguardian.com/p/d6kzd\n52\n27\n\n\nhttps://www.theguardian.com/p/d79cn\n57\n51\n\n\nhttps://www.theguardian.com/p/d6t3c\n28\n26\n\n\nhttps://www.theguardian.com/p/d9vjq\n13\n23\n\n\nhttps://www.theguardian.com/p/d7n8b\n57\n35\n\n\nhttps://www.theguardian.com/p/dag4n\n37\n38\n\n\nhttps://www.theguardian.com/p/d9xtf\n33\n14\n\n\nhttps://www.theguardian.com/p/d7d9t\n23\n11\n\n\n\n\n\nsummary_df &lt;- sent_df |&gt; \n  summarise(pct_pos = mean(pct_pos), \n            n = n(),\n            .by = web_date)\n\ntoc()\n\n0.515 sec elapsed\nPlotting the changing proportion of positive sentiment over time did surprise me a little. The outcome was more balanced than I expected which perhaps confirms the deeper impression left on me by negative articles.\nThe upper plot shows a rolling 7-day mean with a narrowing ribbon representing a narrowing variation in sentiment.\nThe lower plot shows the volume of articles. As we drew closer to the crunch-point the volume picked up.\nwidth &lt;- 7\n\nsent_df2 &lt;- sent_df |&gt;\n  mutate(web_date = as_date(web_publication_date)) |&gt; \n  group_by(web_date) |&gt;\n  summarise(pct_pos = sum(pos) / sum(neg + pos)) |&gt; \n  mutate(\n    roll_mean = slide_dbl(pct_pos, mean, .before = 6),\n    roll_lq = slide_dbl(pct_pos, ~ quantile(.x, probs = 0.25), .before = 6),\n    roll_uq = slide_dbl(pct_pos, ~ quantile(.x, probs = 0.75), .before = 6)\n  )\n\np1 &lt;- sent_df2 |&gt;\n  ggplot(aes(web_date)) +\n  geom_line(aes(y = roll_mean), colour = cols[1]) +\n  geom_ribbon(aes(ymin = roll_lq, ymax = roll_uq), \n              alpha = 0.33, fill = cols[1]) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", \n             colour = cols[4], linewidth = 1) +\n  scale_y_continuous(labels = label_percent(accuracy = 1)) +\n  labs(\n    title = \"Changing Sentiment Towards a UK-EU Trade Deal\",\n    subtitle = glue(\"Rolling {width} days Since the Withdrawal Agreement\"),\n    x = NULL, y = \"Positive Sentiment\"\n  )\n\np2 &lt;- summary_df |&gt; \n  ggplot(aes(web_date, n)) +\n  geom_line(colour = cols[1]) +\n  labs(x = \"Weeks\", y = \"Article Count\",\n       caption = \"Source: Guardian Newspaper\")\n\np1 / p2 + \n  plot_layout(heights = c(2, 1))"
  },
  {
    "objectID": "project/deal/index.html#r-toolbox",
    "href": "project/deal/index.html#r-toolbox",
    "title": "A Frosty Deal?",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nMatrix\nt[1]\n\n\nbase\nas.character[1], c[3], library[14], mean[1], set.seed[2], sum[2]\n\n\nclock\nadd_days[1], add_months[1], date_build[1], date_ceiling[1]\n\n\nconflicted\nconflict_prefer[1], conflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nfilter[2], group_by[1], join_by[2], left_join[2], mutate[5], n[1], rename[1], select[2], slice[1], slice_max[1], slice_sample[1], summarise[2]\n\n\nggplot2\naes[6], annotate[1], geom_col[1], geom_hline[1], geom_label[1], geom_line[2], geom_ribbon[1], ggplot[3], labs[2], scale_fill_manual[1], scale_y_continuous[1], theme[1], theme_bw[1], theme_set[1], theme_void[1]\n\n\nglue\nglue[1]\n\n\nlubridate\nas_date[2]\n\n\nmethods\nnew[1]\n\n\nmlapi\nfit_transform[1]\n\n\npaletteer\npaletteer_d[1]\n\n\npatchwork\nplot_layout[1]\n\n\npurrr\nlist_rbind[1], pmap[1], slowly[1]\n\n\nquanteda\ncorpus[2], dfm[1], fcm[1], kwic[1], phrase[1], tokens[2]\n\n\nscales\nlabel_percent[1]\n\n\nslider\nslide_dbl[3]\n\n\nstats\nquantile[2]\n\n\nstringr\nstr_detect[2], str_remove[1], str_remove_all[3], str_to_lower[1]\n\n\ntext2vec\nsim2[1]\n\n\ntibble\nas_tibble[3], rownames_to_column[1], tibble[2]\n\n\ntictoc\ntic[2], toc[2]\n\n\nusedthese\nused_here[1]"
  },
  {
    "objectID": "project/sw10/index.html",
    "href": "project/sw10/index.html",
    "title": "House Sales",
    "section": "",
    "text": "Various events have impacted house sales in London. There has been a series of increases in stamp duty and the impact of the financial crisis. More recently Brexit and the consequences of Covid-19.\nHow is London postal area SW10 coping with all this?\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(scales)\nlibrary(SPARQL)\nlibrary(clock)\nconflict_prefer(\"date_format\", \"clock\")\nlibrary(wesanderson)\nlibrary(glue)\nlibrary(tsibble)\nlibrary(patchwork)\nlibrary(ggmosaic)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(name = \"Darjeeling1\"))\nHouse prices paid data are provided by HM Land Registry Open Data.\nendpoint &lt;- \"https://landregistry.data.gov.uk/landregistry/query\"\n\nquery &lt;- 'PREFIX  text: &lt;http://jena.apache.org/text#&gt;\nPREFIX  ppd:  &lt;http://landregistry.data.gov.uk/def/ppi/&gt;\nPREFIX  lrcommon: &lt;http://landregistry.data.gov.uk/def/common/&gt;\n  \nSELECT  ?item ?ppd_propertyAddress ?ppd_hasTransaction ?ppd_pricePaid ?ppd_transactionCategory ?ppd_transactionDate ?ppd_transactionId ?ppd_estateType ?ppd_newBuild ?ppd_propertyAddressCounty ?ppd_propertyAddressDistrict ?ppd_propertyAddressLocality ?ppd_propertyAddressPaon ?ppd_propertyAddressPostcode ?ppd_propertyAddressSaon ?ppd_propertyAddressStreet ?ppd_propertyAddressTown ?ppd_propertyType ?ppd_recordStatus\n\nWHERE\n{ ?ppd_propertyAddress text:query _:b0 .\n  _:b0 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; lrcommon:postcode .\n  _:b0 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; _:b1 .\n  _:b1 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; \"( SW10 )\" .\n  _:b1 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; _:b2 .\n  _:b2 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; 3000000 .\n  _:b2 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#nil&gt; .\n  ?item ppd:propertyAddress ?ppd_propertyAddress .\n  ?item ppd:hasTransaction ?ppd_hasTransaction .\n  ?item ppd:pricePaid ?ppd_pricePaid .\n  ?item ppd:transactionCategory ?ppd_transactionCategory .\n  ?item ppd:transactionDate ?ppd_transactionDate .\n  ?item ppd:transactionId ?ppd_transactionId\n  \n  OPTIONAL { ?item ppd:estateType ?ppd_estateType }\n  OPTIONAL { ?item ppd:newBuild ?ppd_newBuild }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:county ?ppd_propertyAddressCounty }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:district ?ppd_propertyAddressDistrict }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:locality ?ppd_propertyAddressLocality }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:paon ?ppd_propertyAddressPaon }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:postcode ?ppd_propertyAddressPostcode }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:saon ?ppd_propertyAddressSaon }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:street ?ppd_propertyAddressStreet }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:town ?ppd_propertyAddressTown }\n  OPTIONAL { ?item ppd:propertyType ?ppd_propertyType }\n  OPTIONAL { ?item ppd:recordStatus ?ppd_recordStatus }\n}'\n\ndata_lst &lt;- SPARQL(endpoint, query)\nThe focus is on the standard price paid.\ndata_df &lt;- data_lst$results |&gt;\n  as_tibble() |&gt;\n  mutate(\n    trans_date = as_datetime(ppd_transactionDate) |&gt; as_date(),\n    amount = ppd_pricePaid,\n    prop_type = str_extract(ppd_propertyType, \"(?&lt;=common/)[\\\\w]+\"),\n    est_type = str_extract(ppd_estateType, \"(?&lt;=common/)[\\\\w]+\"),\n    cat = str_remove(ppd_transactionCategory, \"&lt;http://landregistry.data.gov.uk/def/ppi/\"),\n    prop_type = recode(prop_type, otherPropertyType = \"Other\")\n  ) |&gt;\n  filter(str_detect(cat, \"standard\"))\nA Telegraph article entitled Timeline: 20 years of stamp duty increases for home buyers pinpoints many of the key event dates.\nevents &lt;- tribble(\n  ~event_date, ~change,\n  \"96-07-31\", \"Stamp Duty £250k (1.5%) £500k (2%)\",\n  \"98-03-31\", \"£250k (2%) £500k (3%)\",\n  \"99-03-31\", \"£250k (2.5%) £500k (3.5%)\",\n  \"00-03-31\", \"£250k (3%) £500k (4%)\",\n  \"11-04-30\", \"£250k (3%) £500k (4%) £1m (5%)\",\n  \"12-03-31\", \"£250k (3%) £500k (4%) £1m (5%) £2m (7%)\",\n  \"14-12-31\", \"£250k (5%) £925k (10%) 1.5m (12%)\",\n  \"07-08-09\", \"Financial Crisis\",\n  \"16-06-23\", \"Brexit Vote\",\n  \"20-03-23\", \"Covid-19 Lockdown\"\n) |&gt;\n  mutate(event_date = date_parse(event_date, format = \"%y-%m-%d\"))\n\nevents |&gt; \n  rename(\"Date\" = event_date, \"Event\" = change)\n\n\n\n\nDate\nEvent\n\n\n\n1996-07-31\nStamp Duty £250k (1.5%) £500k (2%)\n\n\n1998-03-31\n£250k (2%) £500k (3%)\n\n\n1999-03-31\n£250k (2.5%) £500k (3.5%)\n\n\n2000-03-31\n£250k (3%) £500k (4%)\n\n\n2011-04-30\n£250k (3%) £500k (4%) £1m (5%)\n\n\n2012-03-31\n£250k (3%) £500k (4%) £1m (5%) £2m (7%)\n\n\n2014-12-31\n£250k (5%) £925k (10%) 1.5m (12%)\n\n\n2007-08-09\nFinancial Crisis\n\n\n2016-06-23\nBrexit Vote\n\n\n2020-03-23\nCovid-19 Lockdown\nVisually, it appears that the financial crisis had a big impact on sales volume, with the Brexit vote sucking much of the remaining oxygen out of the market. Stamp duty increases in between probably slowed any intermediate recovery.\nto_date &lt;- data_df |&gt; \n  summarise(max(trans_date)) |&gt; \n  pull() |&gt; \n  date_format(format = \"%b %d, %Y\")\n\ndata_df |&gt;\n  ggplot(aes(trans_date, amount, colour = est_type)) +\n  geom_point(alpha = 0.2, size = 0.7, show.legend = FALSE) +\n  geom_smooth(se = FALSE, aes(linetype = est_type), size = 1.2) +\n  labs(\n    title = \"SW10 Standard House Prices\",\n    subtitle = glue(\"Prices Paid to {to_date} (Prices &gt; £5m Not Shown)\"\n    ),\n    x = NULL,\n    y = NULL,\n    colour = \"Type\", linetype = \"Type\",\n    caption = \"Source: HM Land Registry\"\n  ) +\n  geom_vline(xintercept = events$event_date, \n             size = 0.5, lty = 2, alpha = 0.4) +\n  annotate(\"text\", events$event_date, 5000000,\n    angle = 90,\n    label = events$change, vjust = 1.4, hjust = 1, size = 3, fontface = 2\n  ) +\n  coord_cartesian(ylim = c(0, 5000000)) +\n  scale_colour_manual(values = cols[c(2, 3)]) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = label_dollar(accuracy = 0.1, prefix = \"£\", \n                                           scale_cut = cut_short_scale()))\nAn alternative way of looking at this is by median quarterly prices (with upper and lower quartiles), supplemented by sales volumes.\nqtr_start &lt;- date_today(\"Europe/London\") |&gt; \n  as_year_quarter_day() |&gt; \n  calendar_start(\"quarter\") |&gt; \n  as_date()\n\nqtile_df &lt;- \n  data_df |&gt; \n  filter(trans_date &lt; qtr_start) |&gt; \n  mutate(yr_qtr = yearquarter(trans_date)) |&gt; \n  reframe(price = quantile(amount, c(0.25, 0.5, 0.75)), \n            quantile = c(\"lower\", \"median\", \"upper\") |&gt; factor(),\n            n = n(),\n            .by = yr_qtr) |&gt; \n  pivot_wider(names_from = quantile, values_from = price)\n\nlast &lt;- qtile_df |&gt; summarise(max(yr_qtr)) |&gt; pull()\nfirst &lt;- qtile_df |&gt; summarise(min(yr_qtr)) |&gt; pull()\n\np1 &lt;- qtile_df |&gt; \n  ggplot(aes(yr_qtr, median)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = cols[5]) +\n  geom_line(colour = \"white\") +\n  geom_hline(yintercept = 1000000, linetype = \"dashed\") +\n  annotate(\"text\", x = yearquarter(\"2020 Q2\"), y = 300000, \n           label = \"Covid-19\\nLockdown\", size = 3) +\n  scale_x_yearquarter(date_breaks = \"2 years\") +\n  scale_y_log10(labels = label_dollar(prefix = \"£\", \n                                      scale_cut = cut_short_scale())) +\n  labs(\n    title = glue(\"Median Quarterly SW10 Property Prices ({first} to {last})\"), \n    subtitle = \"With Upper / Lower Price Quartiles & Sales Volume\",\n    x = NULL, y = \"Price (Log10)\"\n    ) +\n  theme(axis.text.x = element_blank())\n\np2 &lt;- qtile_df |&gt; \n  ggplot(aes(yr_qtr, n)) +\n  geom_line() +\n  annotate(\"text\", x = yearquarter(\"2007 Q3\"), y = 180, \n           label = \"Financial\\nCrisis\", size = 3) +\n  annotate(\"text\", x = yearquarter(\"2016 Q3\"), y = 130, \n           label = \"Brexit\\nVote\", size = 3) +\n  scale_x_yearquarter(date_breaks = \"2 years\") +\n  labs(x = NULL, y = \"Transactions\",\n       caption = \"Source: HM Land Registry\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np1 / p2 + plot_layout(heights = c(2, 1))\nA ggmosaic (Jeppson, Hofmann, and Cook 2021) visualisation of the composition of SW10 reveals the postal area to be overwhelmingly dominated by leasehold flats.\nnum_trans &lt;- data_df |&gt; nrow()\n\ndata_df |&gt; \n  ggplot() +\n  geom_mosaic(aes(product(prop_type, est_type), fill = prop_type), \n              offset = 0.02, divider = mosaic(\"h\")) +\n  scale_fill_manual(values = cols[c(2:5)]) + \n  labs(\n    title = \"SW10 Transactions by Estate & Property Types\",\n    subtitle = glue(\"{comma(num_trans)} Transactions to {to_date}\"),\n    x = \"\", y = \"\", fill = \"Property Type\", \n    caption = \"Source: HM Land Registry\"\n  ) +\n  theme_minimal()\nOther blog posts on quantum jitter look at SW10 property from different perspectives: Digging Deep considers the correlation between house sales and planning applications; and Bootstraps & Bandings uses a sample of recent house sales to infer whether property bands are as representative of property values today as they were three decades ago."
  },
  {
    "objectID": "project/sw10/index.html#r-toolbox",
    "href": "project/sw10/index.html#r-toolbox",
    "title": "House Sales",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nSPARQL\nSPARQL[1]\n\n\nbase\nc[6], factor[1], library[11], max[2], min[1], nrow[1]\n\n\nclock\nas_year_quarter_day[1], calendar_start[1], date_format[1], date_parse[1], date_today[1]\n\n\nconflicted\nconflict_prefer[2], conflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nfilter[2], mutate[3], n[1], pull[3], recode[1], reframe[1], rename[1], summarise[3]\n\n\nggmosaic\ngeom_mosaic[1], mosaic[1], product[1]\n\n\nggplot2\naes[6], annotate[4], coord_cartesian[1], element_blank[1], element_text[1], geom_hline[1], geom_line[2], geom_point[1], geom_ribbon[1], geom_smooth[1], geom_vline[1], ggplot[4], labs[4], scale_colour_manual[1], scale_fill_manual[1], scale_x_date[1], scale_y_continuous[1], scale_y_log10[1], theme[2], theme_bw[1], theme_minimal[1], theme_set[1]\n\n\nglue\nglue[3]\n\n\nlubridate\nas_date[2], as_datetime[1]\n\n\npatchwork\nplot_layout[1]\n\n\nscales\ncomma[1], cut_short_scale[2], label_dollar[2]\n\n\nstats\nquantile[1]\n\n\nstringr\nstr_detect[1], str_extract[2], str_remove[1]\n\n\ntibble\nas_tibble[1], tribble[1]\n\n\ntidyr\npivot_wider[1]\n\n\ntsibble\nscale_x_yearquarter[2], yearquarter[4]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/box/index.html",
    "href": "project/box/index.html",
    "title": "Favourite Things",
    "section": "",
    "text": "Each project closes with a table summarising the R tools used. By aggregating the package and function usage across all projects, there’s an opportunity to:\nSince starting this in 2017, functions like tidyr’s spread and gather have been superseded by pivot_wider and pivot_longer. Newer packages have emerged like tidyclust, which brings cluster modelling to tidymodels (now used in Finding Happiness in ‘The Smoke’). bslib has brought improvements to the latest shiny app version embedded in Plots Thicken. The paletteer package has put it’s arms around the myriad palette packages out there. And scales’ cut_short_scale assisted with plot labelling.\nMost recently, the latest versions of dplyr and purrr have presented a host of enhancements. For example, the .by argument in mutate and friends offers a neat alternative to group_by and ungroup in many situations. Joins have been enhanced (e.g. to support inequality conditions) with the addition of join_by to dplyr. And changes in the map_ family introduce list_rbind and associates with map_dfr, for example, now superseded.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nlibrary(tidytext)\nlibrary(rvest)\nlibrary(paletteer)\nlibrary(janitor)\nlibrary(glue)\nlibrary(ggwordcloud)\nlibrary(patchwork)\nlibrary(clock)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\nn &lt;- 4\npalette &lt;- \"harrypotter::mischief\"\n\ncols &lt;- paletteer_c(palette, n = n)\n\ntibble(x = 1:n, y = 1) |&gt;\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols |&gt; str_remove(\"FF$\")), \n             size = 4, vjust = 2, fill = \"white\") +\n  annotate(\n    \"label\",\n    x = (n + 1) / 2, y = 0.5,\n    label = palette,\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\nSeparation of tidyverse and non-tidyverse packages may be achieved by using the likes of tidyverse_packages which lists all packages in the tidyverse.\ntidy &lt;-\n  c(\n    tidyverse::tidyverse_packages(),\n    fpp3::fpp3_packages(),\n    tidymodels::tidymodels_packages()\n  ) |&gt;\n  unique()\nused_here() has already been used in Quantum Jitter projects to generate a usage table at the foot of each project with the CSS class usedthese. used_there() may now be used to web-scrape all the tables with this class to aggregate package and function usage data.\nbase_packages &lt;- c(\n  \"stats\",\n  \"graphics\",\n  \"grDevices\",\n  \"utils\",\n  \"datasets\",\n  \"methods\",\n  \"base\"\n)\n\nused_df &lt;-\n  used_there(\"https://www.quantumjitter.com/project/\") |&gt;\n  mutate(multiverse = case_match(\n    Package,\n    tidy ~ \"tidy\",\n    base_packages ~ \"base\",\n    .default = \"special\"\n  ))\n\nn_url &lt;- used_df |&gt; summarise(n_distinct(url)) |&gt; pull()\n\npack_df &lt;- used_df |&gt;\n  count(Package, multiverse, wt = n) |&gt;\n  mutate(name = \"package\")\n\nfun_df &lt;- used_df |&gt;\n  count(Function, multiverse, wt = n) |&gt;\n  mutate(name = \"function\")\n\npackfun_df &lt;- pack_df |&gt;\n  bind_rows(fun_df) |&gt;\n  arrange(desc(n)) |&gt;\n  mutate(\n    packfun = coalesce(Package, Function),\n    name = fct_rev(name),\n    .by = name\n  )\nWith the latest versions of dplyr and purrr, usage of group_by, ungroup and map_dfr has fallen away whilst usage of list_rbind and join_by has freshly emerged.\np1 &lt;- packfun_df |&gt;\n  filter(name == \"package\") |&gt; \n  ggplot(aes(fct_reorder(packfun, n), n, fill = multiverse)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  geom_label(aes(label = n), hjust = \"inward\", size = 2, fill = \"white\") +\n  scale_fill_manual(values = cols[c(4, 3, 2)]) +\n  labs(x = NULL, y = NULL, \n       subtitle = glue(\"Package Usage\"))\n\nmin_n &lt;- 4\n\np2 &lt;- packfun_df |&gt;\n  filter(name == \"function\", n &gt;= min_n) |&gt; \n  ggplot(aes(fct_reorder(packfun, n), n, fill = multiverse)) +\n  geom_col() +\n  coord_flip() +\n  geom_label(aes(label = n), hjust = \"inward\", size = 2, fill = \"white\") +\n  scale_fill_manual(values = cols[c(4, 3, 2)]) +\n  labs(x = NULL, y = NULL, \n       subtitle = glue(\"Function Usage &gt;= {min_n}\"))\n\np1 + p2 +\n  plot_annotation(\n    title = glue(\"Favourite Things\"),\n    subtitle = glue(\n      \"Used Across {n_url} Projects as at \",\n      \"{date_format(date_today('Europe/London'), format = '%B %d, %Y')}\"\n    )\n  )\nThis last code chunk generates the word cloud for use as the feature image for this project.\nset.seed = 123\n\npackfun_df |&gt;\n  mutate(angle = 45 * sample(-2:2, n(), \n                             replace = TRUE, \n                             prob = c(1, 1, 4, 1, 1))) |&gt;\n  ggplot(aes(\n    label = packfun,\n    size = n,\n    colour = multiverse,\n    angle = angle\n  )) +\n  geom_text_wordcloud(\n    eccentricity = 1,\n    grid_margin = 0.95,\n    seed = 789\n  ) +\n  scale_size_area(max_size = 20) +\n  scale_colour_manual(values = cols[c(2, 3, 4)]) +\n  theme_void() +\n  theme(plot.background = element_rect(fill = cols[1]))"
  },
  {
    "objectID": "project/box/index.html#r-toolbox",
    "href": "project/box/index.html#r-toolbox",
    "title": "Favourite Things",
    "section": "R Toolbox",
    "text": "R Toolbox\nThis project’s code too should be included in my “favourite things”.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[1], c[6], library[11], sample[1], unique[1]\n\n\nclock\ndate_format[1], date_today[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\narrange[1], bind_rows[1], case_match[1], coalesce[1], count[2], desc[1], filter[2], mutate[5], n[1], n_distinct[1], pull[1], summarise[1]\n\n\nforcats\nfct_reorder[2], fct_rev[1]\n\n\nggplot2\naes[7], annotate[1], coord_flip[2], element_rect[1], geom_col[3], geom_label[3], ggplot[4], labs[2], scale_colour_manual[1], scale_fill_manual[3], scale_size_area[1], theme[2], theme_bw[1], theme_set[1], theme_void[2]\n\n\nggwordcloud\ngeom_text_wordcloud[1]\n\n\nglue\nglue[4]\n\n\npaletteer\npaletteer_c[1]\n\n\npatchwork\nplot_annotation[1]\n\n\nstringr\nstr_remove[1]\n\n\ntibble\ntibble[1]\n\n\ntidymodels\ntidymodels_packages[1]\n\n\ntidyverse\ntidyverse_packages[1]\n\n\nusedthese\nused_here[1], used_there[1]"
  },
  {
    "objectID": "project/thicken/index.html",
    "href": "project/thicken/index.html",
    "title": "Plots Thicken",
    "section": "",
    "text": "One could think of data science as “art grounded in facts”. It tells a story through visualisation. Both story and visualisation rely on a good plot. And an abundance of those has evolved over time. Many have their own dedicated Wikipedia page .\nWhich generate the most interest? How is the interest trending over time? Let’s build an interactive app to find out.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(shiny)\nlibrary(gridlayout)\nlibrary(rvest)\nlibrary(scales)\nlibrary(pageviews)\nlibrary(lubridate)\nlibrary(bslib)\nlibrary(wesanderson)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(8, name = \"IsleofDogs1\", type = \"continuous\"))\nI’m going to start by harvesting some data from Wikipedia’s Statistical charts and diagrams category. I can use this to build a list of all chart types which have a dedicated Wikipedia article page. Using rvest (Wickham 2022) inside the app ensures it will respond to any newly-created articles.\ncharts &lt;-\n  tibble(\n    chart = read_html(str_c(\n      \"https://en.wikipedia.org/wiki/\",\n      \"Category:Statistical_charts_and_diagrams\"\n    )) |&gt;\n      html_elements(\".mw-category-group a\") |&gt;\n      html_text()\n  )\nThe pageviews (Keyes and Lewis 2020) package provides an API into Wikipedia. I’ll create a function wrapped around article_pageviews so I can later iterate through a subset of the list established in the prior code chunk.\npv &lt;- \\(article) {\n  article_pageviews(\n    project = \"en.wikipedia\",\n    article,\n    user_type = \"user\",\n    start = \"2015070100\",\n    end = today()\n  )\n}\nI want an input selector so that a user can choose plot types for comparison. I also want to provide user control of the y-axis scale. A combination of fixed and log10 is better for comparing plots. Free scaling reveals more detail in the individual trends.\nAlthough shinyuieditor (Strayer 2022) is currently in Alpha at the time of this update, using launch_editor(\"/content/project/thicken/\") helped me modify the basic grid layout of the UI for this pre-existing app.R.\nui &lt;- grid_page(\n  theme = bs_theme(version = 5, bootswatch = \"simplex\"),\n  layout = c(\n    \"header  header\",\n    \"sidebar line \"\n  ),\n  row_sizes = c(\n    \"100px\",\n    \"1fr\"\n  ),\n  col_sizes = c(\n    \"250px\",\n    \"1fr\"\n  ),\n  gap_size = \"1rem\",\n  grid_card(\n    area = \"sidebar\",\n    item_alignment = \"top\",\n    title = \"Options\",\n    item_gap = \"13px\",\n    dateRangeInput(\"dates\",\n      label = \"Date range\",\n      start = \"2015-07-01\",\n      end = NULL\n    ),\n    selectizeInput(\n      inputId = \"article\",\n      label = \"Chart type\",\n      choices = charts,\n      selected = c(\n        \"Violin plot\",\n        \"Dendrogram\",\n        \"Histogram\",\n        \"Pie chart\"\n      ),\n      options = list(maxItems = 8),\n      multiple = TRUE\n    ),\n    selectInput(\n      inputId = \"scales\",\n      label = \"Fixed or free y-axis\",\n      choices = c(\"Fixed\" = \"fixed\", \"Free\" = \"free\"),\n      selected = \"fixed\"\n    ),\n    selectInput(\n      inputId = \"log10\",\n      label = \"Log 10 or normal y-axis\",\n      choices = c(\"Log 10\" = \"log10\", \"Normal\" = \"norm\"),\n      selected = \"log10\"\n    )\n  ),\n  grid_card_text(\n    area = \"header\",\n    content = \"   Plot Plotter   \",\n    alignment = \"center\",\n    is_title = FALSE,\n    icon = \"logo1.png\",\n    img_height = 30\n  ),\n  grid_card_plot(area = \"line\")\n)\nThe server component outputs a faceted ggplot.\nserver &lt;- \\(input, output, session) {\n  subsetr &lt;- reactive({\n    req(input$article)\n    pageviews &lt;- map(input$article, pv) |&gt;\n      mutate(\n        date = ymd(date),\n        article = str_replace_all(article, \"_\", \" \")\n      ) |&gt;\n      filter(date &gt;= input$dates[1], date &lt;= input$dates[2])\n  }) |&gt; \n    list_rbind()\n\n  output$line &lt;- renderPlot({\n    p &lt;- ggplot(\n      subsetr(),\n      aes(date,\n        views,\n        colour = article\n      )\n    ) +\n      geom_line() +\n      scale_colour_manual(values = cols) +\n      geom_smooth(colour = cols[7]) +\n      facet_wrap(~article, nrow = 1, scales = input$scales) +\n      theme(\n        legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.margin = margin(1, 1, 1, 1, \"cm\")\n      ) +\n      labs(\n        x = NULL, y = NULL,\n        caption = \"\\nSource: Daily Wikipedia Article Page Views\"\n      )\n\n    switch(input$log10,\n      norm = p,\n      log10 = p + scale_y_log10(\n        labels = label_number(scale_cut = cut_short_scale())\n      )\n    )\n  })\n}\n\nshinyApp(ui, server)\nAnd here’s the live shiny (Chang et al. 2022) app deployed via shinyapps.io.\nNote the utility of selecting the right scaling. The combination of “fixed” and “normal” reveals what must have been “world histogram day” on July 27th 2015, but little else.\nTurning non-interactive code into an app sharpens the mind’s focus on performance. And profvis (Chang, Luraschi, and Mastny 2020), integrated into RStudio via the profile menu option, is a wonderful “tool for helping you understand how R spends its time”.\nMy first version of the app was finger-tappingly slow.\nProfvis revealed the main culprit to be the pre-loading of a dataframe with the page-view data for all chart types (there are more than 100). Profiling prompted the more efficient “reactive” approach of loading the data only for the user’s selection (maximum of 8).\nProfiling also showed that rounding the corners of the plot.background with additional grid-package code was expensive. App efficiency felt more important than minor cosmetic detailing (to the main panel to match the theme’s side panel). And most users would probably barely notice (had I not drawn attention to it here)."
  },
  {
    "objectID": "project/thicken/index.html#r-toolbox",
    "href": "project/thicken/index.html#r-toolbox",
    "title": "Plots Thicken",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[6], library[11], list[1], switch[1]\n\n\nbslib\nbs_theme[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nfilter[1], mutate[1]\n\n\nggplot2\naes[1], element_text[1], facet_wrap[1], geom_line[1], geom_smooth[1], ggplot[1], labs[1], margin[1], scale_colour_manual[1], scale_y_log10[1], theme[1], theme_bw[1], theme_set[1]\n\n\ngridlayout\ngrid_card[1], grid_card_plot[1], grid_card_text[1], grid_page[1]\n\n\nlubridate\ntoday[1], ymd[1]\n\n\npageviews\narticle_pageviews[1]\n\n\npurrr\nlist_rbind[1], map[1]\n\n\nrvest\nhtml_elements[1], html_text[1]\n\n\nscales\ncut_short_scale[1], label_number[1]\n\n\nshiny\ndateRangeInput[1], reactive[1], renderPlot[1], req[1], selectInput[2], selectizeInput[1], shinyApp[1]\n\n\nstringr\nstr_c[1], str_replace_all[1]\n\n\ntibble\ntibble[1]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[1]\n\n\nxml2\nread_html[1]"
  },
  {
    "objectID": "project/planning/index.html",
    "href": "project/planning/index.html",
    "title": "Digging Deep",
    "section": "",
    "text": "In House Sales I looked at how a series of events damped down sales. By combining these sales data with planning applications I’d like to see if home owners “start digging” when they can’t sell.\nPlanning data is harvested with the kind permission of The Royal Borough of Kensington and Chelsea (RBKC). The code for these code chunks is not rendered out of courtesy to RBKC.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(rvest)\nlibrary(SPARQL)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(wesanderson)\nlibrary(tictoc)\nlibrary(htmlwidgets)\nlibrary(clock)\nlibrary(fabletools)\nlibrary(feasts)\nlibrary(tsibble)\nlibrary(DT)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(name = \"Darjeeling2\"))\ncase_df &lt;- readRDS(\"case.rds\")\nurl &lt;- \n  \"https://www.freemaptools.com/download/full-postcodes/ukpostcodes.zip\"\n\nfile_name &lt;- basename(url)\n\nurl |&gt; basename\n\ndownload.file(url, file_name)\ngeocodes &lt;- read_csv(\"ukpostcodes.zip\")\nThe data need a bit of wrangling. And there is also the opportunity to try the newest column-wise enhancements to mutate: mutate_if and mutate_at have been superseded by mutate with across.\nwide_df &lt;- case_df |&gt;\n  pivot_wider(names_from = X1, values_from = X2) |&gt;\n  select(all_of(plan_colnames)) |&gt;\n  mutate(\n    across(c(property_list, property_cons), \\(vec) na_if(vec, \"N/A\")),\n    across(c(app_comp, decision), \\(vec) na_if(vec, \"\"))\n  )\n\ntidy_df &lt;- wide_df |&gt;\n  mutate(\n    dec_date = date_parse(dec_date, format = \"%d %b %Y\"),\n    dec_year = get_year(dec_date),\n    proposal_dev = str_to_lower(proposal_dev),\n    property_pcode = str_extract(property_add, \"SW10[\\\\s]?\\\\d[[:alpha:]]{2}\"),\n    property_pcode = str_replace(property_pcode, \"SW10(?!\\\\s)\", \"SW10 \"),\n    app_comp = str_to_upper(app_comp) |&gt;\n      str_remove_all(\"[:punct:]\") |&gt;\n      str_remove_all(\"\\\\b(?:AND|LTD|CO|LIMITED|UK|GROUP|LLP)\\\\b\") |&gt;\n      str_squish(),\n    decision = fct_explicit_na(decision, na_level = \"Other\"),\n    decision = str_replace(decision, \"/\", \" / \"),\n    dec_lump = fct_lump(decision, prop = 0.03),\n    basement = if_else(str_detect(proposal_dev, \"basement\"), \"Yes\", \"No\"),\n    property_listed = case_match(\n      property_list,\n      c(\"II\", \"II*\", \"2\", \"2*\") ~ \"Yes\",\n      .default = \"No\"\n    ),\n    app_comp = replace_na(app_comp, \"None\"),\n    property_cons = if_else(property_cons == \"\" | is.na(property_cons),\n      \"None\", property_cons\n    ),\n    proposal_dev = if_else(proposal_dev == \"\" | is.na(proposal_dev),\n      \"None\", proposal_dev\n    ),\n    across(where(is.character), str_trim),\n    across(c(\"app_comp\", \"proposal_type\", \"property_cons\"), factor)\n  ) |&gt;\n  left_join(geocodes, by = join_by(property_pcode == postcode))\n\ntidy_df |&gt;\n  count(dec_lump) |&gt;\n  arrange(desc(n)) |&gt;\n  rename(\"Decision\" = dec_lump, \"Count\" = n)\n\n\n\n\nDecision\nCount\n\n\n\nGrant Planning Permission / Consent\n5336\n\n\nWithdrawn by Applicant\n1123\n\n\nOther\n847\n\n\nRefuse Planning Permission / Consent\n752\n\n\nDischarge of Conditions - Grant\n626\n\n\nRaise No Objection\n418\nquanteda (Benoit et al. 2018) to look at key words in context (kwic).\nI’d like to review planning applications by theme. So I’ll first need to get a sense of what the themes are by plotting the words which appear most frequently.\nplus_words &lt;-\n  c(\"new\",\n    \"pp\",\n    \"two\",\n    \"one\",\n    \"dated\",\n    \"withdrawn\",\n    \"flat\",\n    \"x\",\n    \"permission\",\n    \"rear\",\n    \"first\",\n    \"second\",\n    \"planning\",\n    \"floor\",\n    \"erection\"\n  )\n\nwords &lt;- tidy_df |&gt; \n  corpus(text_field = \"proposal_dev\", \n         doc_vars = c(\"dec_date\", \"proposal_type\", \n                      \"decision\", \"dec_year\")) |&gt; \n  dfm(\n    remove = c(stopwords(\"english\"), plus_words),\n    remove_numbers = TRUE,\n    remove_punct = TRUE) |&gt; \n  textstat_frequency() |&gt; \n  slice_head(n = 30) |&gt; \n  mutate(feature = fct_reorder(feature, frequency))\n\nwords |&gt; \n  ggplot(aes(feature, frequency)) +\n  geom_col(fill = cols[4]) +\n  coord_flip() +\n  labs(x = NULL, y = NULL, \n       title = \"Frequent Planning Proposal Words\",\n       caption = \"Source: RBKC Planning Search\")\nNow I can create a theme feature.\nremapped_df &lt;- tidy_df |&gt;\n  mutate(\n    theme = case_when(\n      str_detect(proposal_dev, \"basem|excav\") ~ \"Basement or Excavation\",\n      str_detect(proposal_dev, \"xten|vatory|torey\") ~ \"Extension, Conservatory \\nor Storey\",\n      str_detect(proposal_dev, \"windo|doo\") ~ \"Windows or Doors\",\n      str_detect(proposal_dev, \"roof\") ~ \"Roof\",\n      str_detect(proposal_dev, \"rrac|dsc|garde\") | \n        str_detect(proposal_type, \"Tree\") ~ \"Trees, Landscaping, \\nGarden or Terrace\",\n      .default = \"Other\"\n    ),\n    outcome = case_when(\n      str_detect(decision, \"Gran|No Ob|Accep|Lawf\") ~ \"Positive\",\n      str_detect(decision, \"Refus\") ~ \"Refuse\",\n      str_detect(decision, \"Withdr\") ~ \"Withdrawn\",\n      .default = \"Other\"\n    )\n  )\nI also want to compare house sales with planning applications over time. So, I’ll re-use the SPARQL query from House Sales.\ntic()\n\nendpoint &lt;- \"https://landregistry.data.gov.uk/landregistry/query\"\n\nquery &lt;- 'PREFIX  text: &lt;http://jena.apache.org/text#&gt;\nPREFIX  ppd:  &lt;http://landregistry.data.gov.uk/def/ppi/&gt;\nPREFIX  lrcommon: &lt;http://landregistry.data.gov.uk/def/common/&gt;\n  \nSELECT  ?item ?ppd_propertyAddress ?ppd_hasTransaction ?ppd_pricePaid ?ppd_transactionCategory ?ppd_transactionDate ?ppd_transactionId ?ppd_estateType ?ppd_newBuild ?ppd_propertyAddressCounty ?ppd_propertyAddressDistrict ?ppd_propertyAddressLocality ?ppd_propertyAddressPaon ?ppd_propertyAddressPostcode ?ppd_propertyAddressSaon ?ppd_propertyAddressStreet ?ppd_propertyAddressTown ?ppd_propertyType ?ppd_recordStatus\n\nWHERE\n{ ?ppd_propertyAddress text:query _:b0 .\n  _:b0 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; lrcommon:postcode .\n  _:b0 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; _:b1 .\n  _:b1 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; \"( SW10 )\" .\n  _:b1 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; _:b2 .\n  _:b2 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; 3000000 .\n  _:b2 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#nil&gt; .\n  ?item ppd:propertyAddress ?ppd_propertyAddress .\n  ?item ppd:hasTransaction ?ppd_hasTransaction .\n  ?item ppd:pricePaid ?ppd_pricePaid .\n  ?item ppd:transactionCategory ?ppd_transactionCategory .\n  ?item ppd:transactionDate ?ppd_transactionDate .\n  ?item ppd:transactionId ?ppd_transactionId\n  \n  OPTIONAL { ?item ppd:estateType ?ppd_estateType }\n  OPTIONAL { ?item ppd:newBuild ?ppd_newBuild }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:county ?ppd_propertyAddressCounty }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:district ?ppd_propertyAddressDistrict }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:locality ?ppd_propertyAddressLocality }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:paon ?ppd_propertyAddressPaon }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:postcode ?ppd_propertyAddressPostcode }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:saon ?ppd_propertyAddressSaon }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:street ?ppd_propertyAddressStreet }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:town ?ppd_propertyAddressTown }\n  OPTIONAL { ?item ppd:propertyType ?ppd_propertyType }\n  OPTIONAL { ?item ppd:recordStatus ?ppd_recordStatus }\n}'\n\nsales &lt;- SPARQL(endpoint, query)\n\ntoc()\n\n196.543 sec elapsed\nLet’s now bind the data into one tibble and summarise the transaction volumes over time.\nsales_df &lt;- sales$results |&gt; \n  as_tibble() |&gt;\n  mutate(\n    date = as_datetime(ppd_transactionDate) |&gt; as_date(),\n    dataset = \"Sales\"\n  ) |&gt;\n  summarise(volume = n(), .by = c(date, dataset))\n\napp_df &lt;- remapped_df |&gt;\n  mutate(\n    date = dec_date,\n    dataset = \"Planning\"\n  ) |&gt;\n  summarise(volume = n(), .by = c(date, dataset))\n\ncompare_df &lt;- bind_rows(app_df, sales_df)\n\nsummary_df &lt;- compare_df |&gt;\n  filter(date &gt;= min(sales_df$date)) |&gt; \n  mutate(date = date_build(get_year(date), get_month(date), \"last\")) |&gt; \n  summarise(volume = sum(volume), .by = c(date, dataset))\nThe visualisation below does suggest that home owners “start digging” when they can’t sell. At least in this part of London.\nmonthly_ts &lt;- summary_df |&gt; \n  mutate(date = yearmonth(date)) |&gt; \n  as_tsibble(key = dataset, index = date)\n\nmonthly_ts |&gt; \n  ggplot(aes(date, volume, colour = dataset)) +\n  geom_line(key_glyph = \"timeseries\") +\n  scale_colour_manual(values = cols[c(2, 3)]) +\n  labs(x = NULL, y = NULL, colour = NULL,\n       title = \"Monthly Property Transaction Volume in SW10\",\n       caption = \"Sources: Land Registry & RBKC Planning\"\n       )\nTime-series data may have an underlying trend and a seasonality pattern. I’ll use the seasonal package to decompose each time-series. Each exhibit annual seasonality which evolves over time.\nmonthly_ts |&gt;\n  model(stl = STL(volume ~ season())) |&gt;\n  components() |&gt; \n  autoplot() +\n  scale_colour_manual(values = cols[c(2, 3)]) +\n  labs(x = NULL, title = \"Time Series Decomposition\")\nWe also see some inverse correlation between the two time-series re-affirming the visual conclusion that planning applications increase when the housing market is depressed.\nmonthly_ts |&gt; \n  pivot_wider(names_from = dataset, values_from = volume) |&gt;\n  CCF(Sales, Planning, lag_max = 6) |&gt; \n  autoplot() +\n  labs(title = \"Correlation Between Sales & Planning\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nThe overall volumes of planning applications and house transactions in SW10 are fairly similar.\nsummary_df |&gt;\n  summarise(total = sum(volume), .by = dataset) |&gt; \n  rename(\"Dataset\" = dataset, \"Count\" = total)\n\n\n\n\nDataset\nCount\n\n\n\nPlanning\n8926\n\n\nSales\n11764\nEarlier, I added a “theme” feature to the data. So let’s take a look at the volume of applications over time faceted by theme and coloured by the outcome. We see that the rise in planning applications is fuelled by basements or excavations, and work on outside landscaping and terracing. So perhaps we do “dig” when we can’t sell.\nremapped_df |&gt;\n  ggplot(aes(dec_year, fill = outcome)) +\n  geom_bar() +\n  facet_wrap( ~ theme, nrow = 2) +\n  scale_fill_manual(values = cols[c(1:4)]) +\n  labs(\n    title = \"Planning Application Themes\",\n    x = NULL, y = NULL,\n    caption = \"Source: RBKC Planning Search\"\n    )"
  },
  {
    "objectID": "project/planning/index.html#r-toolbox",
    "href": "project/planning/index.html#r-toolbox",
    "title": "Digging Deep",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nDT\ndatatable[1]\n\n\nSPARQL\nSPARQL[1]\n\n\nbase\nas.numeric[1], basename[1], c[16], is.na[2], library[15], min[1], readRDS[1], saveRDS[1], sum[2]\n\n\nclock\ndate_build[1], date_parse[1], get_month[1], get_year[2]\n\n\nconflicted\nconflict_prefer[1], conflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nacross[4], arrange[1], bind_rows[2], case_match[1], case_when[2], count[1], desc[1], filter[2], if_else[3], join_by[1], left_join[1], mutate[9], n[2], na_if[2], rename[2], select[2], slice_head[1], summarise[4]\n\n\nfabletools\ncomponents[1], model[1]\n\n\nfeasts\nCCF[1], STL[1]\n\n\nforcats\nfct_explicit_na[1], fct_lump[1], fct_reorder[1]\n\n\nggplot2\naes[3], autoplot[2], coord_flip[1], element_text[1], facet_wrap[1], geom_bar[1], geom_col[1], geom_line[1], ggplot[3], labs[5], scale_colour_manual[2], scale_fill_manual[1], theme[1], theme_bw[1], theme_set[1]\n\n\nhtmlwidgets\nsaveWidget[1]\n\n\nlubridate\nas_date[1], as_datetime[1]\n\n\npurrr\nlist_rbind[2], map[2]\n\n\nquanteda\ncorpus[2], dfm[1], kwic[1], phrase[1]\n\n\nquanteda.textstats\ntextstat_frequency[1]\n\n\nreadr\nread_csv[1]\n\n\nrvest\nhtml_attr[1], html_element[2], html_elements[2], html_table[1], html_text[1]\n\n\nstopwords\nstopwords[1]\n\n\nstringr\nstr_c[3], str_detect[11], str_extract[1], str_remove_all[2], str_replace[2], str_squish[1], str_to_lower[1], str_to_upper[1]\n\n\ntibble\nas_tibble[2], tibble[1]\n\n\ntictoc\ntic[2], toc[2]\n\n\ntidyr\npivot_wider[2], replace_na[1]\n\n\ntidyselect\nall_of[1], where[1]\n\n\ntsibble\nas_tsibble[1], yearmonth[1]\n\n\nusedthese\nused_here[1]\n\n\nutils\ndownload.file[1]\n\n\nwesanderson\nwes_palette[1]\n\n\nxml2\nread_html[3]"
  },
  {
    "objectID": "project/bands/index.html",
    "href": "project/bands/index.html",
    "title": "Bootstraps & Bandings",
    "section": "",
    "text": "Are the residential property bands of 3 decades ago becoming less so? Would a sample of those recently-sold reveal band convergence? And what may be inferred about those not sampled?\nOver the years, urban properties have been added to and divided up. And two streets of equal attractiveness, and with equivalently-banded properties, may have diverged as neighbourhoods evolved.\nWhilst properties can and do move to higher or lower bands following alteration, would a sample of those recently-sold reveal band convergence after so long? And what may be inferred about the wider housing stock?\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(rvest)\nlibrary(scales)\nlibrary(SPARQL)\nlibrary(clock)\nconflict_prefer(\"date_format\", \"clock\")\nlibrary(RColorBrewer)\nlibrary(glue)\nlibrary(janitor)\nlibrary(infer)\nlibrary(tsibble)\nlibrary(ggfx)\nlibrary(usedthese)\n\nconflict_scout()\nSetting the theme and colour palette for all graphics (with a little help from the ggfx package).\ntheme_set(theme_bw())\n\ncol &lt;- \"RdYlBu\"\n\nscale_fill_continuous &lt;- \\(...) scale_fill_distiller(palette = col)\n\ncols &lt;- brewer.pal(7, col)\n\ntibble(x = 1, y = 1, fill = 7:1) |&gt; \n  ggplot(aes(x, y, fill = fill)) +\n  as_reference(geom_col(show.legend = FALSE), id = \"cols\") +\n  with_blend(\n    geom_text(\n      x = 1,\n      y = 3.5,\n      label = col,\n      size = 40,\n      fontface = \"bold\"\n    ),\n    bg_layer = \"cols\",\n    blend_type = \"atop\",\n    flip_order = TRUE,\n    id = \"text\"\n  ) +\n  with_outer_glow(\"text\", colour = \"white\") +\n  scale_fill_continuous() +\n  coord_flip() +\n  theme_void()\nProperty band and price-paid data are separately sourced. The free-form street address is the only way to bring the two together. The structure, content and even spelling of the address sometimes differ, for example: “FLAT C, 22 SOME STREET, SOME AREA, SW10 1AA” in one may be “22C 2ND FLR, HOUSE NAME, SOME STREET SW10 1AA” in the other.\nSo, a little string manipulation is needed to create a common key. And reusable patterns will enable a consistent application to both.\nremove_pattern &lt;-\n  str_c(\n1    \", London, SW10 .+$\",\n    \"FLAT \",\n    \"APARTMENT \",\n    \"CHELSEA HARBOUR\",\n2    \"(?&lt;=COURT|SANDHILLS| HOUSE|WALK|ESTATE|ROW).*\",\n    \"[,'\\\\.]\",\n    \"(?&lt;= )AT \",\n    \"(?&lt;=VINT)N\",\n    \"(?&lt;=FARRIER)S\",\n3    \"(1ST|2ND|3RD|4TH|5TH|6TH) FLR \",\n    \"FLR (1ST|2ND|3RD|4TH|5TH|6TH) \",\n    \" ?- ?[0-9]{1,3}\",\n    sep = \"|\"\n  )\n\n4swizzle_from &lt;- \"^([0-9]{1,3})([A-Z])(?= .*)\"\nswizzle_to &lt;- \"\\\\2 \\\\1\"\n\n\n1\n\nLooks for strings that end with 1 or more characters following the London postcode district of “SW10”.\n\n2\n\nUses a positive lookbehind to find anything, for example, following the word COURT or WALK (since the postcode is stored in a separate variable and already isolates the road, so the road name is redundant).\n\n3\n\nFinds any occurrences of 1ST FLR, FLR 1ST etc.\n\n4\n\nUses capture groups to temporarily memorise a number (capture group 1) followed by a letter (capture group 2), then swap them around. This is because a flat number may be C22 in one dataset and 22C in the other.\nCouncil Tax band data are available for non-commercial use1.\nurl1 &lt;-\n  str_c(\n    \"https://www.tax.service.gov.uk/\",\n    \"check-council-tax-band/\",\n    \"search-council-tax-advanced?\",\n    \"postcode=Fkvms5WVQum-uX3L00_pcA&\",\n    \"filters.councilTaxBands=\"\n  )\n\nurl2 &lt;- \"&filters.propertyUse=N&postcode=Fkvms5WVQum-uX3L00_pcA&page=\"\n\nurl3 &lt;- \"&filters.bandStatus=Current\"\n\nindex &lt;- crossing(band = LETTERS[1:8], page = seq(0, 120, 1))\n\nband_df &lt;- map2(index$band, index$page, possibly(\\(i, j) {\n  str_c(url1, i, url2, j, url3) |&gt;\n    read_html() |&gt;\n    html_element(\"#search-results-table\") |&gt;\n    html_table(convert = FALSE)\n}, otherwise = NA_character_)) |&gt; \n  list_rbind()\nband_df2 &lt;- \n  band_df |&gt; \n  clean_names() |&gt; \n  mutate(postcode = str_extract(address, \"SW10 .+$\"),\n         raw_band_address = str_remove(address, \", London, SW10 .+$\"),\n         address = str_remove_all(address, remove_pattern),\n         address = str_replace(address, swizzle_from, swizzle_to),\n         address = str_squish(address)\n  )\nHouse price-paid data are similarly available for non-commercial use2.\nendpoint &lt;- \"https://landregistry.data.gov.uk/landregistry/query\"\n\nquery &lt;- '\nPREFIX  xsd:  &lt;http://www.w3.org/2001/XMLSchema#&gt;\nPREFIX  text: &lt;http://jena.apache.org/text#&gt;\nPREFIX  ppd:  &lt;http://landregistry.data.gov.uk/def/ppi/&gt;\nPREFIX  lrcommon: &lt;http://landregistry.data.gov.uk/def/common/&gt;\n\nSELECT  ?ppd_propertyAddress ?ppd_transactionCategory ?ppd_transactionDate ?ppd_pricePaid ?ppd_estateType ?ppd_propertyAddressCounty ?ppd_propertyAddressDistrict ?ppd_propertyAddressLocality ?ppd_propertyAddressPaon ?ppd_propertyAddressPostcode ?ppd_propertyAddressSaon ?ppd_propertyAddressStreet ?ppd_propertyAddressTown ?ppd_propertyType ?ppd_recordStatus\n\nWHERE\n  { { ?ppd_propertyAddress\n                text:query               ( lrcommon:postcode \"( SW10 )\" 3000000 ) .\n      ?item     ppd:propertyAddress      ?ppd_propertyAddress ;\n                ppd:transactionCategory  ppd:standardPricePaidTransaction ;\n                ppd:transactionDate      ?ppd_transactionDate ;\n                ppd:pricePaid            ?ppd_pricePaid ;\n      FILTER ( ?ppd_transactionDate &gt;= \"2020-01-01\"^^xsd:date )\n    }\n    OPTIONAL{ ?item  ppd:estateType  ?ppd_estateType }\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:county  ?ppd_propertyAddressCounty}\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:district  ?ppd_propertyAddressDistrict}\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:locality  ?ppd_propertyAddressLocality}\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:paon  ?ppd_propertyAddressPaon}\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:postcode  ?ppd_propertyAddressPostcode}\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:saon  ?ppd_propertyAddressSaon}\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:street  ?ppd_propertyAddressStreet}\n    OPTIONAL{ ?ppd_propertyAddress lrcommon:town  ?ppd_propertyAddressTown}\n    OPTIONAL{ ?item  ppd:propertyType  ?ppd_propertyType }\n    OPTIONAL{ ?item  ppd:recordStatus  ?ppd_recordStatus }\n    BIND(ppd:standardPricePaidTransaction AS ?ppd_transactionCategory)\n  }'\n\nprices_list &lt;- SPARQL(endpoint, query)\nprices_df2 &lt;-\n  prices_list$results |&gt;\n  as_tibble() |&gt; \n  clean_names() |&gt;\n  rename_with(~ str_remove_all(., \"ppd_|property_address_\")) |&gt;\n  mutate(\n    transaction_date = as_datetime(transaction_date) |&gt; as_date(),\n    price_paid = price_paid / 1000000\n  ) |&gt;\n  filter(transaction_date &lt; \"2022-01-01\") |&gt;\n  mutate(\n    raw_price_address = str_c(str_replace_na(saon, \"\"), \n                              paon, street, sep = \" \") |&gt; str_squish(),\n    address = str_remove_all(raw_price_address, remove_pattern),\n    address = str_replace(address, swizzle_from, swizzle_to)\n  ) |&gt;\n  select(\n    address,\n    raw_price_address,\n    postcode,\n    price_paid,\n    transaction_date,\n    estate_type,\n    property_type,\n    transaction_category\n  )\nNow there’s a common key to join the data.\njoined_df &lt;-\n  prices_df2 |&gt;\n  inner_join(band_df2, by = join_by(address, postcode)) |&gt;\n  relocate(raw_band_address, .after = raw_price_address) |&gt;\n  arrange(postcode, address) |&gt;\n  mutate(council_tax_band = factor(council_tax_band))\nAs with previous posts Digging Deep and House Sales, I’m focusing on postcodes in the SW10 part of London.\nIt’s not possible to assess all SW10 properties by band since only a tiny fraction will have been sold recently. Recent sales could though be used as a sample and Bootstrap Confidence Intervals then employed to draw a wider inference.\n“Pulling yourself up by your bootstraps” originally meant doing something absurd. Later it came to mean succeeding with only what you have at your disposal. Hence only the sample will be used as a surrogate for the true population by making repeated random draws from it (with replacement).\nA key assumption is that the sample is representative of the true population.\nEven though only recent sales transactions have been selected, a small movement in market prices will have occurred. So ensuring the bands are reasonably well distributed over the period is worthwhile.\njoined_df |&gt;\n  select(transaction_date, price_paid, council_tax_band) |&gt;\n  mutate(yearquarter = yearquarter(transaction_date)) |&gt;\n  count(yearquarter, council_tax_band) |&gt;\n  ggplot(aes(yearquarter, n, fill = council_tax_band)) +\n  geom_col(position = position_fill()) +\n  scale_x_yearquarter() +\n  scale_y_continuous(labels = label_percent(1)) +\n  scale_fill_manual(values = cols[c(1:7)]) +\n  labs(\n    title = \"Distribution of Sales Transactions by Band & Quarter\",\n    x = \"Quarter\", y = \"Proportion\", fill = \"Band\"\n  )\nA violin plot of the property values by band shows some bimodal distribution and oddly shows bands E & F with lower mean prices than band D. This is worth closer inspection to ensure the sample is representative.\nlabels &lt;- joined_df |&gt;\n  summarise(n = n(), mean_price = mean(price_paid),\n            .by = council_tax_band)\n\ntransactions &lt;-\n  joined_df |&gt;\n  count() |&gt;\n  pull()\n\nfrom &lt;- joined_df |&gt;\n  summarise(min(transaction_date) |&gt; yearquarter()) |&gt;\n  pull()\n\nto &lt;- joined_df |&gt;\n  summarise(max(transaction_date) |&gt; yearquarter()) |&gt;\n  pull()\n\njoined_df |&gt;\n  ggplot(aes(council_tax_band, price_paid)) +\n  geom_violin(fill = cols[1]) +\n  geom_label(aes(label = glue(\n    \"n = {n} \\nAvg Price \",\n    \"{dollar(mean_price, prefix = '£', suffix = 'm', accuracy = 0.01)}\"\n  ), y = 16),\n  data = labels, size = 2.3, alpha = 0.7, fill = \"white\"\n  ) +\n  scale_y_log10(labels = label_dollar(\n    prefix = \"£\",\n    suffix = \"m\", accuracy = 0.1\n  )) +\n  labs(\n    title = \"Droopy Bandings\",\n    subtitle = glue(\n      \"Sample of {transactions} Property \",\n      \"Transactions in SW10 ({from} to {to})\"\n    ),\n    x = \"Council Tax Band\", y = \"Sale Price (log10 scale)\",\n    caption = \"Sources: tax.service.gov.uk & landregistry.data.gov.uk\"\n  )\njoined_df2 &lt;- joined_df |&gt;\n  mutate(`SW10 0JR` = if_else(postcode == \"SW10 0JR\", \"Yes\", \"No\"))\nIt turns out that the unusual transactions below £0.3m are almost entirely from one postcode as shown below when isolating “SW10 0JR”. This appears to be a single large new development with all sub-units sold in 2020.\nThese specific transactions feel somewhat unusual at these banding levels. And irrespective of their accuracy, a sample of 170 postcodes heavily dominated by the transactions of just one would not be representative of the true population.\njoined_df2 |&gt;\n  ggplot(aes(council_tax_band, price_paid, fill = `SW10 0JR`)) +\n  geom_violin() +\n  geom_label(aes(label = glue(\n    \"n = {n} \\nAvg Price\\n\",\n    \"{dollar(mean_price, prefix = '£', suffix = 'm', accuracy = 0.01)}\"\n  ), y = 16),\n  data = labels, size = 2.3, alpha = 0.7, fill = \"white\"\n  ) +\n  geom_hline(yintercept = 0.3, linetype = \"dashed\") +\n  scale_y_log10(labels = label_dollar(\n    prefix = \"£\",\n    suffix = \"m\", accuracy = 0.1\n  )) +\n  scale_fill_manual(values = cols[c(1, 5)]) +\n  labs(\n    title = \"Unusual Bandings\",\n    subtitle = glue(\n      \"Sample of {transactions} Property \",\n      \"Transactions in SW10 ({from} to {to})\"\n    ),\n    x = \"Council Tax Band\", y = \"Sale Price (log10 scale)\",\n    caption = \"Sources: tax.service.gov.uk & landregistry.data.gov.uk\"\n  )\n\n\n\njoined_df2 |&gt;\n  count(postcode, sort = TRUE) |&gt;\n  slice_head(n = 10)\n\n\n\n\n\npostcode\nn\n\n\n\n\nSW10 0JR\n79\n\n\nSW10 0HQ\n10\n\n\nSW10 0AA\n7\n\n\nSW10 0HG\n6\n\n\nSW10 9AD\n6\n\n\nSW10 9JP\n6\n\n\nSW10 0DD\n5\n\n\nSW10 0UY\n5\n\n\nSW10 9BT\n5\n\n\nSW10 9JR\n5\nSo, I’ll remove this postcode.\njoined_df3 &lt;- joined_df |&gt; \n  filter(postcode != \"SW10 0JR\")\nThis now feels like a representative sample of 314 property transactions. And broadly-speaking the plot shows a progression in average property values as we step through the bands. There is though substantial convergence between some, with the “drippy” band E still looking almost indistinguishable from band D.\nlabels &lt;- joined_df3 |&gt;\n  summarise(n = n(), mean_price = mean(price_paid),\n            .by = council_tax_band)\n\ntransactions &lt;-\n  joined_df3 |&gt;\n  count() |&gt;\n  pull()\n\njoined_df3 |&gt;\n  ggplot(aes(council_tax_band, price_paid)) +\n  geom_violin(fill = cols[1]) +\n  geom_label(aes(label = glue(\n    \"n = {n} \\nAvg Price \",\n    \"{dollar(mean_price, prefix = '£', suffix = 'm', accuracy = 0.01)}\"\n  ), y = 16),\n  data = labels, size = 2.3, alpha = 0.7, fill = \"white\"\n  ) +\n  scale_y_log10(labels = label_dollar(prefix = \"£\", \n                                       suffix = \"m\", accuracy = 0.1)) +\n  labs(\n    title = \"Drippy Bandings\",\n    subtitle = glue(\n      \"Sample of {transactions} Property \",\n      \"Transactions in SW10 ({from} to {to})\"\n    ),\n    x = \"Council Tax Band\", y = \"Sale Price (log10 scale)\",\n    caption = \"Sources: tax.service.gov.uk & landregistry.data.gov.uk\"\n  )\nSome band E examples:\njoined_df3 |&gt;\n  filter(council_tax_band == \"E\", price_paid &gt;= 0.6) |&gt;\n  select(\n    address = raw_price_address,\n    postcode,\n    transaction_date,\n    price_paid,\n    tax_band = council_tax_band,\n    estate_type,\n    property_type\n  ) |&gt;\n1  mutate(across(ends_with(\"type\"), \\(x) str_remove_all(x, \"^.*mon/|&gt;$\"))) |&gt;\n  select(-address)\n\n\n1\n\nRemoves the URL wrapped around the estate and property types, i.e. “&lt;http://landregistry.data.gov.uk/def/common/” before and “&gt;” after the type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npostcode\ntransaction_date\nprice_paid\ntax_band\nestate_type\nproperty_type\n\n\n\n\nSW10 0AN\n2020-03-18\n0.83000\nE\nleasehold\nflat-maisonette\n\n\nSW10 0AP\n2021-06-22\n0.67500\nE\nleasehold\nflat-maisonette\n\n\nSW10 0AW\n2021-03-29\n0.69500\nE\nleasehold\nflat-maisonette\n\n\nSW10 0AX\n2020-02-21\n0.65000\nE\nleasehold\nflat-maisonette\n\n\nSW10 0BG\n2021-03-30\n0.76500\nE\nleasehold\nflat-maisonette\n\n\nSW10 0HP\n2021-10-12\n0.67500\nE\nleasehold\nflat-maisonette\n\n\nSW10 0LB\n2021-04-15\n0.63000\nE\nleasehold\nflat-maisonette\n\n\nSW10 0PE\n2020-03-25\n0.68000\nE\nleasehold\nflat-maisonette\n\n\nSW10 9AD\n2021-06-21\n1.07500\nE\nleasehold\nflat-maisonette\n\n\nSW10 9ED\n2020-10-29\n0.60000\nE\nleasehold\nflat-maisonette\n\n\nSW10 9JX\n2020-09-14\n1.00000\nE\nleasehold\nflat-maisonette\n\n\nSW10 9JY\n2021-08-30\n1.00000\nE\nleasehold\nflat-maisonette\n\n\nSW10 9JY\n2020-08-06\n0.67950\nE\nleasehold\nflat-maisonette\n\n\nSW10 9PJ\n2020-12-14\n0.68925\nE\nleasehold\nflat-maisonette\nCan we infer that the true population of band Es no longer exhibits any difference in mean values with respect to band D?\nbands_ef &lt;- \n  joined_df3 |&gt;\n  filter(council_tax_band %in% c(\"E\", \"D\"))\n\nobs_stat &lt;- \n  bands_ef |&gt; \n  specify(price_paid ~ council_tax_band) |&gt;\n  calculate(stat = \"diff in means\", order = c(\"E\", \"D\")) |&gt;\n  pull()\n\nset.seed(2)\n\nboot_dist &lt;-\n  bands_ef |&gt; \n  specify(price_paid ~ council_tax_band) |&gt;\n  generate(reps = 2000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"diff in means\", order = c(\"E\", \"D\"))\n\nperc_ci &lt;- get_ci(boot_dist)\n\nlower &lt;- perc_ci |&gt;\n  pull(lower_ci) |&gt;\n  dollar(prefix = \"£\", suffix = \"m\", accuracy = 0.01)\nupper &lt;- perc_ci |&gt;\n  pull(upper_ci) |&gt;\n  dollar(prefix = \"£\", suffix = \"m\", accuracy = 0.01)\n\nboot_dist |&gt;\n  visualise() +\n  shade_confidence_interval(\n    endpoints = perc_ci,\n    color = cols[6], fill = cols[3]\n  ) +\n  geom_vline(xintercept = obs_stat, linetype = \"dashed\", colour = \"white\") +\n  annotate(\"label\",\n    x = -0.12, y = 350, size = 3,\n    label = glue(\n      \"Observed Difference\\nBetween Bands D & E is \",\n      \"{dollar(obs_stat, prefix = '£', suffix = 'm', accuracy = 0.01)}\"\n    )\n  ) +\n  scale_x_continuous(labels = label_dollar(\n    prefix = \"£\",\n    suffix = \"m\", accuracy = 0.1\n  )) +\n  labs(\n    subtitle = glue(\n      \"95% Confident the Difference \",\n      \"in Mean Prices Between Bands D & E is {lower} to {upper}\"\n    ),\n    x = \"Difference in Means\", y = \"Count\",\n    caption = \"Sources: tax.service.gov.uk & landregistry.data.gov.uk\"\n  )\nBootstrapping with a 95% confidence interval suggests the true difference in mean prices between all band D and E properties in SW10 is somewhere in the range -£0.10m to £0.10m. Considerable convergence compared to 3 decades ago when the band E minimum exceeded the band D maximum."
  },
  {
    "objectID": "project/bands/index.html#r-toolbox",
    "href": "project/bands/index.html#r-toolbox",
    "title": "Bootstraps & Bandings",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\n\nPackage\nFunction\n\n\n\n\nRColorBrewer\nbrewer.pal[1]\n\n\nSPARQL\nSPARQL[1]\n\n\nbase\nc[5], factor[1], library[13], max[1], mean[2], min[1], readRDS[1], seq[1], set.seed[2]\n\n\nconflicted\nconflict_prefer[2], conflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nacross[1], arrange[1], count[4], filter[4], if_else[1], inner_join[1], join_by[1], mutate[7], n[2], pull[7], relocate[1], rename_with[1], select[5], slice_head[1], slice_sample[1], summarise[4]\n\n\nggfx\nas_reference[1], with_blend[1], with_outer_glow[1]\n\n\nggplot2\naes[8], annotate[1], coord_flip[1], geom_col[2], geom_hline[1], geom_label[3], geom_text[1], geom_violin[3], geom_vline[1], ggplot[5], labs[5], position_fill[1], scale_fill_continuous[1], scale_fill_distiller[1], scale_fill_manual[2], scale_x_continuous[1], scale_y_continuous[1], scale_y_log10[3], theme_bw[1], theme_set[1], theme_void[1]\n\n\nglue\nglue[8]\n\n\ninfer\ncalculate[2], generate[1], get_ci[1], shade_confidence_interval[1], specify[2], visualise[1]\n\n\njanitor\nclean_names[2]\n\n\nlubridate\nas_date[1], as_datetime[1]\n\n\npurrr\nlist_rbind[1], map2[1], possibly[1]\n\n\nrvest\nhtml_element[1], html_table[1]\n\n\nscales\ndollar[6], label_dollar[4], label_percent[1]\n\n\nstringr\nstr_c[4], str_extract[1], str_remove[1], str_remove_all[4], str_replace[2], str_replace_na[1], str_squish[2]\n\n\ntibble\nas_tibble[1], tibble[1]\n\n\ntidyr\ncrossing[1]\n\n\ntidyselect\nends_with[1]\n\n\ntsibble\nscale_x_yearquarter[1], yearquarter[3]\n\n\nusedthese\nused_here[1]\n\n\nxml2\nread_html[1]"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Little Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Updated - Oldest\n        \n         \n          Updated - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nAn Infinite Number of Monkeys\n\n\n9 min\n\n\nQuantile regression, property sales and anything could happen eventually\n\n\n\nFeb 24, 2023\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Footnote in History\n\n\n8 min\n\n\nThe grammar of tables, footnotes and occupations consigned to history\n\n\n\nNov 1, 2022\n\n\n\n\n\nJan 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Happiness in The Smoke\n\n\n9 min\n\n\nCluster analysis and the characteristics that bind London boroughs\n\n\n\nMar 19, 2022\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstraps & Bandings\n\n\n13 min\n\n\nDecades-old residential property bands and inference using a sample of those recently sold\n\n\n\nMar 8, 2022\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSea Monsters that Lost their Way\n\n\n12 min\n\n\nPredicting uncertain species of cetacean strandings recorded by the Natural History Museum\n\n\n\nDec 4, 2021\n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Frosty Deal?\n\n\n6 min\n\n\nQuantitative textual analysis, word embeddings and analysing shifting trade-talk sentiment?\n\n\n\nSep 18, 2020\n\n\n\n\n\nFeb 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Goldilocks Principle\n\n\n4 min\n\n\nSimulating stock portfolio returns inspired by bowls of porridge left by three bears\n\n\n\nAug 9, 2020\n\n\n\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeathering the Storm\n\n\n2 min\n\n\nTimeseries comparison and the impact of Covid-19 on the financial markets by sector\n\n\n\nAug 2, 2020\n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nFavourite Things\n\n\n4 min\n\n\nR packages & functions that make doing data science a joy based on usage across projects\n\n\n\nJul 26, 2020\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nEast-West Drift\n\n\n5 min\n\n\nAnimated dimension reduction and East-West historical UN voting patterns\n\n\n\nJan 9, 2019\n\n\n\n\n\nJan 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeeing the Wood for the Trees\n\n\n4 min\n\n\nVisualising small multiples when crime data leave you unable to see the wood for the trees\n\n\n\nJan 1, 2019\n\n\n\n\n\nFeb 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan Ravens Forecast?\n\n\n7 min\n\n\nTime series forecasting using cloud services spend data\n\n\n\nJul 29, 2018\n\n\n\n\n\nJan 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSix Months Later\n\n\n5 min\n\n\nExploring colour palettes and small multiples using cloud services spend data\n\n\n\nApr 2, 2018\n\n\n\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriminal Goings-on in a Random Forest\n\n\n10 min\n\n\nCriminal goings-on in a random forest and predictions with tree-based and glm models\n\n\n\nMar 1, 2018\n\n\n\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlots Thicken\n\n\n6 min\n\n\nEvery story needs a good plot. Which plot types generate the most interest on Wikipedia?\n\n\n\nFeb 7, 2018\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster of Six\n\n\n8 min\n\n\nExploring parliamentary voting patterns with hierarchical clustering\n\n\n\nJan 29, 2018\n\n\n\n\n\nFeb 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigging Deep\n\n\n9 min\n\n\nDo we see more planning applications when house sales are depressed?\n\n\n\nJan 10, 2018\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurprising Stories\n\n\n4 min\n\n\nA little interactive geospatial mapping and an unexpected find\n\n\n\nDec 20, 2017\n\n\n\n\n\nFeb 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Sales\n\n\n6 min\n\n\nA series of events, such as the Financial Crisis and the 2016 Brexit vote, that damped down residential property sales in London\n\n\n\nDec 17, 2017\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere Clouds Cross\n\n\n8 min\n\n\nVisualising the dozens of overlapping sets formed by categories of cloud services\n\n\n\nNov 14, 2017\n\n\n\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Jitter\n\n\n5 min\n\n\nWelcome to the tidyverse with data ingestion, cleaning and tidying. And some visualisations of sales data with a little jittering.\n\n\n\nSep 12, 2017\n\n\n\n\n\nFeb 25, 2023\n\n\n\n\n\n\n\nNo matching items\n\n\nThese “little projects” are contributed to R-bloggers.com."
  },
  {
    "objectID": "project/goldilocks/index.html",
    "href": "project/goldilocks/index.html",
    "title": "The Goldilocks Principle",
    "section": "",
    "text": "The Goldilocks principle has its origins in a children’s story about a girl who tastes the bowls of porridge left by three bears. She prefers the one that is neither too hot nor too cold, but is just right.\nWhen it comes to investing in stocks, how many is “just right”?\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(wesanderson)\nlibrary(scales)\nlibrary(truncnorm)\nlibrary(usedthese)\n\nconflict_scout()\nI’ll use this palette.\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(name = \"Darjeeling2\"))\nSuppose the average stock market return is around 10%. And you do extensive research, burning the midnight oil, poring over stock fundamentals. Or perhaps you develop a cool machine learning model. And you arrive at a list of 50 promising stocks you feel confident would, on average, deliver well-above-market returns.\nI’ll create some randomly made up stocks with an average return close to 40%. Some will tank due to events one could not foresee; I’ll allow some to lose up to 20%. Similarly, some could generate exceptional returns.\nset.seed(123)\n\nstock_data &lt;- tibble(\n  stock = chartr(\"0123456789\", \"abcdefghij\", sample(50)),\n  return = rtruncnorm(50, a = -0.2, mean = 0.4, sd = 0.5)\n)\n\nmean(stock_data$return)\n\n[1] 0.4200244\nHere’s the resultant distribution I’ll use to assess the impact of portfolio size. Stock markets are fairly close to a normal distribution, albeit with fatter tails due to a few extreme outcomes.\nstock_data |&gt; \n  ggplot(aes(return)) +\n  geom_histogram(fill = cols[2]) +\n  scale_x_continuous(labels = label_percent()) +\n  labs(title = \"50 Randomly-generated Stock Returns\", \n       x = \"Annual Return\", y = \"Count\")\nNow suppose you share 2 stocks, selected at random, with 1,000 of your social network friends (selecting a different pair of stocks for each friend). Will they all still be friends a year later? And if you repeated the same scenario with portfolio sizes of 5, 10, 20 and 50 stocks per person, would that change the outcome? Let’s see.\nportfolio &lt;- \\(x) {\n  stock_data |&gt;\n    slice_sample(n = x, replace = TRUE) |&gt;\n    summarise(\n      portfolio_return = mean(return),\n      portfolio_size = x\n    ) |&gt;\n    bind_rows()\n}\n\nset.seed(456)\n\nportfolios &lt;-\n  map(c(\n    rep(2, 1000),\n    rep(5, 1000),\n    rep(10, 1000),\n    rep(20, 1000),\n    rep(50, 1000)\n  ), portfolio) |&gt;\n  list_rbind() |&gt; \n  mutate(portfolio_size = factor(portfolio_size))\n\nmean_returns &lt;- portfolios |&gt;\n  summarise(\n    mean_return = mean(portfolio_return),\n    min_return = min(portfolio_return),\n    .by = portfolio_size\n  )\n\nportfolios |&gt;\n  ggplot(aes(portfolio_size, portfolio_return, group = portfolio_size)) +\n  geom_violin(aes(fill = portfolio_size), show.legend = FALSE) +\n  geom_label(aes(portfolio_size, 1.5,\n    label = percent(mean_return, accuracy = 1)\n  ),\n  data = mean_returns, fill = cols[4],\n  ) +\n  geom_label(aes(portfolio_size, -0.2,\n    label = percent(min_return, accuracy = 1)\n  ),\n  data = mean_returns, fill = cols[1],\n  ) +\n  scale_y_continuous(labels = label_percent(), breaks = breaks_extended(9)) +\n  scale_fill_manual(values = cols[c(1:5)]) +\n  labs(\n    x = \"Portfolio Size\", y = \"Return\",\n    title = \"How Portfolio Size Changes Downside & Upside Risk\",\n    subtitle = \"BLUE Labels = Mean Return; BROWN Labels = Worst Return\"\n  )\nSo, for all portfolio sizes, the average return across your 1,000 friends is around 42%.\nBut, when the portfolio size is 2, you may be erased from a few Christmas card lists (or worse). If one of those two stocks has an extreme negative outcome, there’s little else in the portfolio to dissipate the effect. As the portfolio size increases, the risk (downside and upside) dramatically reduces.\nBut is more always better? Well, irrespective of whether your list of promising stocks resulted from desk research or a model, there will be a varying degree of confidence in the 50. A machine learning model, for example, would assign class probabilities to each stock.\nSo by picking a smaller number, one can select those in which one feels most confident, or which have the highest class probability. And by picking a larger number (ideally across different sectors to further reduce risk) one can weaken the effects of a bad egg or two caused by events no research or model could foresee.\nSo perhaps the answer is to pick a worst-case scenario one would be prepared to accept. In the plot above, accepting a small chance of only a 12% return (still better than the historical average market return), might provide the “just right” portfolio. A portfolio of a manageable size, focused on your highest-confidence stocks, and with pretty good odds of the desired return."
  },
  {
    "objectID": "project/goldilocks/index.html#r-toolbox",
    "href": "project/goldilocks/index.html#r-toolbox",
    "title": "The Goldilocks Principle",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[2], chartr[1], factor[1], library[6], mean[3], min[1], rep[5], sample[1], set.seed[2]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nbind_rows[1], mutate[1], slice_sample[1], summarise[2]\n\n\nggplot2\naes[5], geom_histogram[1], geom_label[2], geom_violin[1], ggplot[2], labs[2], scale_fill_manual[1], scale_x_continuous[1], scale_y_continuous[1], theme_bw[1], theme_set[1]\n\n\npurrr\nlist_rbind[1], map[1]\n\n\nscales\nbreaks_extended[1], label_percent[2], percent[2]\n\n\ntibble\ntibble[1]\n\n\ntruncnorm\nrtruncnorm[1]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/stories/index.html",
    "href": "project/stories/index.html",
    "title": "Surprising Stories",
    "section": "",
    "text": "Late in 2017 I experimented with geospatial mapping techniques in R. The log file for my blog seemed like a good source of data. I thought it might appeal to a wider audience of one (including me).\nCombined with longitude and latitude data from MaxMind’s GeoLite2, this offered a basis for analysis. Although less precise than the GeoIP2 database, this would be more than adequate for my purpose of getting to country and city level. I settled on the leaflet (Cheng, Karambelkar, and Xie 2022) package for visualisation given the interactivity and pleasing choice of aesthetics.\nThe results however were a little puzzling.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(rgeolocate)\nlibrary(R.utils)\nlibrary(leaflet)\nlibrary(rgdal)\nlibrary(wesanderson)\nlibrary(htmlwidgets)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(\"Darjeeling2\"))\nzip_file &lt;- \"world_shape_file.zip\"\nshape_file &lt;- \"TM_WORLD_BORDERS_SIMPL-0.3\"\n\nstr_c(\"http://thematicmapping.org/downloads/\", shape_file, \".zip\") |&gt;\n  download.file(zip_file)\n\nunzip(zip_file)\n\nworld_spdf &lt;- readOGR(getwd(), shape_file, verbose = FALSE)\nurl &lt;- \"http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.mmdb.gz\"\n\nfile_name &lt;- basename(url)\n\ndownload.file(url, file_name)\n\ngunzip(file_name, overwrite = TRUE)\nstats &lt;- read_csv(\"stats.csv\")\nip_df &lt;- map2(stats$IP, stats$Pages, \\(x, y) {\n  maxmind(\n    x,\n    \"GeoLite2-City.mmdb\",\n    c(\n      \"country_name\",\n      \"city_name\",\n      \"longitude\",\n      \"latitude\",\n      \"region_name\"\n    )\n  ) |&gt;\n    mutate(IP = x) |&gt;\n    rename(\n      country = country_name,\n      region = region_name,\n      city = city_name\n    ) |&gt;\n    mutate(\n      Pages = y,\n      Views = case_when(\n        Pages &lt; 500 ~ 1,\n        Pages &lt; 1000 ~ 2,\n        Pages &lt; 2000 ~ 3,\n        .default = 4\n      )\n    )\n}) |&gt;\n  list_rbind()\n\nip_df &lt;- ip_df |&gt;\n  filter(!is.na(longitude) | !is.na(latitude)) |&gt;\n  arrange(Pages)\nThe concentration of page views in central London was of no immediate surprise as this was likely to be my site maintenance and blogging. What did strike me as odd though was the high concentration of page views in the centre of the US. More curious still, when I zoomed in on Kansas and found myself in the middle of the Cheney Reservoir.\npal &lt;-\n  colorFactor(cols[c(2:5)],\n    domain = c(1, 2, 3, 4)\n  )\n\nmap1 &lt;- leaflet(world_spdf) |&gt; # World view\n  addProviderTiles(providers$CartoDB.Positron,\n    options = providerTileOptions(maxZoom = 21)\n  ) |&gt;\n  setView(-30, 35, zoom = 2) |&gt; # World view\n  addPolygons(\n    fillColor = cols[1],\n    stroke = TRUE,\n    fillOpacity = 1,\n    color = cols[5],\n    weight = 0.3,\n    highlight = highlightOptions(\n      weight = 3,\n      color = cols[3],\n      fillOpacity = 0.3,\n      bringToFront = FALSE\n    ),\n    label = world_spdf@data$NAME,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\"),\n      textsize = \"12px\"\n    )\n  ) |&gt;\n  addCircleMarkers(\n    lng = ip_df$longitude,\n    lat = ip_df$latitude,\n    radius = ~ case_match(\n      ip_df$Views,\n      1 ~ 5,\n      2 ~ 10,\n      3 ~ 15,\n      .default = 20\n    ),\n    fillColor = ~ pal(ip_df$Views),\n    color = cols[5],\n    weight = 1,\n    fillOpacity = 0.7,\n    popup = str_c(\n      \"&lt;b&gt;\",\n      ip_df$city,\n      \"&lt;/b&gt;\",\n      \"&lt;br/&gt;\",\n      ip_df$region,\n      \"&lt;br/&gt;\",\n      as.character(ip_df$Pages),\n      \" \",\n      \"page views\"\n    )\n  ) |&gt;\n  addLegend(\n    colors = cols[c(2:5)],\n    labels = c(\"&lt;500\", \"500+\", \"1,000+\", \"2,000+\"),\n    opacity = 1,\n    title = \"Page Views&lt;br/&gt;Oct-23 to Dec-31 2017\",\n    position = \"bottomleft\"\n  )\nI imagined someone drifting in the expanse of water with laptop, flask of coffee and box of sandwiches, whiling away the hours absorbed in my blog. Perhaps not. How could such a small number of blog pages generate in excess of 2,000 page views in one spot in less than two months?\nThen I chanced upon a BBC news article from August 2016. When unable to locate IPs, MaxMind chose the geographical centre of the US as a default. This initially turned out to be a rented house in Kansas, which was rather unfortunate for the occupants, and brought upon them all kinds of unwanted attention.\nMaxMind subsequently changed its default centre points to be the middle of bodies of water. And this solved another puzzle. Some of the page views in London appeared to be in the middle of the River Thames."
  },
  {
    "objectID": "project/stories/index.html#r-toolbox",
    "href": "project/stories/index.html#r-toolbox",
    "title": "Surprising Stories",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nR.utils\ngunzip[1]\n\n\nbase\nas.character[1], basename[1], c[5], getwd[1], is.na[2], library[9], list[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\narrange[1], case_match[1], case_when[1], filter[1], if_else[2], mutate[3], rename[1]\n\n\nggplot2\ntheme_bw[1], theme_set[1]\n\n\nhtmlwidgets\nsaveWidget[1]\n\n\nleaflet\naddCircleMarkers[1], addLegend[1], addPolygons[1], addProviderTiles[1], colorFactor[1], highlightOptions[1], labelOptions[1], leaflet[1], providerTileOptions[1], setView[1]\n\n\npurrr\nlist_rbind[1], map2[1]\n\n\nreadr\nread_csv[1]\n\n\nrgdal\nreadOGR[1]\n\n\nrgeolocate\nmaxmind[1]\n\n\nstringr\nstr_c[2]\n\n\nusedthese\nused_here[1]\n\n\nutils\ndownload.file[2], unzip[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/happiness/index.html",
    "href": "project/happiness/index.html",
    "title": "Finding Happiness in The Smoke",
    "section": "",
    "text": "The Smoke, to use London’s nickname, has 32 boroughs plus the central business district known as the City of London. What does Cluster Analysis tell us about the characteristics that bind them?\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(tidyclust)\nlibrary(glue)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(ggrepel)\nlibrary(sf)\nlibrary(scales)\nlibrary(usedthese)\n\nconflict_scout()\nThe graphics will use a custom palette created in Adobe Colour.\ntheme_set(theme_bw())\n\ncols &lt;- c(\"#0AC449\", \"#CF4E0A\", \"#0057B7\", \"#FFD700\", \"#870AC4\") |&gt;\n  fct_inorder()\n\ntibble(x = 1:5, y = 1) |&gt;\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols), size = 4, vjust = 2, fill = \"white\") +\n  annotate(\n    \"label\",\n    x = 3, y = 0.5,\n    label = \"Custom Pallette\",\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\nThe London Datastore provides data profiling each area.\nraw_df &lt;-\n  read_xlsx(\"london-borough-profiles.xlsx\", sheet = 2) |&gt;\n  clean_names() |&gt;\n  filter(str_starts(code, \"E\")) |&gt;\n  mutate(across(where(is.character), \\(x) na_if(x, \".\")),\n    inner_outer_london = str_remove(inner_outer_london, \" London\")\n  )"
  },
  {
    "objectID": "project/happiness/index.html#dimensionality-reduction",
    "href": "project/happiness/index.html#dimensionality-reduction",
    "title": "Finding Happiness in The Smoke",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nThese data include 81 numeric variables quantifying such things as population density, happiness and age. Way too many variables to visualise two-dimensionally. Principal Components Analysis can reduce the bulk of the information down to two variables. It is then possible to more easily visualise the relationships.\nThe City of London, aka “The Square Mile”, is quite distinct from the other 32 areas and has many NA values.\n\nraw_df |&gt; \n  rowwise() |&gt; \n  mutate(na_count = sum(is.na(cur_data()))) |&gt; \n  select(area_name, na_count) |&gt;\n  filter(na_count != 0) |&gt;\n  arrange(desc(na_count))\n\n\n\n\narea_name\nna_count\n\n\n\nCity of London\n27\n\n\nKensington and Chelsea\n3\n\n\nBarnet\n1\n\n\nCamden\n1\n\n\nHackney\n1\n\n\nHaringey\n1\n\n\nHarrow\n1\n\n\nIslington\n1\n\n\nLewisham\n1\n\n\nMerton\n1\n\n\nRichmond upon Thames\n1\n\n\nWaltham Forest\n1\n\n\nWandsworth\n1\n\n\n\n\n\n\nNot surprisingly, the two-dimensional visualisation sets the City of London apart. And the other 32 are broadly, albeit with some mixing, divided into inner and outer London boroughs.\n\npca_fit &lt;- raw_df |&gt;\n  select(where(is.numeric)) |&gt;\n  prcomp(scale = TRUE)\n\npca_augmented &lt;-\n  pca_fit |&gt;\n  augment(raw_df)\n\npca_augmented |&gt;\n  ggplot(aes(.fittedPC1, .fittedPC2, fill = inner_outer_london)) +\n  geom_label(aes(label = area_name), size = 2, hjust = \"inward\") +\n  scale_fill_manual(values = as.character(cols)) +\n  labs(\n    title = \"33 London Areas\", fill = \"London\",\n    x = \"Principal Component 1\", y = \"Principal Component 2\",\n    caption = \"Source: data.london.gov.uk\"\n  )\n\n\n\n\nAfter squeezing the many dimensions into two, how much of the original information was it possible to retain?\n\npca_tidied &lt;- pca_fit |&gt;\n  tidy(matrix = \"eigenvalues\")\n\npct_explained &lt;-\n  pca_tidied |&gt;\n  pluck(\"cumulative\", 2)\n\npca_tidied |&gt;\n  ggplot(aes(PC, percent)) +\n  geom_col(aes(fill = if_else(PC &lt;= 2, TRUE, FALSE)),\n    alpha = 0.8, show.legend = FALSE\n  ) +\n  scale_y_continuous(labels = label_percent(1)) +\n  scale_fill_manual(values = as.character(cols)) +\n  coord_flip() +\n  labs(\n    title = glue(\n      \"{percent(pct_explained, 0.1)} of the \",\n      \"Variance Explained by Principal Components 1 & 2\"\n    ),\n    x = \"Principal Component\", y = NULL\n  )\n\n\n\n\nWhilst we do lose ease of interpretation by distilling the information in this way, it is still possible to understand which of the original variables influenced their two-dimensional positioning.\nThe axes depicted by the arrows below tell us that anxiety scores play a significant role in the placement of the City of London towards the upper-left. Average age pushes areas more towards the top. And happiness influences the bottom-right.\n\npattern &lt;-\n  str_c(\"_\\\\d{4}|_st.+|_score|_rates|^percent(_of)?_|\",\n        \"^proportion_of_|^population(_of)?_|^number(_of)?_|\", \n        \"_\\\\d{2}_out_of_\\\\d{2}|_estimate|_percent\")\n\npca_fit |&gt;\n  tidy(matrix = \"rotation\") |&gt;\n  pivot_wider(names_from = \"PC\", names_prefix = \"PC\", \n              values_from = \"value\") |&gt;\n  mutate(column = str_remove_all(column, pattern)) |&gt;\n  ggplot(aes(PC1, PC2)) +\n  geom_segment(\n    xend = 0, yend = 0, colour = \"grey70\",\n    arrow = arrow(ends = \"first\", length = unit(8, \"pt\"))\n  ) +\n  geom_text_repel(aes(label = column), size = 3) +\n  theme_minimal() +\n  labs(\n    x = \"PC 1\", y = \"PC 2\",\n    title = \"Characteristics Influencing Area Positioning\",\n    caption = \"Source: data.london.gov.uk\"\n  ) +\n  theme(axis.text = element_blank())\n\n\n\n\nThis may be validated by ranking all 33 areas by these three original variables.\n\npca_long &lt;- \n  pca_augmented |&gt;\n  select(area_name, matches(\"happ|anx|average_age\")) |&gt;\n  rename_with(~ str_remove(., \"_.*\")) |&gt;\n  rename(\"avg_age\" = \"average\") |&gt;\n  pivot_longer(-area, values_to = \"score\") |&gt;\n  mutate(area = reorder_within(area, score, name)) \n\npca_long |&gt;\n  ggplot(aes(area, score, colour = name)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~name, scales = \"free\") +\n  scale_x_reordered() +\n  scale_colour_manual(values = as.character(cols)) +\n  coord_flip() +\n  labs(x = NULL, caption = \"Source: data.london.gov.uk\")"
  },
  {
    "objectID": "project/happiness/index.html#cluster-modelling",
    "href": "project/happiness/index.html#cluster-modelling",
    "title": "Finding Happiness in The Smoke",
    "section": "Cluster Modelling",
    "text": "Cluster Modelling\nTo collect these areas into their natural groupings, a decision is needed on the desired number of clusters. We can visualise dividing the areas into 1, 2, 3 and so forth clusters. And, per below, 3 appears to nicely capture the natural grouping of the coloured points.\n\nset.seed(2022)\n\nkclusts &lt;-\n  tibble(k = 1:6) |&gt;\n  mutate(\n    kclust = map(k, \\(k) kmeans(\n      pca_augmented |&gt; select(.fittedPC1, .fittedPC2), k)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, pca_augmented)\n  )\n\nassignments &lt;-\n  kclusts |&gt;\n  unnest(cols = c(augmented))\n\nclusters &lt;-\n  kclusts |&gt;\n  unnest(cols = c(tidied))\n\nassignments |&gt;\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point(aes(color = .cluster)) +\n  facet_wrap(~k, nrow = 2) +\n  scale_colour_manual(values = as.character(cols[c(1:6)])) +\n  geom_point(data = clusters, size = 4, shape = 13) +\n  labs(\n    title = \"How Many Clusters Best Captures the Groupings?\",\n    subtitle = \"X Marks the Cluster Centre\",\n    caption = \"Source: data.london.gov.uk\"\n  )\n\n\n\n\nThe elbow method provides a more mathematical approach to the choice. The compactness of the clustering (as measured by the total within-cluster sum of squares) is significantly optimised when choosing 3 clusters, with diminishing returns thereafter.\n\nkclusts |&gt;\n  unnest(cols = c(glanced)) |&gt;\n  ggplot(aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point() +\n  geom_label(aes(label = if_else(k == 3, \"Elbow\", NA_character_)),\n    nudge_y = -25, fill = cols[1]\n  ) +\n  labs(\n    title = \"Elbow Method\",\n    x = \"Clusters\", y = \"Within-Cluster Variance\"\n  )\n\n\n\n\nAnd settling on this choice of 3 clusters, we get this split.\n\nassignments |&gt;\n  filter(k == 3) |&gt;\n  ggplot(aes(.fittedPC1, .fittedPC2, fill = .cluster)) +\n  geom_label(aes(label = area_name), \n             size = 2, hjust = \"inward\", overlap = FALSE) +\n  scale_fill_manual(values = as.character(cols[c(1, 2, 4)])) +\n  labs(\n    title = \"Closely-Related London Areas\", fill = \"Cluster\",\n    x = \"Principal Component 1\", y = \"Principal Component 2\",\n    caption = \"Source: data.london.gov.uk\"\n  )"
  },
  {
    "objectID": "project/happiness/index.html#using-tidymodels",
    "href": "project/happiness/index.html#using-tidymodels",
    "title": "Finding Happiness in The Smoke",
    "section": "Using Tidymodels",
    "text": "Using Tidymodels\nAn alternative approach is to use the new tidyclust(Hvitfeldt and Bodwin 2022) package which augments the tidymodels framework with a tidy unified interface to clustering models.\nFirst we tune the model with 1 to 6 clusters and review how well they capture the natural groupings.\n\nkmeans_spec &lt;- k_means(num_clusters = tune()) |&gt; \n  set_engine(\"stats\", algorithm = \"Hartigan-Wong\")\n\nkmeans_rec &lt;- raw_df |&gt; \n  select(where(is.numeric)) |&gt; \n  recipe(~ .) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), threshold = 0.9)\n\nkmeans_wflow &lt;- workflow() |&gt;\n  add_model(kmeans_spec) |&gt;\n  add_recipe(kmeans_rec)\n\nkmeans_cv &lt;- vfold_cv(pca_augmented, v = 5)\n\nkmeans_res &lt;- tune_cluster(\n  kmeans_wflow,\n  resamples = kmeans_cv,\n  grid = crossing(\n    num_clusters = seq(1, 6, 1)\n  ),\n  control = control_grid(save_pred = TRUE),\n  metrics = cluster_metric_set(sse_total, sse_ratio)\n)\n\nkmeans_metrics &lt;- kmeans_res |&gt; collect_metrics()\n\nkmeans_metrics |&gt;\n  filter(.metric == \"sse_ratio\") |&gt;\n  ggplot(aes(num_clusters, mean)) +\n  geom_point() +\n  geom_line() +\n  geom_label(aes(label = if_else(num_clusters == 3, \"Elbow\", NA_character_)),\n             nudge_y = -0.1, fill = cols[1]) +\n  labs(title = \"Elbow Method\", x = \"Clusters\", y = \"WSS\") +\n  scale_x_continuous(breaks = 1:6)\n\n\n\n\nAgain we can visualise the 3 clusters suggested by the elbow method.\n\nkmeans_spec &lt;- k_means(num_clusters = 3) |&gt; \n  set_engine(\"stats\", algorithm = \"Hartigan-Wong\")\n\nkmeans_wflow &lt;- kmeans_wflow |&gt; \n  update_model(kmeans_spec)\n\nkmeans_fit &lt;- kmeans_wflow |&gt; \n  fit(pca_augmented) \n\nkmeans_clust &lt;- kmeans_fit |&gt; \n  extract_centroids() |&gt; \n  rename_with(~ str_c(\".fitted\", .), starts_with(\"PC\"))\n\nkmeans_aug &lt;- kmeans_fit |&gt; \n  augment(pca_augmented)\n\nkmeans_aug |&gt;\n  mutate(.pred_cluster = str_remove(.pred_cluster, \"Cluster_\")) |&gt; \n  ggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_label(aes(label = area_name, colour = .pred_cluster),\n             size = 2, hjust = \"inward\") +\n  scale_colour_manual(values = as.character(cols[c(1:3, 5)])) +\n  geom_point(data = kmeans_clust, size = 4, shape = 13) +\n  labs(\n    title = \"Closely-Related London Areas\", fill = \"Cluster\",\n    subtitle = \"X Marks the Cluster Centre\",\n    x = \"Principal Component 1\", y = \"Principal Component 2\",\n    colour = \"Cluster\",\n    caption = \"Source: data.london.gov.uk\"\n  )\n\n\n\n\nHow does this look with geospatial data? And how do the clusters relate to inner and outer London?\n\nshape_df &lt;-\n  st_read(\"statistical-gis-boundaries-london/ESRI\",\n    \"London_Borough_Excluding_MHW\",\n    as_tibble = TRUE, quiet = TRUE\n  ) |&gt;\n  left_join(assignments |&gt; \n              filter(k == 3), by = join_by(GSS_CODE == code)) |&gt;\n  select(.cluster, inner_outer_london, NAME, geometry) |&gt;\n  pivot_longer(c(.cluster, inner_outer_london)) |&gt;\n  mutate(value = recode(value, \"1\" = \"Cluster 1\", \n                        \"2\" = \"Cluster 2\", \"3\" = \"Cluster 3\"))\n\nshape_df |&gt;\n  mutate(name = recode(name,\n    \".cluster\" = \"By Cluster\",\n    \"inner_outer_london\" = \"By Inner/Outer\"\n  )) |&gt;\n  ggplot() +\n  geom_sf(aes(fill = value), colour = \"white\") +\n  geom_sf_label(aes(label = NAME), size = 2, alpha = 0.7) +\n  scale_fill_manual(values = as.character(cols[c(3, 4, 1, 2, 5)])) +\n  facet_wrap(~name) +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  labs(fill = NULL)\n\n\n\n\nNot too dissimilar, but with some notable differences.\nThe City of London is a cluster apart in the heart of London. Kensington and Chelsea is an inner-London borough, but exhibits outer-London characteristics. And the reverse is true of the likes of Brent and Greenwich.\nDimensionality reduction is further explored in East-West Drift coupled with animation."
  },
  {
    "objectID": "project/happiness/index.html#r-toolbox",
    "href": "project/happiness/index.html#r-toolbox",
    "title": "Finding Happiness in The Smoke",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[8], c[9], is.na[1], library[12], seq[1], set.seed[1], sum[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nacross[1], arrange[1], cur_data[1], desc[1], filter[5], if_else[3], join_by[1], left_join[1], mutate[8], na_if[1], recode[2], rename[1], rename_with[2], rowwise[1], select[6], starts_with[1], where[3]\n\n\nforcats\nfct_inorder[1]\n\n\ngenerics\naugment[2], fit[1], tidy[2]\n\n\nggplot2\naes[21], annotate[1], arrow[1], coord_flip[2], element_blank[1], facet_wrap[3], geom_col[2], geom_label[6], geom_line[2], geom_point[6], geom_segment[1], geom_sf[1], geom_sf_label[1], ggplot[11], labs[10], scale_colour_manual[3], scale_fill_manual[5], scale_x_continuous[1], scale_y_continuous[1], theme[3], theme_bw[1], theme_minimal[1], theme_set[1], theme_void[2]\n\n\nggrepel\ngeom_text_repel[1]\n\n\nglue\nglue[1]\n\n\ngrid\nunit[1]\n\n\nhardhat\ntune[1]\n\n\njanitor\nclean_names[1]\n\n\nparsnip\nset_engine[2]\n\n\npurrr\nmap[4], pluck[1]\n\n\nreadxl\nread_xlsx[1]\n\n\nrecipes\nall_predictors[3], recipe[1], step_normalize[1], step_pca[1], step_zv[1]\n\n\nrsample\nvfold_cv[1]\n\n\nscales\nlabel_percent[1], number[1], percent[2]\n\n\nsf\nst_read[1]\n\n\nstats\nkmeans[1], prcomp[1]\n\n\nstringr\nstr_c[2], str_remove[3], str_remove_all[1], str_starts[1]\n\n\ntibble\ntibble[2]\n\n\ntidyclust\ncluster_metric_set[1], extract_centroids[1], k_means[2], tune_cluster[1]\n\n\ntidyr\ncrossing[1], pivot_longer[2], pivot_wider[1], population[1], unnest[3]\n\n\ntidyselect\nmatches[1]\n\n\ntidytext\nreorder_within[1], scale_x_reordered[1]\n\n\ntune\ncollect_metrics[1], control_grid[1]\n\n\nusedthese\nused_here[1]\n\n\nworkflows\nadd_model[1], add_recipe[1], update_model[1], workflow[1]"
  },
  {
    "objectID": "project/sets/index.html",
    "href": "project/sets/index.html",
    "title": "Where Clouds Cross",
    "section": "",
    "text": "When visualising a small number of overlapping sets, Venn diagrams work well. But what if there are more. Here’s a tidyverse approach to the exploration of sets and their intersections.\nIn Let’s Jitter I looked at a relatively simple set of cloud-service-related sales data. G-Cloud data offers a much richer source with many thousands of services documented by several thousand suppliers and hosted across myriad web pages. These services straddle many categories. I’ll use these data to explore the sets and where they cross.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(rvest)\nlibrary(furrr)\nlibrary(wesanderson)\nlibrary(tictoc)\nlibrary(ggupset)\nlibrary(ggVennDiagram)\nlibrary(glue)\nlibrary(usedthese)\n\nconflict_scout()\n\n3 conflicts:\n* `filter`        : [dplyr]\n* `guess_encoding`: rvest, readr\n* `lag`           : [dplyr]\n\nplan(multisession)\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(\"Royal2\"))\nI’m going to focus on the Cloud Hosting lot. Suppliers document the services they want to offer to Public Sector buyers. Each supplier is free to assign each of their services to one or more service categories. It would be interesting to see how these categories overlap when looking at the aggregated data.\nI’ll begin by harvesting the URL for each category’s search results. And I’ll also capture the number of search pages for each category. This will enable me to later control how R iterates through the web pages to extract the required data.\npath &lt;- \n  str_c(\"https://www.applytosupply.digitalmarketplace\", \n        \".service.gov.uk/g-cloud/search?lot=cloud-\")\n\nlot_urls &lt;-\n  c(\n    str_c(path, \"hosting\"),\n    str_c(path, \"software\"),\n    str_c(path, \"support\")\n  )\n\ncat_urls &lt;- future_map(lot_urls, \\(x) {\n  nodes &lt;- x |&gt;\n    read_html() |&gt;\n    html_elements(\".app-lot-filter__last-list li a\")\n\n  tibble(\n    url = nodes |&gt;\n      html_attr(\"href\"),\n\n    pages = nodes |&gt;\n      html_text()\n  )\n}) |&gt;\n  list_rbind() |&gt; \n  mutate(\n    pages = parse_number(as.character(pages)),\n    pages = if_else(pages %% 30 &gt; 0, pages %/% 30 + 1, pages %/% 30),\n    lot = str_extract(url, \"(?&lt;=cloud-)[\\\\w]+\"),\n    url = str_remove(url, \".*(?=&)\")\n  )\n\nversion &lt;- lot_urls[[1]] |&gt; \n  read_html() |&gt; \n  html_elements(\".app-search-result:first-child\") |&gt; \n  html_text() |&gt; \n  str_extract(\"G-Cloud \\\\d\\\\d\")\nSo now I’m all set to parallel process through the data at two levels. At category level. And within each category, I’ll iterate through the multiple pages of search results, harvesting 100 service IDs per page.\nI’ll also auto-abbreviate the category names so I’ll have the option of more concise names for less-cluttered plotting later on.\ntic()\n\ndata_df &lt;-\n  future_pmap(\n    list(\n      cat_urls$url,\n      cat_urls$pages,\n      cat_urls$lot\n    ),\n    \\(x, y, z) {\n      future_map_dfr(1:y, \\(y) {\n        refs &lt;- str_c(\n          \"https://www.applytosupply.digitalmarketplace\", \n          \".service.gov.uk/g-cloud/search?page=\",\n          y,\n          x,\n          \"&lot=cloud-\",\n          z\n        ) |&gt;\n          read_html() |&gt;\n          html_elements(\"#js-dm-live-search-results .govuk-link\") |&gt;\n          html_attr(\"href\") \n        \n       tibble(\n            lot = str_c(\"Cloud \", str_to_title(z)),\n1            service_id = str_extract(refs, \"[[:digit:]]{15}\"),\n            cat = str_remove(x, \"&serviceCategories=\") |&gt;\n2              str_replace_all(\"\\\\Q+\\\\E\", \" \") |&gt;\n              str_remove(\"%28[[:print:]]+%29\")\n          )\n      })\n    }\n  ) |&gt;\n  list_rbind() |&gt;\n  select(lot:cat) |&gt;\n  mutate(\n    cat = str_trim(cat) |&gt; str_to_title(),\n    abbr = str_remove(cat, \"and\") |&gt; abbreviate(3) |&gt; str_to_upper()\n  )\n\ntoc()\n\n\n1\n\n[[:digit:]]{15} finds the 15-digit service ID in the scraped link.\n\n2\n\n\\\\Q+\\\\E finds the literal + character rather than interpreting it as a one or more quantifier (i.e. if the \\\\Q and \\\\E were not specified)\n\n\n\n\n1347.41 sec elapsed\nNow that I have a nice tidy tibble (Müller and Wickham 2022), I can start to think about visualisations.\nI like Venn diagrams. But to create one I’ll first need to do a little prep as ggVennDiagram (Gao 2022) requires separate character vectors for each set.\nhost_df &lt;- data_df |&gt;\n  filter(lot == \"Cloud Hosting\") |&gt;\n  group_by(abbr)\n\nkeys &lt;- host_df |&gt; \n  group_keys() |&gt; \n  pull(abbr)\n\nall_cats &lt;- host_df |&gt; \n  group_split() |&gt;\n  map(\"service_id\") |&gt; \n  set_names(keys)\nVenn diagrams work best with a small number of sets. So we’ll select four categories.\nfour_cats &lt;- all_cats[c(\"CAAH\", \"PAAS\", \"OBS\", \"IND\")]\n\nfour_cats |&gt; \n  ggVennDiagram(label = \"count\", label_alpha = 0) +\n  scale_fill_gradient(low = cols[5], high = cols[3]) +\n  scale_colour_manual(values = cols[c(rep(4, 4))]) +\n  labs(\n    x = \"Category Combinations\", y = NULL, fill = \"# Services\",\n    title = \"The Most Frequent Category Combinations\",\n    subtitle = glue(\"Focusing on Four {version} Service Categories\"),\n    caption = \"Source: digitalmarketplace.service.gov.uk\\n\"\n  )\nLet’s suppose I want to find out which Service IDs lie in a particular intersection. Perhaps I want to go back to the web site with those IDs to search for, and read up on, those particular services. I could use purrr’s reduce to achieve this. For example, let’s extract the IDs at the heart of the Venn which intersect all categories.\nfour_cats |&gt; reduce(intersect)\n\n [1] \"498337261767401\" \"735625897584273\" \"941404079421892\" \"468519278288161\"\n [5] \"528818827198493\" \"590998313986731\" \"745846238180953\" \"173163384195854\"\n [9] \"924318408511326\" \"920078916328776\" \"507106315499984\" \"247181335014212\"\n[13] \"760565690434196\" \"567990943722560\" \"674396953847294\" \"546389562586229\"\n[17] \"616594390875571\" \"720996025285364\"\nAnd if we wanted the IDs intersecting the “OBS” and “IND” categories?\nlist(\n  four_cats$OBS,\n  four_cats$IND\n) |&gt;\n  reduce(intersect)\n\n [1] \"498337261767401\" \"622063745429810\" \"146737546710992\" \"735625897584273\"\n [5] \"282378513056803\" \"824432812764583\" \"941404079421892\" \"924245378460936\"\n [9] \"468519278288161\" \"979714835327372\" \"528818827198493\" \"361367891935175\"\n[13] \"590998313986731\" \"964621745018513\" \"745846238180953\" \"406334290207572\"\n[17] \"173163384195854\" \"924318408511326\" \"226716894364641\" \"920078916328776\"\n[21] \"507106315499984\" \"247181335014212\" \"760565690434196\" \"567990943722560\"\n[25] \"390438216681657\" \"263304084312287\" \"133984215794494\" \"674396953847294\"\n[29] \"761608237467474\" \"426708477587492\" \"147659063793653\" \"546389562586229\"\n[33] \"616594390875571\" \"172104444338022\" \"720996025285364\" \"266583255948268\"\n[37] \"420184478022971\" \"647839522738604\" \"746066603748154\" \"829256437326217\"\n[41] \"455997758057773\"\nSometimes though we need something a little more scalable than a Venn diagram. The ggupset package provides a good solution. Before we try more than four sets though, I’ll first use the same four categories so we may compare the visualisation to the Venn.\nset_df &lt;- data_df |&gt;\n  filter(abbr %in% c(\"CAAH\", \"PAAS\", \"OBS\", \"IND\")) |&gt;\n  mutate(category = list(cat), .by = service_id) |&gt;\n  distinct(service_id, category) |&gt;\n  mutate(n = n(), .by = category)\n\nset_df |&gt;\n  ggplot(aes(category)) +\n  geom_bar(fill = cols[1]) +\n  geom_label(aes(y = n, label = n), vjust = -0.1, size = 3, fill = cols[5]) +\n  scale_x_upset() +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  theme(panel.border = element_blank()) +\n  labs(\n    x = \"Category Combinations\", y = NULL,\n    title = \"The Most Frequent Category Combinations\",\n    subtitle = glue(\"Focusing on Four {version} Service Categories\"),\n    caption = \"Source: digitalmarketplace.service.gov.uk\"\n  )\nNow let’s take a look at the intersections across all the categories. And let’s suppose that our particular interest is all services which appear in one, and only one, category.\nset_df &lt;- data_df |&gt;\n  filter(n() == 1, lot == \"Cloud Hosting\", .by = service_id) |&gt;\n  mutate(category = list(cat), .by = service_id) |&gt;\n  distinct(service_id, category) |&gt;\n  mutate(n = n(), .by = category)\n\nset_df |&gt;\n  ggplot(aes(category)) +\n  geom_bar(fill = cols[2]) +\n  geom_label(aes(y = n, label = n), vjust = -0.1, size = 3, fill = cols[3]) +\n  scale_x_upset(n_sets = 10) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  theme(panel.border = element_blank()) +\n  labs(\n    x = \"Category Combinations\", y = NULL,\n    title = \"10 Most Frequent Single-Category Services\",\n    subtitle = \"Focused on Service Categories in the Cloud Hosting Lot\",\n    caption = \"Source: digitalmarketplace.service.gov.uk\"\n  )\nSuppose we want to extract the intersection data for the top intersections across all sets. I could use functions from the tidyr package to achieve this.\ncat_mix &lt;- data_df |&gt;\n  filter(lot == \"Cloud Hosting\") |&gt;\n  mutate(x = cat) |&gt;\n  pivot_wider(service_id, names_from = cat, values_from = x, values_fill = \"^\") |&gt;\n  unite(col = intersect, -service_id, sep = \"/\") |&gt;\n  count(intersect) |&gt;\n  mutate(\n    intersect = str_replace_all(intersect, \"(?:\\\\Q/^\\\\E|\\\\Q^/\\\\E)\", \"\"),\n    intersect = str_replace_all(intersect, \"/\", \" | \")\n  ) |&gt;\n  arrange(desc(n)) |&gt;\n  slice(1:21)\n\ncat_mix |&gt;\n  rename(\n    \"Intersecting Categories\" = intersect,\n    \"Services Count\" = n\n  )\n\n\n\n\n\n\n\n\n\nIntersecting Categories\nServices Count\n\n\n\n\nPlatform As A Service\n507\n\n\nCompute And Application Hosting\n183\n\n\nNetworking\n144\n\n\nArchiving Backup And Disaster Recovery\n101\n\n\nOther Storage Services\n94\n\n\nArchiving Backup And Disaster Recovery | Compute And Application Hosting | Nosql Database | Other Database Services | Networking | Platform As A Service | Search | Block Storage | Object Storage | Other Storage Services\n76\n\n\nLogging And Analysis\n67\n\n\nOther Database Services\n57\n\n\nInfrastructure And Platform Security\n57\n\n\nCompute And Application Hosting | Platform As A Service\n51\n\n\nArchiving Backup And Disaster Recovery | Compute And Application Hosting | Content Delivery Network | Data Warehousing | Distributed Denial Of Service Attack Protection | Firewall | Infrastructure And Platform Security | Intrusion Detection | Platform As A Service | Protective Monitoring\n39\n\n\nRelational Database\n38\n\n\nContainer Service\n34\n\n\nMessage Queuing And Processing\n29\n\n\nInfrastructure And Platform Security | Intrusion Detection | Logging And Analysis | Protective Monitoring\n27\n\n\nArchiving Backup And Disaster Recovery | Compute And Application Hosting | Nosql Database | Relational Database | Other Database Services | Networking | Platform As A Service | Search | Block Storage | Other Storage Services\n23\n\n\nArchiving Backup And Disaster Recovery | Compute And Application Hosting | Firewall | Infrastructure And Platform Security | Intrusion Detection | Load Balancing | Logging And Analysis | Networking | Platform As A Service | Protective Monitoring\n22\n\n\nBlock Storage | Object Storage | Other Storage Services\n22\n\n\nCompute And Application Hosting | Container Service | Platform As A Service\n21\n\n\nArchiving Backup And Disaster Recovery | Other Storage Services\n20\n\n\nInfrastructure And Platform Security | Networking\n20\nAnd I can compare this table to the equivalent ggupset (Ahlmann-Eltze 2020)visualisation.\nset_df &lt;- data_df |&gt;\n  filter(lot == \"Cloud Hosting\") |&gt;\n  mutate(category = list(cat), .by = service_id) |&gt;\n  distinct(service_id, category) |&gt;\n  mutate(n = n(), .by = category)\n\nset_df |&gt;\n  ggplot(aes(category)) +\n  geom_bar(fill = cols[5]) +\n  geom_label(aes(y = n, label = n), vjust = -0.1, size = 3, fill = cols[4]) +\n  scale_x_upset(n_sets = 22, n_intersections = 21) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +\n  theme(panel.border = element_blank()) +\n  labs(\n    x = \"Category Combinations\", y = NULL,\n    title = \"Top Intersections Across all Sets\",\n    subtitle = \"Focused on Service Categories in the Cloud Hosting Lot\",\n    caption = \"Source: digitalmarketplace.service.gov.uk\"\n  )\nAnd if I want to extract all the service IDs for the top 5 intersections, I could use dplyr (Wickham et al. 2022) and tidyr (Wickham and Girlich 2022) verbs to achieve this too.\nI won’t print them all out though!\ntop5_int &lt;- data_df |&gt;\n  filter(lot == \"Cloud Hosting\") |&gt;\n  select(service_id, abbr) |&gt;\n  mutate(x = abbr) |&gt;\n  pivot_wider(names_from = abbr, values_from = x, values_fill = \"^\") |&gt;\n  unite(col = intersect, -service_id, sep = \"/\") |&gt;\n  mutate(\n    intersect = str_replace_all(intersect, \"(?:\\\\Q/^\\\\E|\\\\Q^/\\\\E)\", \"\"),\n    intersect = str_replace(intersect, \"/\", \" | \")\n  ) |&gt;\n  mutate(count = n_distinct(service_id), .by = intersect) |&gt;\n  arrange(desc(count), intersect, service_id) |&gt;\n  add_count(intersect, wt = count, name = \"temp\") |&gt;\n  mutate(temp = dense_rank(desc(temp))) |&gt;\n  filter(temp %in% 1:5) |&gt;\n  distinct(service_id)\n\ntop5_int |&gt;\n  summarise(service_ids = n_distinct(service_id))\n\n\n\n\n\nservice_ids\n\n\n\n\n1029"
  },
  {
    "objectID": "project/sets/index.html#r-toolbox",
    "href": "project/sets/index.html#r-toolbox",
    "title": "Where Clouds Cross",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\n\nPackage\nFunction\n\n\n\n\nbase\nabbreviate[1], as.character[1], c[7], library[10], list[5], rep[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nadd_count[1], arrange[2], count[1], dense_rank[1], desc[3], distinct[4], filter[7], group_by[1], group_keys[1], group_split[1], if_else[1], mutate[14], n[4], n_distinct[2], pull[1], rename[1], select[2], slice[1], summarise[1]\n\n\nfurrr\nfuture_map[1], future_map_dfr[1], future_pmap[1]\n\n\nfuture\nplan[1]\n\n\nggVennDiagram\nggVennDiagram[1]\n\n\nggplot2\naes[6], element_blank[3], expansion[3], geom_bar[3], geom_label[3], ggplot[3], labs[4], scale_colour_manual[1], scale_fill_gradient[1], scale_y_continuous[3], theme[3], theme_bw[1], theme_set[1]\n\n\nggupset\nscale_x_upset[3]\n\n\nglue\nglue[2]\n\n\npurrr\nlist_rbind[2], map[1], reduce[2]\n\n\nreadr\nparse_number[1]\n\n\nrlang\nset_names[1]\n\n\nrvest\nhtml_attr[2], html_elements[3], html_text[2]\n\n\nstringr\nstr_c[6], str_extract[3], str_remove[4], str_replace[1], str_replace_all[4], str_to_title[2], str_to_upper[1], str_trim[1]\n\n\ntibble\ntibble[2]\n\n\ntictoc\ntic[1], toc[1]\n\n\ntidyr\npivot_wider[2], unite[2]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[1]\n\n\nxml2\nread_html[3]"
  },
  {
    "objectID": "project/monkeys/index.html",
    "href": "project/monkeys/index.html",
    "title": "An Infinite Number of Monkeys",
    "section": "",
    "text": "More on the monkeys later.\nFor now, the focus is on modelled house sales in London SW10 using quantile (and linear) regression. Several other “little projects” have looked at these residential properties from other perspectives:\noptions(xts.warn_dplyr_breaks_lag = FALSE)\n\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflicts_prefer(purrr::map)\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(scales)\nlibrary(glue)\nlibrary(SPARQL)\nlibrary(paletteer)\nlibrary(tsibble)\nlibrary(rvest)\nlibrary(Quandl)\nlibrary(corrr)\nlibrary(quantreg)\nlibrary(Qtools)\nlibrary(ggfx)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\nn &lt;- 4\npalette &lt;- \"wesanderson::Royal1\"\n\ncols &lt;- paletteer_d(palette, n = n)\n\ntibble(x = 1:n, y = 1) |&gt;\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols |&gt; str_remove(\"FF$\")), \n             size = 4, vjust = 2, fill = \"white\") +\n  annotate(\n    \"label\",\n    x = (n + 1) / 2, y = 0.5,\n    label = palette,\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\nHouse transaction data are provided by HM Land Registry Open Data.\nendpoint &lt;- \"https://landregistry.data.gov.uk/landregistry/query\"\n\nquery &lt;- 'PREFIX  text: &lt;http://jena.apache.org/text#&gt;\nPREFIX  ppd:  &lt;http://landregistry.data.gov.uk/def/ppi/&gt;\nPREFIX  lrcommon: &lt;http://landregistry.data.gov.uk/def/common/&gt;\n  \nSELECT  ?item ?ppd_propertyAddress ?ppd_hasTransaction ?ppd_pricePaid ?ppd_transactionCategory ?ppd_transactionDate ?ppd_transactionId ?ppd_estateType ?ppd_newBuild ?ppd_propertyAddressCounty ?ppd_propertyAddressDistrict ?ppd_propertyAddressLocality ?ppd_propertyAddressPaon ?ppd_propertyAddressPostcode ?ppd_propertyAddressSaon ?ppd_propertyAddressStreet ?ppd_propertyAddressTown ?ppd_propertyType ?ppd_recordStatus\n\nWHERE\n{ ?ppd_propertyAddress text:query _:b0 .\n  _:b0 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; lrcommon:postcode .\n  _:b0 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; _:b1 .\n  _:b1 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; \"( SW10 )\" .\n  _:b1 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; _:b2 .\n  _:b2 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#first&gt; 3000000 .\n  _:b2 &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#rest&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#nil&gt; .\n  ?item ppd:propertyAddress ?ppd_propertyAddress .\n  ?item ppd:hasTransaction ?ppd_hasTransaction .\n  ?item ppd:pricePaid ?ppd_pricePaid .\n  ?item ppd:transactionCategory ?ppd_transactionCategory .\n  ?item ppd:transactionDate ?ppd_transactionDate .\n  ?item ppd:transactionId ?ppd_transactionId\n  \n  OPTIONAL { ?item ppd:estateType ?ppd_estateType }\n  OPTIONAL { ?item ppd:newBuild ?ppd_newBuild }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:county ?ppd_propertyAddressCounty }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:district ?ppd_propertyAddressDistrict }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:locality ?ppd_propertyAddressLocality }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:paon ?ppd_propertyAddressPaon }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:postcode ?ppd_propertyAddressPostcode }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:saon ?ppd_propertyAddressSaon }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:street ?ppd_propertyAddressStreet }\n  OPTIONAL { ?ppd_propertyAddress lrcommon:town ?ppd_propertyAddressTown }\n  OPTIONAL { ?item ppd:propertyType ?ppd_propertyType }\n  OPTIONAL { ?item ppd:recordStatus ?ppd_recordStatus }\n}'\n\ndata_lst &lt;- SPARQL(endpoint, query)\nI intend to model the data by year-month and project two months beyond the last recorded actuals.\ndata_df &lt;- data_lst |&gt;\n  pluck(\"results\") |&gt;\n  as_tibble() |&gt;\n  mutate(\n    date = as_datetime(ppd_transactionDate) |&gt; as_date(),\n    amount = ppd_pricePaid,\n    cat = str_remove(ppd_transactionCategory, \n                     \"&lt;http://landregistry.data.gov.uk/def/ppi/\"),\n  ) |&gt;\n  filter(str_detect(cat, \"standard\")) |&gt;\n  arrange(date) |&gt;\n  mutate(yr_mon = yearmonth(date)) |&gt;\n  count(yr_mon)\n\nnext_month &lt;- data_df |&gt;\n  summarise(last(yr_mon) + 1) |&gt;\n  pull()\n\n# Add two months for predictions\ndata_df2 &lt;- data_df |&gt;\n  rows_insert(tibble(yr_mon = next_month), by = \"yr_mon\") |&gt;\n  rows_insert(tibble(yr_mon = next_month + 1), by = \"yr_mon\")\nThere are various sources available for macroeconomic factors that may affect house sales activity. Inflation and interest rates (the latter impacting mortgage rates) could be influential. SW10 has many overseas buyers, so the effective exchange rate versus a basket of currencies could too be a useful predictor.\nI’ll use the Quandl (2021) R package to grab most of these.\n# Price of a selection of goods & services for a typical consumer\ncpi_df &lt;- Quandl(\"RATEINF/CPI_GBR\") |&gt;\n  select(date = Date, cpi_macro = Value) |&gt;\n  arrange(date) |&gt;\n  mutate(yr_mon = yearmonth(date)) |&gt;\n  select(-date)\n\n# YOY rate of change in CPI\ninflation_df &lt;- Quandl(\"RATEINF/INFLATION_GBR\") |&gt;\n  select(date = Date, inflation_macro = Value) |&gt;\n  arrange(date) |&gt;\n  mutate(yr_mon = yearmonth(date)) |&gt;\n  select(-date)\n\n# BOE base rate\ninterest_df &lt;-\n  read_html(\"https://www.bankofengland.co.uk/boeapps/database/Bank-Rate.asp\") |&gt;\n  html_elements(\"#stats-table\") |&gt;\n  html_table() |&gt;\n  pluck(1) |&gt;\n  mutate(date = dmy(`Date Changed`)) |&gt;\n  select(date, interest_macro = Rate) |&gt;\n  arrange(date) |&gt;\n  mutate(yr_mon = yearmonth(date)) |&gt;\n  select(-date) |&gt;\n  slice_tail(n = 1, by = yr_mon)\n\n# Effective exchange rate of sterling versus multiple other currencies\nsterling_df &lt;- Quandl(\"BOE/XUDLBK67\") |&gt;\n  select(date = Date, sterling_macro = Value) |&gt;\n  arrange(date) |&gt;\n  mutate(yr_mon = yearmonth(date)) |&gt;\n  slice_tail(n = 1, by = yr_mon) |&gt;\n  select(-date)\n\nmacro_list &lt;-\n  list(\n    inflation_df,\n    cpi_df,\n    interest_df,\n    sterling_df\n  )\n\nmacro_df &lt;- reduce(macro_list, left_join, join_by(yr_mon)) |&gt;\n  arrange(yr_mon) |&gt;\n  fill(ends_with(\"macro\"), .direction = \"down\") |&gt;\n  drop_na()\n\nmacro_df |&gt;\n  pivot_longer(-yr_mon) |&gt;\n  mutate(name = str_remove(name, \"_macro\")) |&gt;\n  ggplot(aes(yr_mon, value)) +\n  geom_line(colour = \"grey70\") +\n  geom_smooth() +\n  facet_wrap(~name, scales = \"free_y\", nrow = 1) +\n  labs(title = \"Macroeconomic Factors\", x = NULL) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nThe UK government imposes a stamp duty on house buyers as a percentage of the sale price, so changes in the rate, particularly the top rate, could be a helpful input to the model.\nstamp_df &lt;- read_html(\"https://www.investmentguide.co.uk/historical-stamp-duty/\") |&gt;\n  html_elements(\"strong , .column-1, .column-2\") |&gt;\n  html_text() |&gt;\n  as_tibble() |&gt;\n  filter(!value %in% c(\"Rate\", \"Charge band\")) |&gt;\n  mutate(type = case_when(\n    str_detect(value, \"%\") ~ \"rate\",\n    str_detect(value, \"£\") ~ \"band\",\n    .default = \"date\"\n  )) |&gt;\n  mutate(yr_mon = if_else(type == \"date\", yearmonth(dmy(value)), NA)) |&gt;\n  fill(yr_mon) |&gt;\n  filter(type != \"date\") |&gt;\n  mutate(row = row_number(), .by = c(yr_mon, type)) |&gt;\n  pivot_wider(names_from = type, values_from = value) |&gt;\n  separate_wider_delim(band, \" and under \",\n    names = c(\"from\", \"to\"), too_few = \"align_start\"\n  ) |&gt;\n  mutate(\n    to = if_else(str_starts(from, \"Up to\"), parse_number(from), parse_number(to)),\n    to = replace_na(to, Inf)\n  ) |&gt;\n  select(-from)\n\nstamp_df2 &lt;- stamp_df |&gt;\n  filter(to == Inf) |&gt;\n  select(yr_mon, stamp_macro = rate) |&gt;\n  mutate(stamp_macro = parse_number(stamp_macro))\n\n# saveRDS(stamp_df, \"stamp_df\")\nScatter plots and correlations suggest these could be worth incorporating.\nA further refinement would be to try the correlations at various lags to see where the relationship is strongest. This is because it often takes weeks from decision-to-sell to completion. On the other hand one can often get a broad sense of the direction of some of these factors ahead of time.\nMonths are included as, for example, there are typically weaker sales in winter.\njoin_df &lt;- data_df2 |&gt;\n  left_join(macro_df, join_by(yr_mon == yr_mon)) |&gt;\n  left_join(stamp_df2, join_by(closest(yr_mon &gt;= yr_mon))) |&gt;\n  select(-starts_with(\"date\"), -yr_mon.y) |&gt;\n  mutate(across(ends_with(\"_macro\"), lag, 2)) |&gt;\n  drop_na(-n) |&gt;\n  rename(yr_mon = yr_mon.x) |&gt;\n  mutate(month = month(yr_mon))\n\njoin_df |&gt; \n  mutate(month = month(yr_mon, label = TRUE)) |&gt; \n  ggplot(aes(month, n, group = month)) +\n  geom_boxplot() +\n  labs(title = \"Sales by Month\")\n\n\n\njoin_df |&gt;\n  select(-month) |&gt;\n  correlate() |&gt;\n  focus(n) |&gt;\n  arrange(n)\n\n\n\n\nterm\nn\n\n\n\ncpi_macro\n-0.6455062\n\n\nstamp_macro\n-0.6009742\n\n\ninflation_macro\n-0.2273802\n\n\nsterling_macro\n0.5551985\n\n\ninterest_macro\n0.5702645\n\n\n\n\n\njoin_df |&gt;\n  pivot_longer(cols = ends_with(\"_macro\"), names_pattern = \"(.*)_macro\") |&gt;\n  ggplot(aes(value, n)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~name, scales = \"free_x\", nrow = 1)\nThe quantreg (Koenker 2022) R package is used here to model the 0.5 (median), 0.05 and 0.95 quantiles, Qtools (Geraci 2022) to extract the goodness-of-fit and broom (Robinson, Hayes, and Couch 2023) to tidy the term estimates.\nset.seed(123)\n\nrq_fit &lt;- rq(\n  n ~ cpi_macro + sterling_macro + interest_macro +\n    stamp_macro + inflation_macro + month + yr_mon,\n  data = join_df,\n  tau = c(0.05, 0.5, 0.95)\n)\n\nbroom::tidy(rq_fit)\n\n\n\n\nterm\nestimate\nconf.low\nconf.high\ntau\n\n\n\n(Intercept)\n17.4009501\n-14.7567771\n59.4204115\n0.05\n\n\ncpi_macro\n1.2033609\n0.5643921\n2.0407098\n0.05\n\n\nsterling_macro\n0.5006562\n0.0987317\n0.6939156\n0.05\n\n\ninterest_macro\n-2.7824965\n-5.7330986\n0.9568033\n0.05\n\n\nstamp_macro\n-0.8282729\n-2.0024868\n0.6587759\n0.05\n\n\ninflation_macro\n-0.6752318\n-3.3044990\n1.1818529\n0.05\n\n\nmonth\n0.0937169\n-0.3222418\n0.9383533\n0.05\n\n\nyr_mon\n-0.0095708\n-0.0162135\n-0.0069172\n0.05\n\n\n(Intercept)\n-21.3647250\n-54.1113789\n22.7165990\n0.50\n\n\ncpi_macro\n1.0272546\n0.3906825\n1.6199574\n0.50\n\n\nsterling_macro\n0.7949216\n0.5864052\n1.0089153\n0.50\n\n\ninterest_macro\n-1.2533022\n-2.8965037\n0.3466491\n0.50\n\n\nstamp_macro\n-1.4047908\n-2.5197048\n-0.2199744\n0.50\n\n\ninflation_macro\n-0.9364530\n-1.8156209\n0.7039052\n0.50\n\n\nmonth\n-0.0556925\n-0.3636425\n0.3755751\n0.50\n\n\nyr_mon\n-0.0064439\n-0.0101636\n-0.0041408\n0.50\n\n\n(Intercept)\n-15.3193185\n-206.2506214\n66.5261549\n0.95\n\n\ncpi_macro\n1.6141478\n0.0050755\n4.9586657\n0.95\n\n\nsterling_macro\n0.8385557\n-0.1130984\n1.7694980\n0.95\n\n\ninterest_macro\n-0.4391986\n-4.6575196\n5.9785501\n0.95\n\n\nstamp_macro\n-1.0817821\n-6.1706778\n4.0196441\n0.95\n\n\ninflation_macro\n-2.6217660\n-8.0469028\n1.6613280\n0.95\n\n\nmonth\n0.0435900\n-1.3331433\n1.6996293\n0.95\n\n\nyr_mon\n-0.0095591\n-0.0228147\n0.0044122\n0.95\n\n\n\n\n\nGOFTest(rq_fit)\n\nGoodness-of-fit test for quantile regression based on the cusum process \nA large test statistic (small p-value) is evidence of lack of fit \nQuantile 0.05: Test statistic = 7e-04; p-value = 0.62 \nQuantile 0.5: Test statistic = 0.0061; p-value = 0.23 \nQuantile 0.95: Test statistic = 0.001; p-value = 0.53 \n\nlm_fit &lt;- rq(\n  n ~ cpi_macro + sterling_macro + interest_macro +\n    stamp_macro + inflation_macro + month + yr_mon,\n  data = join_df\n)\n\nrq_preds &lt;- rq_fit |&gt;\n  predict(join_df,\n    type = \"quantiles\",\n    quantiles = c(0.05, 0.5, 0.95)\n  ) |&gt;\n  as_tibble() |&gt;\n  rename(\n    lower = `tau= 0.05`,\n    median = `tau= 0.50`,\n    upper = `tau= 0.95`\n  ) |&gt;\n  bind_cols(join_df) |&gt;\n  mutate(coverage = if_else(between(n, lower, upper), TRUE, FALSE))\n\nlm_preds &lt;- lm_fit |&gt;\n  predict(join_df) |&gt;\n  as_tibble() |&gt;\n  bind_cols(join_df) |&gt;\n  select(yr_mon, lm = value)\nThe goodness-of-fit seems reasonable, so let’s visualise the quantile and linear regressions along with the actual sales for comparison.\ncoverage &lt;- rq_preds |&gt; \n  summarise(coverage = percent(mean(coverage, na.rm = TRUE), 0.1)) |&gt; \n  pull()\n\nrq_preds |&gt;\n  left_join(lm_preds, join_by(yr_mon == yr_mon)) |&gt;\n  ggplot(aes(yr_mon, median)) +\n  as_reference(geom_ribbon(aes(ymin = lower, ymax = upper), \n                           fill = cols[1]), id = \"ribbon\") +\n  with_blend(\n    annotate(\n      \"rect\",\n      xmin = ymd(\"2022-12-31\"), xmax = ymd(\"2023-02-28\"),\n      ymin = -Inf, ymax = Inf, fill = cols[2], linetype = \"dashed\"\n    ),\n    bg_layer = \"ribbon\", blend_type = \"atop\"\n  ) +\n  geom_line(aes(y = n), colour = \"black\") +\n  geom_line(colour = \"white\", linewidth = 1) +\n  geom_line(aes(y = lm), colour = cols[4], linetype = \"dashed\") +\n  geom_vline(xintercept = ymd(\"2008-09-06\"), \n             linetype = \"dashed\", colour = \"grey30\") +\n  annotate(\"label\",\n    x = yearmonth(\"2008 Sep\"), y = 100,\n    label = \"Lehman\\nBrothers\\nCollapses\", size = 3\n  ) +\n  geom_vline(xintercept = ymd(\"2014-12-03\"), \n             linetype = \"dashed\", colour = \"grey30\") +\n  annotate(\"label\",\n    x = yearmonth(\"2014 Dec\"), y = 100,\n    label = \"Jump in\\nTop-rate\\nStamp\\nDuty\", size = 3\n  ) +\n  geom_vline(xintercept = ymd(\"2016-06-23\"), \n             linetype = \"dashed\", colour = \"grey30\") +\n  annotate(\"label\",\n    x = yearmonth(\"2016 Jun\"), y = 65,\n    label = \"Brexit\\nVote\", size = 3\n  ) +\n  annotate(\"label\",\n    x = yearmonth(\"2020 Jun\"), y = 120,\n    label = glue(\n      \"Actual (Black)\\nLinear (Dashed Orange)\\n\",\n      \"Quantiles (Grey / White)\\nPredicted (Red / White)\"\n    ),\n    size = 3\n  ) +\n  annotate(\"label\",\n    x = yearmonth(\"1999 Jan\"), y = 125,\n    label = glue(\"{coverage} Coverage\"), \n    size = 3, fill = cols[1], colour = \"white\"\n  ) +\n  scale_x_yearmonth(date_breaks = \"2 years\") +\n  labs(\n    title = \"Monthly House Sales in London Postcode Area SW10\",\n    subtitle = \"Quantile (0.05, 0.5, 0.95) & Linear Regression\",\n    x = NULL, y = \"Number of Sales\", fill = NULL\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nThe 90% prediction interval (0.05 to 0.95 quantiles represented by the grey ribbon) covers 89.2% of the historical observations and suggests a 95% probability of no more than 26 sales in February.\nOf course that means there is a one-in-twenty chance of more, and an even smaller chance of repeating the summer of 2003. Anything can happen eventually.\nAs Bob Newhart pointed out, an infinite number of monkeys given enough time, could one day type out all the great books! A wide prediction interval though would suggest gibberish. So, you might want to busy yourself with other things in the meantime."
  },
  {
    "objectID": "project/monkeys/index.html#r-toolbox",
    "href": "project/monkeys/index.html#r-toolbox",
    "title": "An Infinite Number of Monkeys",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nQtools\nGOFTest[1]\n\n\nQuandl\nQuandl[3]\n\n\nSPARQL\nSPARQL[1]\n\n\nbase\nas.character[1], c[5], library[14], list[1], mean[1], options[1], set.seed[1]\n\n\nbroom\ntidy[1]\n\n\nconflicted\nconflict_prefer[1], conflict_prefer_all[1], conflict_scout[1], conflicts_prefer[1]\n\n\ncorrr\ncorrelate[1], focus[1]\n\n\ndplyr\nacross[1], arrange[7], between[1], bind_cols[2], case_when[1], count[1], filter[4], if_else[3], join_by[4], last[1], left_join[3], mutate[17], pull[2], rename[2], row_number[1], rows_insert[2], select[13], slice_tail[2], summarise[2]\n\n\nggfx\nas_reference[1], with_blend[1]\n\n\nggplot2\naes[9], annotate[7], element_text[2], facet_wrap[2], geom_boxplot[1], geom_col[1], geom_label[1], geom_line[4], geom_point[1], geom_ribbon[1], geom_smooth[2], geom_vline[3], ggplot[5], labs[3], scale_fill_manual[1], theme[3], theme_bw[1], theme_set[1], theme_void[1]\n\n\nglue\nglue[2]\n\n\nlubridate\nas_date[1], as_datetime[1], dmy[2], month[2], ymd[5]\n\n\npaletteer\npaletteer_d[1]\n\n\npurrr\npluck[2], reduce[1]\n\n\nquantreg\nrq[2]\n\n\nreadr\nparse_number[3]\n\n\nrvest\nhtml_elements[2], html_table[1], html_text[1]\n\n\nscales\npercent[1]\n\n\nstats\npredict[2]\n\n\nstringr\nstr_detect[3], str_remove[3], str_starts[1]\n\n\ntibble\nas_tibble[4], tibble[3]\n\n\ntidyr\ndrop_na[2], fill[2], pivot_longer[2], pivot_wider[1], replace_na[1], separate_wider_delim[1]\n\n\ntidyselect\nends_with[3], starts_with[1]\n\n\ntsibble\nscale_x_yearmonth[1], yearmonth[11]\n\n\nusedthese\nused_here[1]\n\n\nxml2\nread_html[2]"
  },
  {
    "objectID": "project/jitter/index.html",
    "href": "project/jitter/index.html",
    "title": "Let’s Jitter",
    "section": "",
    "text": "Welcome to the tidyverse (Wickham et al. 2019) with data ingestion, cleaning and tidying. And some visualisations of sales data with a little jittering.\nThis first little project uses the tidyverse collection of packages to import, explore and visualise some sales data. The UK Government’s Digital Marketplace provides a rich and varied source of public data under the Open Government Licence 1.\nThe marketplace was set up with an intent to break down barriers that impede Small and Medium Enterprises (SMEs) from bidding for Public Sector contracts. So, let’s see how that’s going.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflict_prefer(\"as_date\", \"lubridate\")\nlibrary(clock)\nconflict_prefer(\"date_format\", \"clock\")\nlibrary(janitor)\nlibrary(scales)\nlibrary(paletteer)\nlibrary(glue)\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\nn &lt;- 4\npalette &lt;- \"wesanderson::Royal1\"\n\ncols &lt;- paletteer_d(palette, n = n)\n\ntibble(x = 1:n, y = 1) |&gt;\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols |&gt; str_remove(\"FF$\")), \n             size = 4, vjust = 2, fill = \"white\") +\n  annotate(\n    \"label\",\n    x = (n + 1) / 2, y = 0.5,\n    label = palette,\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\nThe tidyverse framework sits at the heart of all my data science work as evidenced in my favourite things. So I’ll begin by using two of my most used tidyverse packages (readr (Wickham, Hester, and Bryan 2022) and dplyr (Wickham et al. 2022)) to import and tidy the cloud services (G-Cloud) sales data.\nWild data are often scruffy affairs. Cleaning and tidying is a necessary first step. In the case of these data, there are characters in an otherwise numeric spend column. And the date column is a mix of two formats.\nurl &lt;- str_c(\n  \"https://www.gov.uk/government/\",\n  \"uploads/system/uploads/attachment_data/\",\n  \"file/639799/g-cloud-sales-figures-july2017.csv\"\n)\n\ngcloud_df &lt;-\n  read_csv(url) |&gt; \n  clean_names() |&gt; \n  mutate(\n    evidenced_spend = str_remove_all(evidenced_spend, \"[^0-9-]\") |&gt;\n      parse_number(),\n    spend_date = as_date(as.numeric(return_month), origin = \"1899-12-30\"),\n    spend_date = if_else(\n      is.na(spend_date),\n      dmy(return_month),\n      spend_date\n    ),\n    sme_status = if_else(sme_status == \"SME\", \"SME\", \"Non-SME\"),\n    sme_spend = if_else(sme_status == \"SME\", evidenced_spend, 0)\n  )\nNow we can summarise and visualise how the SME share has changed over time using the ggplot2 package.\nshare_df &lt;- gcloud_df |&gt; \n  summarise(\n    evidenced_spend = sum(evidenced_spend, na.rm = TRUE),\n    sme_spend = sum(sme_spend, na.rm = TRUE),\n    pct = sme_spend / evidenced_spend, \n    .by = spend_date\n  ) |&gt; \n  arrange(spend_date)\n\nlast_date &lt;- gcloud_df |&gt; \n  summarise(max(spend_date)) |&gt; \n  pull() |&gt; \n  date_format(format = \"%B %d, %Y\")\n\nshare_df |&gt; \n  ggplot(aes(spend_date, pct)) +\n  geom_point(colour = cols[4]) +\n  geom_smooth(colour = cols[2], fill = cols[3]) +\n  scale_y_continuous(labels = label_percent()) +\n  scale_x_date(date_breaks = \"years\", date_labels = \"%Y\") +\n  labs(\n    x = NULL, y = NULL,\n    title = glue(\"SME Share of G-Cloud to {last_date}\"), \n    subtitle = \"Dots = % Monthly Sales via SMEs\",\n    caption = \"Source: GOV.UK G-Cloud Sales\"\n  )\nSales grew steadily to a cumulative £2.4B by July 2017. And as the volume of sales grew, an increasingly clearer picture of sustained growth in the SME share emerged. However, in those latter few months, SMEs lost a little ground.\nDig a little deeper, and one can also see variation by sub-sector. And that’s after setting aside those buyers with cumulative G-Cloud spend below £100k, where large enterprise suppliers are less inclined to compete.\nsector_df &lt;- gcloud_df |&gt;\n  mutate(sector = if_else(\n    sector %in% c(\"Central Government\", \"Local Government\", \n                  \"Police\", \"Health\"), sector, \"Other Sector\")\n    ) |&gt;\n  summarise(\n    evidenced_spend = sum(evidenced_spend, na.rm = TRUE),\n    sme_spend = sum(sme_spend, na.rm = TRUE),\n    pct = sme_spend / evidenced_spend,\n    .by = c(customer_name, sector)\n  ) |&gt;\n  filter(evidenced_spend &gt;= 100000) |&gt;\n  mutate(median_pct = median(pct), .by = sector) |&gt;\n  mutate(sector = fct_reorder(sector, median_pct))\n\nn_df &lt;- sector_df |&gt; summarise(n = n(), .by = sector)\n\nsector_df |&gt; \n  ggplot(aes(sector, pct)) +\n  geom_boxplot(outlier.shape = FALSE, fill = cols[3]) +\n  geom_jitter(width = 0.2, alpha = 0.5, colour = cols[2]) +\n  geom_label(aes(y = .75, label = glue(\"n = {n}\")),\n    data = n_df,\n    fill = cols[1], colour = \"white\"\n  ) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(\n    x = NULL, y = NULL,\n    title = glue(\"SME Share of G-Cloud to {last_date}\"),\n    subtitle = \"% Sales via SMEs for Buyers with Cumulative Sales &gt;= £100k\",\n    caption = \"Source: gov.uk G-Cloud Sales\"\n  )\nThe box plot, overlaid with jittered points to avoid over-plotting, shows:\nSo, irrespective of whether service integration is taken in-house or handled by a service integrator, large enterprise suppliers have much to offer:\nSMEs offer flexibility, fresh thinking and broader competition, often deploying their resources and building their mission around a narrower focus. They tend to do one thing, or a few things, exceptionally well.\nThese data are explored further in Six months later and Can Ravens Forecast."
  },
  {
    "objectID": "project/jitter/index.html#r-toolbox",
    "href": "project/jitter/index.html#r-toolbox",
    "title": "Let’s Jitter",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[1], as.numeric[1], c[2], is.na[1], library[8], max[1], sum[4]\n\n\nclock\ndate_format[1]\n\n\nconflicted\nconflict_prefer[2], conflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\narrange[1], filter[1], if_else[4], mutate[4], n[1], pull[1], summarise[4]\n\n\nforcats\nfct_reorder[1]\n\n\nggplot2\naes[5], annotate[1], geom_boxplot[1], geom_col[1], geom_jitter[1], geom_label[2], geom_point[1], geom_smooth[1], ggplot[3], labs[2], scale_fill_manual[1], scale_x_date[1], scale_y_continuous[2], theme[1], theme_bw[1], theme_set[1], theme_void[1]\n\n\nglue\nglue[3]\n\n\njanitor\nclean_names[1]\n\n\nlubridate\nas_date[1], dmy[1]\n\n\npaletteer\npaletteer_d[1]\n\n\nreadr\nparse_number[1], read_csv[1]\n\n\nscales\nlabel_percent[2]\n\n\nstats\nmedian[1]\n\n\nstringr\nstr_c[1], str_remove[1], str_remove_all[1]\n\n\ntibble\ntibble[1]\n\n\nusedthese\nused_here[1]"
  },
  {
    "objectID": "project/six/index.html",
    "href": "project/six/index.html",
    "title": "Six Months Later",
    "section": "",
    "text": "In September 2017 I wrote a post entitled Let’s Jitter. It looked at how the SME share of sales evolved over time. I revisited this topic here six months later, along the way adding a splash of colour and some faceted plots.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(clock)\nlibrary(scales)\nlibrary(wesanderson)\nlibrary(usedthese)\n\nconflict_scout()\n\n5 conflicts:\n* `col_factor` : scales, readr\n* `date_format`: scales, clock\n* `discard`    : scales, purrr\n* `filter`     : [dplyr]\n* `lag`        : [dplyr]\nI’m using one of the beautiful range of Wes Anderson Palettes. I often use the palettes provided by ColorBrewer too.\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(9, name = \"Chevalier1\", type = \"continuous\"))\nIn Let’s Jitter I assumed the G-Cloud data file adopted the UK Government standard of UTF-8. I used the stringr package to fix any issues.\nThis time around, I’m importing the files for two frameworks (G-Cloud and DOS) after first checking the encoding to see if I can get a cleaner import. guess_encoding suggests these files use the ISO-8859-1 standard.\nurl &lt;- \n  \"https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/\"\n\ngcloud_csv &lt;- str_c(url, \"678283/G-Cloud-spend-Dec2017.csv\")\n\ndos_csv &lt;- str_c(url, \"678286/DOS-spend-Dec2017.csv\")\n\nnames &lt;- c(gcloud_csv, dos_csv)\n\nmap(names, guess_encoding)\n\n[[1]]\n# A tibble: 2 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.39\n2 ISO-8859-2       0.24\n\n[[2]]\n# A tibble: 2 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.44\n2 ISO-8859-2       0.23\nNext I’ll set up a vector of column names to apply consistently to both files, import the data with the suggested encoding, and bind them into one tibble.\ncolnam &lt;-\n  c(\"sector\",\n    \"lot\",\n    \"month\",\n    \"spend\",\n    \"status\",\n    \"supplier\",\n    \"customer\",\n    \"framework\")\n\nread_dm &lt;- \\(x){\n  read_csv(\n    x,\n    col_names = colnam,\n    skip = 1,\n    locale = locale(encoding = \"ISO-8859-1\"),\n    col_types = NULL,\n    show_col_types = FALSE)\n}\n\ncombined_df &lt;- map(names, read_dm) |&gt; \n  set_names(c(\"gcloud\", \"dos\")) |&gt; \n  bind_rows() |&gt; \n  mutate(framework = if_else(is.na(framework), \"DOS\", framework))\nI’d like to create some new features: Month-end dates, something to distinguish between the two frameworks (G-Cloud or DOS) and the framework version (i.e. G-Cloud 1 to 9). The spend has a messy format and needs a bit of cleaning too.\nclean_df &lt;- combined_df |&gt;\n  mutate(\n    month_end = date_parse(str_c(month, \"01\", sep = \"-\"), \n                           format = \"%b-%y-%d\") |&gt; \n      add_months(1) |&gt; add_days(-1),\n    version = str_remove(framework, fixed(\"-Cloud \")),\n    version = str_replace(version, fixed(\"G-Cloud\"), \"G1\"),\n    version = str_replace(version, fixed(\"GIII\"), \"G3\"),\n    version = str_replace(version, fixed(\"GServices II\"), \"G2\"),\n    framework = str_extract(framework, \".{3,7}\"),\n    spend = str_remove(spend, fixed(\"£\")),\n    spend = str_replace(spend, \"^\\\\(\", \"-\"),\n    spend = parse_number(spend),\n    SME_spend = if_else(status == \"SME\", spend, 0)\n  )\nFinding the interval between G-Cloud versions will enable me to calculate and print the average in the next paragraph using inline r code.\nversion_df &lt;- clean_df |&gt;\n  filter(version != \"DOS\") |&gt; \n  summarise(start = min(month_end), .by = version) |&gt; \n  mutate(next_start = lead(start),\n         interval = lubridate::interval(start, next_start) %/% months(1))\nEvery 7.8 months, on average, suppliers are asked to resubmit their G-Cloud offerings with their latest pricing and service descriptions. It’s a chance for new suppliers, often smaller ones, to join the existing list of suppliers and increase overall competitiveness for Cloud services.\nLet’s visualise how each of these framework versions grows, matures and fades away as the next one takes over.\nvers_summary &lt;- clean_df |&gt;\n  filter(version != \"DOS\") |&gt; \n  summarise(sales = sum(spend) / 1000000,\n            .by = c(version, month_end))\n\nvers_summary |&gt; \n  ggplot(aes(month_end, sales, colour = version)) +\n  geom_line() +\n  geom_smooth(linewidth = 2) +\n  scale_x_date(date_breaks = \"years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = label_dollar(prefix = \"£\", suffix = \"m\")) +\n  scale_colour_manual(values = cols) +\n  labs(x = NULL, y = NULL, title = \"The Lifecycle of G-Cloud Versions\", \n       subtitle = \"Monthly Sales by Version\") + \n  labs(caption = \"\\nSource: GOV.UK's Digital Marketplace\")\nLet’s Jitter showed signs of a weakening in the SME share of G-Cloud sales by value. This plot shows this trend to have persisted, and also reflects the Digital Outcomes & Specialists (DOS) framework exhibiting a downward trend.\nfw_summary &lt;- clean_df |&gt;\n  summarise(pct = sum(SME_spend, na.rm = TRUE) / sum(spend, na.rm = TRUE),\n            .by = c(framework, month_end))\n\nfw_summary |&gt; \n  ggplot(aes(month_end, pct, colour = framework)) +\n  geom_line() +\n  geom_smooth(linewidth = 2) +\n  scale_y_continuous(breaks = c(0.25, 0.5, 0.75, 1), labels = label_percent()) +\n  scale_x_date(date_breaks = \"years\", date_labels = \"%Y\") +\n  scale_colour_manual(values = cols[c(1, 9)]) +\n  labs(x = NULL, y = NULL, \n       title = \"The Waning SME Share of Sales\", \n       subtitle = \"% Monthly Sales Value via SME (vs Large Enterprise) Suppliers\") + \n  labs(caption = \"\\nSource: GOV.UK's Digital Marketplace\")\nOverall spending via the combined frameworks however continues to grow across all parts of Public Sector. I’ll use a small multiples visualisation technique to show this using ggplot2’s(Wickham 2016) facet_wrap.\nsect_summary &lt;-\n  clean_df |&gt;\n  filter(!sector %in% c(\n    \"Unregistered or Unknown\",\n    \"Utility (Historic)\",\n    \"Wider Public Sector\"\n  )) |&gt;\n  summarise(\n    sales = sum(spend) / 1000000,\n    pct = sum(SME_spend, na.rm = TRUE) / sum(spend, na.rm = TRUE),\n    .by = c(sector, month_end)\n  )\n\nsect_summary |&gt; \n  ggplot(aes(month_end, sales, colour = sector)) +\n  geom_line() +\n  geom_smooth(size = 2) +\n  facet_wrap(~ sector, scales = \"free_y\") +\n  theme(legend.position = \"none\") +\n  scale_x_date(date_breaks = \"years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = label_dollar(prefix = \"£\", suffix = \"m\")) +\n  scale_colour_manual(values = cols) +\n  labs(x = NULL, y = NULL, \n       title = \"All Sectors Increase Digital Marketplace Spend\", \n       subtitle = \"G-Cloud & DOS Spend by Sector\") + \n  labs(caption = \"\\nSource: GOV.UK's Digital Marketplace\")\nThe decline in the proportion of spend via SMEs is also fairly broad-based.\nsect_summary |&gt; \n  ggplot(aes(month_end, pct, colour = sector)) +\n  geom_line() +\n  geom_smooth(size = 2) +\n  facet_wrap(~ sector, scales = \"free_y\") +\n  theme(legend.position = \"none\") +\n  scale_x_date(date_breaks = \"years\", date_labels = \"%Y\") +\n  scale_y_continuous(labels = label_percent()) +\n  scale_colour_manual(values = cols) +\n  labs(x = NULL, y = NULL, \n       title = \"Most Sectors Spend Proportionately Less on SMEs\", \n       subtitle = \"Pct SME G-Cloud & DOS Spend by Sector\") + \n  labs(caption = \"\\nSource: GOV.UK's Digital Marketplace\")"
  },
  {
    "objectID": "project/six/index.html#r-toolbox",
    "href": "project/six/index.html#r-toolbox",
    "title": "Six Months Later",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[9], is.na[1], library[6], min[1], months[1], sum[6]\n\n\nclock\nadd_days[1], add_months[1], date_parse[1]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\nbind_rows[1], filter[3], if_else[2], lead[1], mutate[3], summarise[4]\n\n\nggplot2\naes[4], facet_wrap[2], geom_line[4], geom_smooth[4], ggplot[4], labs[8], scale_colour_manual[4], scale_x_date[4], scale_y_continuous[4], theme[2], theme_bw[1], theme_set[1]\n\n\nlubridate\ninterval[1]\n\n\npurrr\nmap[2]\n\n\nreadr\nlocale[1], parse_number[1], read_csv[1]\n\n\nrlang\nset_names[1]\n\n\nscales\nlabel_dollar[2], label_percent[2]\n\n\nstringr\nfixed[5], str_c[3], str_extract[1], str_remove[2], str_replace[4]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/storm/index.html",
    "href": "project/storm/index.html",
    "title": "Weathering the Storm",
    "section": "",
    "text": "In 2020, Covid-19 began battering financial markets now further impacted by the war in Ukraine. Which sectors are faring best?\nI’ll compare each sector in the S&P 500 with the overall market. Baselining each at zero as of February 19th, we’ll see which were the first to recover lost ground.\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(wesanderson)\nlibrary(scales)\nlibrary(glue)\nlibrary(tidyquant)\nlibrary(clock)\nconflict_prefer(\"date_format\", \"clock\")\nlibrary(usedthese)\n\nconflict_scout()\ntheme_set(theme_bw())\n\n(cols &lt;- wes_palette(\"Moonrise2\"))\nsymbols &lt;-\n  c(\n    \"SPY\",\n    \"XLV\",\n    \"XLK\",\n    \"XLE\",\n    \"XLF\",\n    \"XLC\",\n    \"XLI\",\n    \"XLY\",\n    \"XLP\",\n    \"XLRE\",\n    \"XLU\",\n    \"XLB\"\n  )\n\nfrom &lt;- \"2020-02-19\"\n\nfrom_formatted &lt;- date_parse(from, format = \"%Y-%m-%d\") |&gt; \n  date_format(format = \"%b %d, %Y\")\neod_sectors &lt;-\n  tq_get(symbols, from = from) |&gt;\n  mutate(\n    norm_close = adjusted / first(adjusted) - 1,\n    type = if_else(symbol == \"SPY\", \"Market\", \"Sector\"),\n    sector = case_match(\n      symbol,\n      \"SPY\"  ~ \"S&P 500\",\n      \"XLB\"  ~ \"Materials\",\n      \"XLE\"  ~ \"Energy\",\n      \"XLU\"  ~ \"Utilities\",\n      \"XLI\"  ~ \"Industrical\",\n      \"XLRE\" ~ \"Real Estate\",\n      \"XLV\"  ~ \"Health\",\n      \"XLK\"  ~ \"Technology\",\n      \"XLF\"  ~ \"Financial\",\n      \"XLC\"  ~ \"Communication\",\n      \"XLY\"  ~ \"Consumer Discretionary\",\n      \"XLP\"  ~ \"Consumer Staples\",\n      .default = \"Other\"\n    ), .by = symbol\n  ) |&gt;\n  drop_na()\nPerhaps not too surprising to see that Tech led the way back from Covid. But with the further impact of the situation in Ukraine, the Energy sector is now the strongest performer relative to February 2020. Comms, with all that home-working, benefited initially during the lockdown, but has faded since.\neod_sectors |&gt;\n  mutate(\n    sector = str_wrap(sector, 12),\n    sector = fct_reorder(sector, norm_close, last, .desc = TRUE)\n  ) |&gt;\n  ggplot(aes(date, norm_close, colour = sign(norm_close))) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", colour = \"grey80\") +\n  geom_line() +\n  facet_wrap(~sector) +\n  scale_colour_gradient(low = cols[2], high = cols[1]) +\n  scale_y_continuous(labels = label_percent()) +\n  labs(\n    title = \"S&P 500 Sector Impact of Covid-19 & Ukraine\",\n    subtitle = glue(\"Relative to {from_formatted}\"),\n    x = NULL, y = NULL, colour = NULL\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")"
  },
  {
    "objectID": "project/storm/index.html#r-toolbox",
    "href": "project/storm/index.html#r-toolbox",
    "title": "Weathering the Storm",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[1], library[8], sign[1]\n\n\nclock\ndate_format[1], date_parse[1]\n\n\nconflicted\nconflict_prefer[1], conflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\ncase_match[1], first[1], if_else[1], mutate[2]\n\n\nforcats\nfct_reorder[1]\n\n\nggplot2\naes[1], element_text[1], facet_wrap[1], geom_hline[1], geom_line[1], ggplot[1], labs[1], scale_colour_gradient[1], scale_y_continuous[1], theme[1], theme_bw[1], theme_set[1]\n\n\nglue\nglue[1]\n\n\nscales\nlabel_percent[1]\n\n\nstringr\nstr_wrap[1]\n\n\ntidyquant\ntq_get[1]\n\n\ntidyr\ndrop_na[1]\n\n\nusedthese\nused_here[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/footnote/index.html",
    "href": "project/footnote/index.html",
    "title": "A Footnote in History",
    "section": "",
    "text": "The nature of employment has seen significant shifts over time. Occupations are being consigned to ‘footnotes in history’ whilst others grow driven by trends such as concern for the environment.\nProducing a journal-quality table requires fine-grained and reproducible control over presentation. Surgical targeting of footnotes, capable of adapting to changes in the underlying data, is one example.\nThis post briefly explores the shifts in the nature of employment whilst at the same time more fully exploring the grammar of tables gt(Iannone et al. 2022): The natural companion to the grammar of graphics ggplot2(Wickham 2016).\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(readxl)\nlibrary(gt)\nlibrary(usedthese)\n\nconflict_scout()\n\n2 conflicts:\n* `filter`: [dplyr]\n* `lag`   : [dplyr]\nIn Digging Deep, the DT package is used to produce a reactable table; one with sortable and searchable columns. DT is intended as an R interface to the DataTables library, but reactivity is not yet supported in gt.\nData frames are liberally printed across all projects including, for example, a table to summarise an auto-generated overview of R packages and functions used in each project. The YAML option df-print: kable renders a nice table (with striped rows) in these cases.\nFor this project something a little more sophisticated is needed.\nAs a guiding principle, Posit packages are my first port of call. This provides a confidence in cross-package consistency, interoperability, longevity and an investment in development and support. Hence gt is the go-to package for the footnoted table further down.\nAs the intent is to present a summary in the style of the Financial Times, we’ll need a suitable custom colour palette.\ntheme_set(theme_bw())\n\ncols &lt;- c(\n  \"#FFF1E5\", \"#F2DFCE\",\n  \"#333333\", \"#800D33\",\n  \"#C00000\", \"#00994D\"\n) |&gt;\n  fct_inorder()\n\ntibble(x = 1:6, y = 1) |&gt;\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols),\n    nudge_y = -0.1, fill = \"white\"\n  ) +\n  annotate(\n    \"label\",\n    x = 3.5, y = 0.5,\n    label = \"Financial Times\",\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\nThe labour market data are sourced from the Office for National Statistics1.\nread_data &lt;- \\(x) {\n  read_xlsx(\n    x,\n    skip = 12,\n    col_names = c(\n      \"occupation\",\n      \"persons\"\n    ),\n    col_types = c(\n      \"text\",\n      \"numeric\",\n      \"skip\",\n      \"skip\",\n      \"skip\",\n      \"skip\",\n      \"skip\"\n    )\n  )\n} |&gt; \n  mutate(year = x |&gt; str_remove(\".xlsx\") |&gt; as.integer())\n\npop_df &lt;- list(\"2004.xlsx\", \"2021.xlsx\") |&gt; \n  map(read_data) |&gt; \n  list_rbind()\nThere’s a hierarchy to the data, so I’ll extract the lowest level and then slice off the top and bottom occupations based on their percentage change over time.\nchange_df &lt;- pop_df |&gt; \n  filter(str_starts(occupation, \"\\\\d{4} \")) |&gt; \n  pivot_wider(names_from = year, values_from = persons) |&gt; \n  separate_wider_regex(occupation, \n                       c(soc = \"\\\\d{4}\", \" \", occupation = \".*\")) |&gt; \n  mutate(change = `2021` / `2004` - 1) |&gt; \n  arrange(desc(change)) |&gt; \n  mutate(group = if_else(row_number() &lt;= 10, \"Risers\", \"Fallers\")) |&gt; \n  slice(c(1:10, (n()-10):n())) |&gt; \n  relocate(group)\nThe handling of footnotes is a particularly nice feature in gt: The package automatically assigns, and maintains the order of, the superscripted numbers (could also be symbols) to ensure they flow naturally. And targeting offers a high degree of control and reproducibility.\nFor example, two entries (highlighted light blue) in the table below use the abbreviation n.e.c.. The footnote may be targeted at rows which contain that string rather than having to manually identify the rows. And once added, any subsequent footnotes would be renumbered to maintain the flow. So, if I were to change the source datasets to different years or countries, all references to n.e.c. would be auto-magically found and appropriately footnoted.\ngt_tbl &lt;- change_df |&gt;\n  gt(rowname_col = c(\"occupation\"), groupname_col = \"group\") |&gt;\n  tab_header(title = \"UK Employment by Occupation\") |&gt; \n  tab_options(table.width = pct(100)) |&gt; \n  fmt_number(\n    columns = starts_with(\"2\"),\n    decimals = 0\n  ) |&gt;\n  fmt_percent(\n    columns = starts_with(\"c\"),\n    decimals = 0,\n    force_sign = TRUE\n  ) |&gt;\n  sub_missing() |&gt;\n  tab_spanner(\n    label = \"Year\",\n    columns = starts_with(\"2\")\n  ) |&gt; \n  tab_style(\n    style = cell_text(transform = \"capitalize\"),\n    locations = cells_column_labels(!starts_with(\"s\"))\n  ) |&gt; \n  tab_style(\n    style = cell_text(transform = \"uppercase\"),\n    locations = cells_column_labels(\"soc\")\n  ) |&gt; \n  tab_footnote(\n    footnote = \"Not elsewhere classified\",\n    locations = cells_stub(rows = contains(\"n.e.c.\"))\n  ) |&gt; \n  tab_footnote(\n    footnote = \"Count of all persons\",\n    locations = cells_column_spanners()\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Standard Occupational Classification 2020\",\n    locations = cells_column_labels(columns = \"soc\")\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Top & bottom 10 occupations ordered by percent change\",\n    locations = cells_row_groups(groups = c(\"Risers\", \"Fallers\"))\n  ) |&gt;\n  tab_footnote(\n    footnote = \"Figures suppressed as statistically unreliable\",\n    locations = cells_body(\n      columns = c(change, `2021`),\n      rows = is.na(change)\n    )\n  ) |&gt;\n  tab_source_note(source_note = \"Source: Office for National Statistics (ONS)\")\n\ngt_tbl |&gt;\n  tab_style_body(\n    style = cell_fill(color = \"lightblue\"),\n    pattern = \"n.e.c.\",\n    extents = \"stub\"\n  ) |&gt; \n  opt_stylize(style = 6, color = \"gray\", add_row_striping = TRUE) |&gt; \n  as_raw_html()\n\n\n  \n  \nUK Employment by Occupation\nsoc2Year1\nchange\n2004\n2021\nRisers3\nIndustrial cleaning process occupations\n9132\n26,300\n241,100\n+817%\nHealth professionals n.e.c.4\n2219\n9,000\n70,100\n+679%\nPolice community support officers\n3315\n2,100\n13,900\n+562%\nBusiness and financial project management professionals\n2424\n58,300\n350,000\n+500%\nAdvertising and public relations directors\n1134\n19,400\n69,000\n+256%\nIT business analysts, architects and systems designers\n2135\n82,800\n270,500\n+227%\nAircraft maintenance and related trades\n5235\n27,200\n88,300\n+225%\nQuality assurance and regulatory professionals\n2462\n43,700\n138,000\n+216%\nOfficers of non-governmental organisations\n4114\n41,800\n127,200\n+204%\nEnvironment professionals\n2142\n17,500\n52,100\n+198%\nFallers3\nSheet metal workers\n5213\n29,100\n10,900\n−63%\nProcess operatives n.e.c.4\n8119\n22,200\n8,200\n−63%\nFootwear and leather working trades\n5413\n13,000\n4,800\n−63%\nPrinting machine assistants\n8127\n22,600\n8,100\n−64%\nAssemblers (electrical and electronic products)\n8131\n63,200\n20,700\n−67%\nPrinters\n5422\n63,600\n8,900\n−86%\nChartered architectural technologists\n2435\n2,400\n5 —\n5 —\nMoulders, core makers and die casters\n5212\n3,900\n5 —\n5 —\nAir-conditioning and refrigeration engineers\n5225\n19,400\n5 —\n5 —\nPre-press technicians\n5421\n8,600\n5 —\n5 —\nCoal mine operatives\n8122\n2,600\n5 —\n5 —\nSource: Office for National Statistics (ONS)\n1 Count of all persons\n2 Standard Occupational Classification 2020\n3 Top & bottom 10 occupations ordered by percent change\n4 Not elsewhere classified\n5 Figures suppressed as statistically unreliable\nThe above table uses one of the in-built style theme options. It looks clean and polished. But sometimes the table to be published needs a high degree of customisation to match, for example, a specific branding. gt offers this as we’ll demonstrate by attempting to replicate the style employed by the market data in the Financial Times.\ngt_ft &lt;- gt_tbl |&gt; \n  tab_options(\n    table.border.top.color = \"#FFF1E5\",\n    table.border.bottom.color = \"#FFF1E5\",\n    table.background.color = \"#FFF1E5\",\n    table.font.size = px(10),\n    table.font.color = \"#262A33\",\n    heading.align = \"left\",\n    heading.title.font.size = px(20),\n    heading.title.font.weight = \"bold\",\n    heading.background.color = \"#FFF1E5\",\n    row.striping.include_table_body = TRUE,\n    row.striping.include_stub = TRUE,\n    row.striping.background_color = \"#F2DFCE\",\n    row_group.background.color = \"#FFF1E5\"\n  ) |&gt; \n  tab_header(title = html(\"UK Employment by Occupation  \", \n                          local_image(\"logo.png\", height = 20))) |&gt; \n  tab_style(\n    style = list(\n      cell_text(font = \"Financier Display\"),\n      cell_borders(sides = \"bottom\", weight = px(3), color = \"#262A33\")\n      ),\n    locations = cells_title()\n  ) |&gt;\n  tab_style(\n    style = cell_text(size = px(14)),\n    locations = cells_row_groups()\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"#800D33\", weight = \"bold\"),\n    locations = cells_stub()\n  ) |&gt; \n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = list(cells_column_labels(), \n                     cells_column_spanners(), \n                     cells_row_groups())\n  ) |&gt; \n  tab_style(\n    style = cell_borders(style = \"hidden\"),\n    locations = list(cells_body(),\n                     cells_row_groups(),\n                     cells_stub())\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"#00994D\", weight = \"bold\"),\n    locations = cells_body(\n      columns = change,\n      rows = change &gt;= 0\n    )\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"#C00000\", weight = \"bold\"),\n    locations = cells_body(\n      columns = change,\n      rows = change &lt; 0\n    )\n  ) |&gt; \n  tab_style(\n    style = cell_text(color = \"grey40\", size = px(9)),\n    locations = list(cells_footnotes(), cells_source_notes())\n  )\n\ngt_ft |&gt; as_raw_html()\n\n\n  \n  \nUK Employment by Occupation \nsoc2Year1\nchange\n2004\n2021\nRisers3\nIndustrial cleaning process occupations\n9132\n26,300\n241,100\n+817%\nHealth professionals n.e.c.4\n2219\n9,000\n70,100\n+679%\nPolice community support officers\n3315\n2,100\n13,900\n+562%\nBusiness and financial project management professionals\n2424\n58,300\n350,000\n+500%\nAdvertising and public relations directors\n1134\n19,400\n69,000\n+256%\nIT business analysts, architects and systems designers\n2135\n82,800\n270,500\n+227%\nAircraft maintenance and related trades\n5235\n27,200\n88,300\n+225%\nQuality assurance and regulatory professionals\n2462\n43,700\n138,000\n+216%\nOfficers of non-governmental organisations\n4114\n41,800\n127,200\n+204%\nEnvironment professionals\n2142\n17,500\n52,100\n+198%\nFallers3\nSheet metal workers\n5213\n29,100\n10,900\n−63%\nProcess operatives n.e.c.4\n8119\n22,200\n8,200\n−63%\nFootwear and leather working trades\n5413\n13,000\n4,800\n−63%\nPrinting machine assistants\n8127\n22,600\n8,100\n−64%\nAssemblers (electrical and electronic products)\n8131\n63,200\n20,700\n−67%\nPrinters\n5422\n63,600\n8,900\n−86%\nChartered architectural technologists\n2435\n2,400\n5 —\n5 —\nMoulders, core makers and die casters\n5212\n3,900\n5 —\n5 —\nAir-conditioning and refrigeration engineers\n5225\n19,400\n5 —\n5 —\nPre-press technicians\n5421\n8,600\n5 —\n5 —\nCoal mine operatives\n8122\n2,600\n5 —\n5 —\nSource: Office for National Statistics (ONS)\n1 Count of all persons\n2 Standard Occupational Classification 2020\n3 Top & bottom 10 occupations ordered by percent change\n4 Not elsewhere classified\n5 Figures suppressed as statistically unreliable"
  },
  {
    "objectID": "project/footnote/index.html#r-toolbox",
    "href": "project/footnote/index.html#r-toolbox",
    "title": "A Footnote in History",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\nused_here()\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[1], as.integer[1], c[8], is.na[1], library[5], list[5]\n\n\nconflicted\nconflict_prefer_all[1], conflict_scout[1]\n\n\ndplyr\narrange[1], desc[1], filter[1], if_else[1], mutate[3], n[2], relocate[1], row_number[1], slice[1]\n\n\nforcats\nfct_inorder[1]\n\n\nggplot2\naes[2], annotate[1], geom_col[1], geom_label[1], ggplot[1], scale_fill_manual[1], theme[1], theme_bw[1], theme_set[1], theme_void[1]\n\n\ngt\nas_raw_html[2], cell_borders[2], cell_fill[1], cell_text[9], cells_body[4], cells_column_labels[4], cells_column_spanners[2], cells_footnotes[1], cells_row_groups[4], cells_source_notes[1], cells_stub[3], cells_title[1], fmt_number[1], fmt_percent[1], gt[1], html[1], local_image[1], opt_stylize[1], pct[1], px[5], sub_missing[1], tab_footnote[5], tab_header[2], tab_options[2], tab_source_note[1], tab_spanner[1], tab_style[10], tab_style_body[1]\n\n\npurrr\nlist_rbind[1], map[1]\n\n\nreadxl\nread_xlsx[1]\n\n\nstringr\nstr_remove[1], str_starts[1]\n\n\ntibble\ntibble[1]\n\n\ntidyr\npivot_wider[1], separate_wider_regex[1]\n\n\ntidyselect\ncontains[1], starts_with[4]\n\n\nusedthese\nused_here[1]"
  }
]