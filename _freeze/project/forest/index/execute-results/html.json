{
  "hash": "c13f6cf4a66acdbfd85978523e77f6b1",
  "result": {
    "markdown": "---\ntitle: \"Criminal Goings-on in a Random Forest\"\ndate: \"2018-03-01\"\ncategories: [R, machine learning]\ndescription: \"Criminal goings-on in a random forest and predictions with tree-based and glm models\"\nbibliography: references.bib\n---\n\n\n![](feature.gif){fig-alt=\"A micro-forest in the middle of nowhere has the sign \\\"Random Forest\\\" nailed to a tree. A white rabbit peers out at a thief tip-toeing away with a bag labelled \\\"swag\\\"..\"}\n\nWhen first posted in 2018 this project used the caret package to model [crime in London](https://data.gov.uk/dataset). Since then, the newer [tidymodels](https://www.tidymodels.org)[@tidymodels] framework, consistent with tidy data principles, has rapidly evolved.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\")\nlibrary(tidymodels)\nlibrary(janitor)\nlibrary(scales)\nlibrary(vip)\nlibrary(poissonreg)\nlibrary(usedthese)\n\nconflict_scout()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10 conflicts:\n* `chisq.test` : janitor, stats\n* `col_factor` : scales, readr\n* `discard`    : scales, purrr\n* `filter`     : [dplyr]\n* `fisher.test`: janitor, stats\n* `fixed`      : recipes, stringr\n* `lag`        : [dplyr]\n* `spec`       : yardstick, readr\n* `step`       : recipes, stats\n* `vi`         : vip, utils\n```\n:::\n:::\n\n\nThis custom palette was created in [Adobe Colour](https://color.adobe.com/create/color-wheel) as the basis for the feature image above and with the hex codes loaded for use in ggplot. `colorRampPalette` enables interpolation of an extended set of colours to support the number of offence types.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_bw())\n\ncols <- c(\"#798E87\", \"#C27D38\", \"#CCC591\", \"#29211F\") |>\n  fct_inorder()\n\ntibble(x = 1:4, y = 1) |>\n  ggplot(aes(x, y, fill = cols)) +\n  geom_col(colour = \"white\") +\n  geom_label(aes(label = cols), size = 4, vjust = 2, fill = \"white\") +\n  annotate(\n    \"label\",\n    x = 2.5, y = 0.5,\n    label = \"Custom Pallette\",\n    fill = \"white\",\n    alpha = 0.8,\n    size = 6\n  ) +\n  scale_fill_manual(values = as.character(cols)) +\n  theme_void() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/theme-1.png){width=100%}\n:::\n\n```{.r .cell-code}\ncols10 <- colorRampPalette(cols)(10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- str_c(\n  \"https://data.london.gov.uk/\",\n  \"download/recorded_crime_rates/\",\n  \"c051c7ec-c3ad-4534-bbfe-6bdfee2ef6bb/\",\n  \"crime%20rates.csv\"\n)\n\nraw_df <-\n  read_csv(url, col_types = \"cfcfdn\") |>\n  clean_names() |>\n  mutate(\n    year = str_extract(year, \"(?:1999|200[0-9]|201[0-7])\"), # 1999-2007\n    year = as.numeric(year)\n  ) |>\n  summarise(number_of_offences = sum(number_of_offences),\n            .by = c(year, borough, offences))\n```\n:::\n\n\nA faceted plot is one way to get a sense of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_df |>\n  mutate(borough = str_wrap(borough, 11)) |>\n  ggplot(aes(year, number_of_offences, \n             colour = offences, group = offences)) +\n  geom_line() +\n  facet_wrap(~borough, scales = \"free_y\", ncol = 4) +\n  labs(\n    x = NULL, y = NULL, title = \"London Crime by Borough\",\n    colour = \"Offence\", caption = \"Source: data.gov.uk\"\n  ) +\n  scale_colour_manual(values = cols10) +\n  guides(colour = guide_legend(nrow = 4)) +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/facet-1.png){width=100%}\n:::\n:::\n\n\nVisualising data in small multiples using `facet_wrap` or `facet_grid` can be a useful way to explore data. When there are a larger number of these however, as we're starting to see in the example above, there are alternative techniques one can employ. This is explored in [Seeing the Wood for the Trees](/project/wood).\n\nNonetheless, one can anyway see there are data aggregated at multiple levels. So to net these data down to purely borough-level, I'll filter out the summarised rows, for example, \"England and Wales\" and \"Inner London\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime_df <- raw_df |>\n  filter(\n    offences != \"All recorded offences\",\n    !borough %in% c(\n      \"England and Wales\",\n      \"Met Police Area\", \n      \"Inner London\", \n      \"Outer London\"\n    )\n  )\n```\n:::\n\n\nThere are 9 types of offence in 33 boroughs. The dataset covers the period 1999 to 2016.\n\nThe faceted plot hints at a potential interaction between borough and type of offence. In more affluent boroughs, and/or those attracting greater visitor numbers, e.g. Westminster and Kensington & Chelsea, \"theft and handling\" is the more dominant category. In Lewisham, for example, \"violence against the person\" exhibits higher counts. However, for the purpose of this basic model comparison, I'm going to set aside the potential interaction term.\n\nBefore modelling, I'll visualise the dependent variable against each independent variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime_df |>\n  summarise(number_of_offences = sum(number_of_offences),\n            .by = c(offences, borough)) |>\n  mutate(\n    median_offences = median(number_of_offences),\n    offences = str_wrap(offences, 10),\n    .by = offences\n  ) |>\n  ggplot(aes(fct_reorder(offences, median_offences), number_of_offences)) +\n  geom_boxplot(fill = cols[1]) +\n  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Number of Offences by Type\",\n    caption = \"Source: data.gov.uk\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/by type-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime_df |>\n  summarise(number_of_offences = sum(number_of_offences),\n            .by = c(offences, borough)) |>\n  mutate(\n    median_offences = median(number_of_offences),\n    offences = str_wrap(offences, 10),\n    .by = borough\n  ) |>\n  ggplot(aes(fct_reorder(borough, median_offences), number_of_offences)) +\n  geom_boxplot(fill = cols[1]) +\n  scale_y_log10(labels = label_number(scale_cut = cut_short_scale())) +\n  coord_flip() +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Number of Offences by Borough\",\n    caption = \"Source: data.gov.uk\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/by borough-1.png){width=100%}\n:::\n:::\n\n\nThe offences and borough variables show significant variation in crime counts. And there is also evidence of a change over time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime_df |>\n  summarise(number_of_offences = sum(number_of_offences), .by = year) |>\n  ggplot(aes(year, number_of_offences)) +\n  geom_line(colour = cols[4], linetype = \"dashed\") +\n  geom_smooth(colour = cols[5], fill = cols[1]) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_short_scale())) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"Number of Offences by Year\",\n    caption = \"Source: data.gov.uk\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/by year-1.png){width=100%}\n:::\n:::\n\n\nI'll separate out some test data so I can compare the performance of the models on data they have not see during model training.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ndata_split <- \n  crime_df |>\n  initial_split(strata = offences)\n\ncrime_train <- data_split |>\n  training()\n\ncrime_test <- data_split |>\n  testing()\n```\n:::\n\n\nI'm using the recipes package to establish the role of the variables. Alternatively I could have used a formula-based approach, i.e. `number_of_offences ~ borough + offences + year`.\n\nWhilst `borough` and `offences` are nominal, I'm not creating any dummy variables since I intend to use tree-based models which will anyway branch left and right based on groups of values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime_recipe <-\n  crime_train |>\n  recipe() |>\n  update_role(number_of_offences, new_role = \"outcome\") |>\n  update_role(-has_role(\"outcome\"), new_role = \"predictor\")\n\nsummary(crime_recipe)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|variable           |type                          |role      |source   |\n|:------------------|:-----------------------------|:---------|:--------|\n|year               |double , numeric              |predictor |original |\n|borough            |factor   , unordered, nominal |predictor |original |\n|offences           |factor   , unordered, nominal |predictor |original |\n|number_of_offences |double , numeric              |outcome   |original |\n\n</div>\n:::\n:::\n\n\nI'll start with a Recursive Partitioning And Regression Trees (rpart) model. The feature importance plot tells me which variables are having the biggest influence on the model. The type of offence is the most important predictor in the rpart model, followed by the location of the offences. This makes intuitive sense.\n\nClearly there is a temporal component too otherwise there would be no trend.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrp_model <- \n  decision_tree() |>\n  set_engine(\"rpart\") |>\n  set_mode(\"regression\")\n\nrp_wflow <- workflow() |>\n  add_recipe(crime_recipe) |>\n  add_model(rp_model)\n\nrp_fit <- rp_wflow |> \n  fit(crime_train)\n\nrp_fit |>\n  extract_fit_parsnip() |> \n  vip(aesthetics = list(fill = cols[1])) +\n  labs(title = \"Feature Importance -- rpart\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rpart-1.png){width=100%}\n:::\n\n```{.r .cell-code}\nrp_results <- rp_fit |> \n  augment(crime_test) |> \n  mutate(model = \"rpart\")\n```\n:::\n\n\nRanger is an implementation of random forests or recursive partitioning that, according to the documentation, is particularly suited to high dimensional data. My data is not high-dimensional, but let's throw it into the mix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranger_model <- \n  rand_forest() |>\n  set_engine(\"ranger\", importance = \"impurity\") |>\n  set_mode(\"regression\")\n\nranger_wflow <- workflow() |>\n  add_recipe(crime_recipe) |>\n  add_model(ranger_model)\n\nranger_fit <- ranger_wflow |> \n  fit(crime_train)\n\nranger_fit |>\n  extract_fit_parsnip() |> \n  vip(aesthetics = list(fill = cols[3])) +\n  labs(title = \"Feature Importance -- Ranger\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/ranger-1.png){width=100%}\n:::\n\n```{.r .cell-code}\nranger_results <- ranger_fit |> \n  augment(crime_test) |> \n  mutate(model = \"ranger\")\n```\n:::\n\n\nAnd of course my project title would make little sense without a Random Forest.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model <- \n  rand_forest() |>\n  set_engine(\"randomForest\") |>\n  set_mode(\"regression\")\n\nrf_wflow <- workflow() |>\n  add_recipe(crime_recipe) |>\n  add_model(rf_model)\n\nrf_fit <- rf_wflow |> \n  fit(crime_train)\n\nrf_fit |>\n  extract_fit_parsnip() |> \n  vip(aesthetics = list(fill = \"grey60\")) +\n  labs(title = \"Feature Importance -- Random Forest\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/forest-1.png){width=100%}\n:::\n\n```{.r .cell-code}\nrf_results <- rf_fit |> \n  augment(crime_test) |> \n  mutate(model = \"random forest\")\n```\n:::\n\n\nFor good measure, I'll also include a generalized linear model (glm)\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson_model <- \n  poisson_reg() |>\n  set_engine(\"glm\") |>\n  set_mode(\"regression\")\n\npoisson_wflow <- workflow() |>\n  add_recipe(crime_recipe) |>\n  add_model(poisson_model)\n\npoisson_fit <- poisson_wflow |> \n  fit(crime_train)\n\npoisson_fit |>\n  extract_fit_parsnip() |> \n  vip(aesthetics = list(fill = cols[4])) +\n  labs(title = \"Feature Importance -- glm\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/poisson-1.png){width=100%}\n:::\n\n```{.r .cell-code}\npoisson_results <- poisson_fit |> \n  augment(crime_test) |> \n  mutate(model = \"glm\")\n```\n:::\n\n\nThe Random Forest and the glm models performed the best here, with the former edging the Mean Absolute Error and R Squared metrics, and the latter with its nose in front on the Root Mean Squared Error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_results <- \n  rp_results |> \n  bind_rows(ranger_results) |> \n  bind_rows(rf_results) |> \n  bind_rows(poisson_results) |> \n  group_by(model) |> \n  metrics(truth = number_of_offences, estimate = .pred)\n\nmodel_results |> \n  ggplot(aes(model, .estimate, fill = model)) +\n  geom_col() +\n  geom_label(aes(label = round(.estimate, 2)), size = 3, fill = \"white\") +\n  facet_wrap(~ .metric, scales = \"free_y\") +\n  scale_fill_manual(values = as.character(cols[c(4, 5, 3, 1)])) +\n  labs(x = NULL, y = NULL, title = \"Comparison of Model Metrics\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/results-1.png){width=100%}\n:::\n:::\n\n\nAnother way of approaching all this would be to use time-series forecasting. This would major on auto-regression, i.e. looking at how the lagged number-of-offences influence future values. And one could further include exogenous data such as, say, the numbers of police. It would be reasonable to expect that increasing police numbers would, in time, lead to decreased crime levels.\n\nI explored time-series in other posts such as [Digging Deep](/project/planning), so I won't go down that path here.\n\nWhat I could do though is to strengthen my tree-based models above by engineering some additional temporal features. Let's try that just with the Random Forest to see if it improves the outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp_df <- \n  crime_df |> \n  mutate(num_lag1 = lag(number_of_offences),\n         num_lag2 = lag(number_of_offences, 2),\n         num_lag3 = lag(number_of_offences, 3)) |> \n  drop_na()\n```\n:::\n\n\nSo, when predicting the number of offences, the model will now additionally consider, for each borough, type of offence and year, the number of offences in each of the three prior years.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ndata_split <- \n  temp_df |>\n  initial_split(strata = offences)\n\ntemp_train <- data_split |>\n  training()\n\ntemp_test <- data_split |>\n  testing()\n\ntemp_recipe <-\n  temp_train |>\n  recipe() |>\n  update_role(number_of_offences, new_role = \"outcome\") |>\n  update_role(-has_role(\"outcome\"), new_role = \"predictor\")\n\nsummary(temp_recipe)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|variable           |type                          |role      |source   |\n|:------------------|:-----------------------------|:---------|:--------|\n|year               |double , numeric              |predictor |original |\n|borough            |factor   , unordered, nominal |predictor |original |\n|offences           |factor   , unordered, nominal |predictor |original |\n|number_of_offences |double , numeric              |outcome   |original |\n|num_lag1           |double , numeric              |predictor |original |\n|num_lag2           |double , numeric              |predictor |original |\n|num_lag3           |double , numeric              |predictor |original |\n\n</div>\n:::\n\n```{.r .cell-code}\ntemp_model <- \n  rand_forest() |>\n  set_engine(\"randomForest\") |>\n  set_mode(\"regression\")\n\ntemp_wflow <- workflow() |>\n  add_recipe(temp_recipe) |>\n  add_model(temp_model)\n\ntemp_fit <- temp_wflow |> \n  fit(temp_train)\n\ntemp_fit |>\n  extract_fit_parsnip() |> \n  vip(aesthetics = list(fill = cols[2])) +\n  labs(title = \"Feature Importance -- Random Forest with Lags\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/forest2-1.png){width=100%}\n:::\n\n```{.r .cell-code}\ntemp_results <- temp_fit |> \n  augment(temp_test) |> \n  metrics(truth = number_of_offences, estimate = .pred) |> \n  mutate(model = \"rf with lags\")\n```\n:::\n\n\nThe recipe summary includes the three new predictors. And the feature importance plot shows the lags playing a larger role in the model than the `year` variable, so looks like we should anticipate a model improvement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nupdated_results <- \n  model_results |> \n  bind_rows(temp_results)\n\nupdated_results |> \n  ggplot(aes(model, .estimate, fill = model)) +\n  geom_col() +\n  geom_label(aes(label = round(.estimate, 2)), size = 3, fill = \"white\") +\n  facet_wrap(~ .metric, scales = \"free_y\") +\n  scale_fill_manual(values = as.character(cols[c(4, 5, 3, 2, 1)])) +\n  labs(x = NULL, y = NULL, title = \"Comparison of Model Metrics\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/results2-1.png){width=100%}\n:::\n:::\n\n\nThe model metrics bear this out. The `mae` and `rmse` are markedly smaller, and the `rsq` significantly improved. We could have tried further lags. We could have tried tweaking some parameters. We could have tried time-series forecasting with, for example a statistical model like ARIMA, or a Neural Network model such as NNETAR.\n\nThe best approach would depend upon a more precise definition of the objective. And some trial and error, comparing approaches after more extensive feature-engineering, validation, testing and tuning. For the purposes of this post though I wanted to merely explore some techniques. So I'll leave it there.\n\n## R Toolbox\n\nSummarising below the packages and functions used in this post enables me to separately create a [toolbox visualisation](/project/box) summarising the usage of packages and functions across all posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nused_here()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"usedthese table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Package </th>\n   <th style=\"text-align:left;\"> Function </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> base </td>\n   <td style=\"text-align:left;\"> as.character[3], as.numeric[1], c[7], library[8], list[5], round[2], set.seed[2], sum[4], summary[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> conflicted </td>\n   <td style=\"text-align:left;\"> conflict_prefer_all[1], conflict_scout[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dplyr </td>\n   <td style=\"text-align:left;\"> bind_rows[4], filter[1], group_by[1], lag[3], mutate[10], summarise[4] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forcats </td>\n   <td style=\"text-align:left;\"> fct_inorder[1], fct_reorder[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> generics </td>\n   <td style=\"text-align:left;\"> augment[5], fit[5] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ggplot2 </td>\n   <td style=\"text-align:left;\"> aes[10], annotate[1], coord_flip[1], element_text[3], facet_wrap[3], geom_boxplot[2], geom_col[3], geom_label[3], geom_line[2], geom_smooth[1], ggplot[7], guide_legend[1], guides[1], labs[11], scale_colour_manual[1], scale_fill_manual[3], scale_y_continuous[1], scale_y_log10[2], theme[4], theme_bw[1], theme_set[1], theme_void[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> grDevices </td>\n   <td style=\"text-align:left;\"> colorRampPalette[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hardhat </td>\n   <td style=\"text-align:left;\"> extract_fit_parsnip[5] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> janitor </td>\n   <td style=\"text-align:left;\"> clean_names[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> parsnip </td>\n   <td style=\"text-align:left;\"> decision_tree[1], poisson_reg[1], rand_forest[3], set_engine[5], set_mode[5] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> readr </td>\n   <td style=\"text-align:left;\"> read_csv[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> recipes </td>\n   <td style=\"text-align:left;\"> has_role[2], recipe[2], update_role[4] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> rsample </td>\n   <td style=\"text-align:left;\"> initial_split[2], testing[2], training[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> scales </td>\n   <td style=\"text-align:left;\"> cut_short_scale[3], label_number[3] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stats </td>\n   <td style=\"text-align:left;\"> median[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stringr </td>\n   <td style=\"text-align:left;\"> str_c[1], str_extract[1], str_wrap[3] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tibble </td>\n   <td style=\"text-align:left;\"> tibble[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tidyr </td>\n   <td style=\"text-align:left;\"> drop_na[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> usedthese </td>\n   <td style=\"text-align:left;\"> used_here[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> vip </td>\n   <td style=\"text-align:left;\"> vip[5] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> workflows </td>\n   <td style=\"text-align:left;\"> add_model[5], add_recipe[5], workflow[5] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> yardstick </td>\n   <td style=\"text-align:left;\"> metrics[2] </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}