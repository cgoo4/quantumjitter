{
  "hash": "815058169771d101fe055cae8af649ce",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Readability & Lexical Dispersion\"\nauthor: \"Carl Goodwin\"\ndate: \"2025-10-09\"\ncategories: [textual analysis]\ndescription: \"A novel analysis - literally\"\nimage: \"feature.png\"\ndraft: true\neditor: \n  mode: source\n---\n\n\n\n![](feature.png){.profile-img fig-alt=\"AI-assisted digital composition of the book manuscript and a laptop displaying a boxplot.\"}\n\n<br>\n\nA little under a year ago, I embarked on a journey to write my first novel, *Such an Odd Word to Use*. The process was both exhilarating and challenging, pushing me to explore new creative depths. It was also a chance to deploy my skills in data analysis and visualisation in a novel context—literally.\n\nOn my author website, also built with Quarto, I wrote a [blog post](carlgoodwin.com/analysis) comparing my writing style to established literature using R and the `quanteda` package. This version of that post incorporates the R code used to delve into the structure and readability of my manuscript.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted)\nlibrary(tidyverse)\nconflicts_prefer(ggplot2::annotate, readtext::texts, quanteda::tokens, dplyr::filter)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(readtext)\nlibrary(glue)\nlibrary(scales)\nlibrary(gutenbergr)\n\nset_theme(theme_bw(paper = \"#ddd9d7\", ink = \"#181d1e\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-time Convert PDF → text\n# system2(\"pdftotext\", args = c(\"-nopgbrk\", \"manuscript.pdf\", \"manuscript.txt\"))\n\nbook_text <- readtext(\"manuscript.txt\")\n\n# Define a regular expression for your section headers\nsection_pattern <- \n  regex(\"\\\\b(Chapter\\\\s+\\\\d+|Prologue|Disclaimer|Acknowledgements)\\\\b\")\n\n# Split based on this pattern\nsections <- str_split(book_text, pattern = section_pattern, simplify = FALSE)[[1]]\n\n# Extract headers separately to keep them\nheaders <- str_extract_all(book_text, section_pattern, simplify = TRUE)\n\nhead_foot <- \n  regex(\"carl goodwin|\\\\n\\\\n\\\\n\\\\s*[0-9]+\\\\n\\\\n|vii|such an odd word to use\")\n\n# Combine headers and content into a tibble\nbook_split <- tibble(\n  heading = str_to_title(headers),\n  text = str_trim(sections[-1])  # -1 drop preamble before first heading\n) |> \n  mutate(text = str_remove_all(text, head_foot) |> str_squish())\n\nbook_corp <- book_split |> \n  corpus(docid_field = \"heading\") \n\nbook_toks <- book_corp |> \n  ntoken(split_hyphens = TRUE, remove_punct = TRUE) |> \n  as_tibble(rownames = \"heading\") |> \n  filter(str_starts(heading, \"Cha|Pro\")) |> \n  mutate(\n    heading = fct_inorder(heading),  # Preserve factor order\n    cum_toks = cumsum(value)\n  )\n\nword_count <- comma(sum(book_toks$value))\n\navg_words <- mean(book_toks$value)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbook_toks |> \n  ggplot(aes(heading, value)) +\n  geom_col(fill = \"#82b777\", colour = \"#181d1e\") +\n  geom_hline(yintercept = avg_words, linetype = \"dashed\") +\n  annotate(\"label\", x = 15.5, y = avg_words, label = \"Average\") +\n  labs(\n    title = glue(\"Such an Odd Word to Use -- {word_count} Words\"),\n    x = NULL, y = \"Words\"\n    ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-3-1.png){width=100%}\n:::\n:::\n\n\n\n::: {style=\"text-align:center; margin: 2rem 0;\"}\n\n\n{{< iconify clarity:music-note-solid size=20px >}}\n\n\n\n:::\n\nTo explore how *Such an Odd Word to Use* compares to classic literature in style and complexity, I analysed its readability using the Flesch-Kincaid grade level—an estimate of the US school year required to understand a given text. This metric was calculated chapter by chapter, revealing how the reading complexity varies across the book.\n\nFor comparison, I drew on works from [Project Gutenberg](https://www.gutenberg.org), a digital library of public domain classics. Selecting works of a similar length, and after stripping out introductory front matter, I computed the same chapter-level readability and word counts for each title. This allowed a side-by-side view of how *Such an Odd Word to Use* sits in relation to the established canon—not just in overall difficulty, but in how that difficulty shifts over the course of the narrative.\n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# x <- gutenberg_works()\n\nbooks <- gutenberg_download(\n  c(11, 42, 2097, 19337, 64317), \n  meta_fields = c(\"title\", \"author\")\n  ) |> \n  distinct() |> \n  mutate(front_matter = case_when(\n    author == \"Carroll, Lewis\" & row_number() <= 17 ~ 1,\n    author == \"Doyle, Arthur Conan\" & row_number() <= 17 ~ 1,\n    author == \"Dickens, Charles\" & row_number() <= 83 ~ 1,\n    author == \"Stevenson, Robert Louis\" & row_number() <= 22 ~ 1,\n    author == \"Fitzgerald, F. Scott (Francis Scott)\" & row_number() <= 22 ~ 1,\n    .default = 0\n  ), .by = title)\n\nsaveRDS(books, \"books\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmethods <- c(\"Flesch.Kincaid\")\n\nextract_title_phrase <- function(text) {\n  words <- str_split(text, \"\\\\s+\")[[1]]\n  \n  # Clean punctuation but preserve letters, accents, apostrophes, dashes\n  clean_words <- str_remove_all(words, \"[^\\\\p{L}\\\\p{Pd}’'-]\")\n\n  allowed_lc <- c(\"the\", \"and\")\n\n  # Title-case detection with Unicode support via stringi\n  is_title <- stringi::stri_detect_regex(clean_words,\n                                pattern = \"^[\\\\p{Lu}][\\\\p{Ll}’']*(?:-[\\\\p{Lu}][\\\\p{Ll}’']*)?$\") |\n              tolower(clean_words) %in% allowed_lc\n\n  stop_index <- which(!is_title)[1]\n  if (is.na(stop_index) || stop_index < 2) return(NA_character_)\n\n  str_c(words[1:(stop_index - 1)], collapse = \" \")\n}\n\nsuch <- book_split |> \n  rename(chapter = heading) |> \n  filter(str_starts(chapter, \"Chapter|Prol\")) |> \n  mutate(\n    title = \"Such an Odd Word to Use\", \n    author = \"Goodwin, Carl\",\n    chapter = if_else(chapter == \"Prologue\", 0, parse_number(chapter)),\n    chapter_name = map_chr(text, extract_title_phrase),\n    chapter_name = str_remove(chapter_name, \"\\\\s+\\\\S+$\"),\n    chapter_name = str_remove(chapter_name, \"As$|Amid$|In$|Over$\"),\n    chapter_name = if_else(chapter == 0, \"Prologue\", chapter_name)\n    )\n\npattern <- \"chapter|scene|the prologue|stave\"\n\nbooks_corp <- readRDS(\"books\") |> \n  filter(front_matter != 1) |> \n  mutate(\n    text = if_else(\n      str_detect(text, \"^[[:upper:]]{2}\") & author == \"Stevenson, Robert Louis\",\n      str_c(\"Chapter \", text), text\n      ),\n    text = if_else(\n      str_count(str_squish(text), \".\") <= 4 & title == \"The Great Gatsby\",\n      str_c(\"Chapter \", str_squish(text)), text\n      ),\n    chapter = cumsum(str_starts(text, regex(pattern, ignore_case = TRUE))),\n    .by = title\n    ) |> \n  bind_rows(such) |> \n  summarise(\n    text = str_c(text, collapse = \" \"), \n    .by = c(\"title\", \"author\", \"chapter\")\n    ) |> \n  mutate(doc_id = str_c(title, author, chapter, sep = \"|\")) |> \n  corpus()\n\nbooks_toks <- books_corp |> \n  ntoken(split_hyphens = TRUE, remove_punct = TRUE) |> \n  as_tibble(rownames = \"doc\") |> \n  separate_wider_delim(doc, delim = \"|\", names = c(\"title\", \"author\", \"chapter\")) |> \n  summarise(words = sum(value), .by = c(\"title\", \"author\")) |> \n  mutate(\n    title = str_c(title, \" | \", author),\n    words = label_number(\n      accuracy = 0.1, \n      scale_cut = append(cut_short_scale(), 1, 1))(words)\n    )\n\nreadability_df <- books_corp |> \n  textstat_readability(measure = methods) |>\n  pivot_longer(-document) |>\n  separate_wider_delim(\n    document, delim = \"|\", names = c(\"title\", \"author\", \"chapter\")\n    )\n\nreadability_df |> \n  mutate(\n    title = str_c(title, \" | \", author), \n    fill = if_else(str_detect(title, \"Such\"), \"#82b777\", NA)\n    ) |> \n  ggplot(aes(value, title)) +\n  geom_boxplot(aes(fill = fill)) +\n  geom_text(\n    aes(x = -Inf, y = title, label = words), \n    size = 3, hjust = -0.3, data = books_toks\n    ) +\n  scale_fill_identity() +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  labs(\n    x = \"Score\", y = NULL,\n    title = \"Readability Boxplot (Flesch-Kincaid)\",\n    subtitle = \"Dot = Outlier Chapter\"\n    ) +\n  theme(axis.text.y = element_text(size = 9))\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-5-1.png){width=100%}\n:::\n\n```{.r .cell-code}\n# http://readability.mackayst.com/catalog\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nread_df <- such |> \n  mutate(doc_id = str_c(chapter, \" | \", chapter_name)) |> \n  corpus() |> \n  textstat_readability(measure = methods) |>\n  pivot_longer(-document) |>\n  mutate(document = fct_inorder(document)) |>\n  rename(chapter = document) \n\navgs <- read_df |> \n  summarise(avg_score = mean(value), .by = name)\n```\n:::\n\n\n\nChapters with a more conversational tone—such as the Lana-focused chapters 19 and 23—tend to be easier to read, while more analytical sections, like the cypher-driven chapter 26, are more complex. The overall Flesch-Kincaid Grade Level score averages 8.4, indicating the text is suitable for readers at a middle to early high school level (roughly ages 13–15), which aligns with the book’s intended accessibility.\n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_df |> \n  ggplot(aes(chapter, value, group = name)) +\n  geom_hline(\n    aes(yintercept = avg_score), linetype = \"dashed\",\n    colour = \"grey80\", data = avgs\n    ) +\n  geom_label(\n    aes(\n      \"15 | Pancakes and Paranoia\", avg_score, \n      label = glue(\"Average\\n{round(avgs$avg_score, 1)}\")\n      ), \n    nudge_y = -1.5, size = 3, data = avgs\n    ) +\n  geom_line(colour = \"#82b777\") +\n  geom_point() +\n  coord_cartesian(ylim = c(0, 14), expand = TRUE) +\n  labs(\n    x = NULL, y = \"Score\", \n    title = \"Such an Odd Word to Use\",\n    subtitle = \"Chapter Readability (Flesch-Kincaid)\"\n    ) +\n  theme(\n    axis.text.x = element_text(angle = 55, hjust = 1),\n    legend.position = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-7-1.png){width=100%}\n:::\n:::\n\n\n\n::: {style=\"text-align:center; margin: 2rem 0;\"}\n\n\n{{< iconify clarity:music-note-solid size=20px >}}\n\n\n\n:::\n\nA lexical dispersion plot visualises where and how often key characters are mentioned across the chapters. It highlights patterns in their narrative presence — for example, Alistair (the antagonist) appears consistently throughout, suggesting a sustained influence on the storyline. Lana (an ally) is introduced early but becomes more prominent in later chapters. Imogen (a supporting character) is mentioned more sparingly, reflecting her secondary role in the narrative arc.\n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuch |> \n  mutate(docid = str_c(chapter, \" | \", chapter_name)) |> \n  corpus(docid_field = \"docid\") |> \n  tokens(split_hyphens = TRUE) |> \n  kwic(\n    pattern = c(\"Alistair\", \"Lana\", \"Imogen\"), \n    valuetype = \"fixed\"\n    ) |>\n  textplot_xray() +\n  scale_x_continuous(\n    breaks = c(0, 0.5, 1), \n    labels = c(\"Start\", \"Middle\", \"End\")) +\n  labs(\n    x = \"Relative Position in Chapter\", y = \"Chapter\", \n    title = \"Lexical Dispersion - Character Appearance\"\n    ) +\n  theme(\n    panel.spacing.x = unit(0.5, \"lines\"),\n    legend.position = \"none\",\n    strip.background = element_rect(fill = \"#82b777\", colour = \"#181d1e\"),\n    panel.background = element_rect(fill = \"#ddd9d7\", colour = NA),\n    plot.background = element_rect(fill = \"#ddd9d7\", colour = NA)\n  )\n```\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/unnamed-chunk-8-1.png){width=100%}\n:::\n:::\n",
    "supporting": [
      "analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}