{
  "hash": "6596aa00c7631a270a1a6962853677a7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayes Lived Here (Probably)\"\ndate: \"2023-06-12\"\ncategories: [R, bayesian regression, markov chain monte carlo, machine learning, web scraping, special effects]\ndescription: \"Predicting the interest rate for a fixed-rate mortgage using Bayesian regression\"\nbibliography: references.bib\n---\n\n\n![](feature.jpg){fig-alt=\"A plaque on the gate-post of Thomas Bayes's former home confirming he lived there from 1731 to 1761. The word 'probably' is spray-painted below the plaque.\"}\n\nIt's quite common in the UK, and London in particular, for prominent individuals, such as Dickens, Darwin and [Galton](https://www.english-heritage.org.uk/visit/blue-plaques/francis-galton/), who passed at least 20 years ago, to be [commemorated by a plaque](https://www.english-heritage.org.uk/visit/blue-plaques/) on a building in which they lived or with which they were associated.\n\nThomas Bayes lived in Tunbridge Wells, a few miles to the north of London. His former home has a [plaque on the front gate](https://openplaques.org/people/4461). I couldn't resist a little digital graffiti.\n\nMortgages weren't a thing back then. So I wonder what he would have made of modelling fixed mortgage interest rates with Bayesian linear regression and tuning tree-based models with Bayesian optimisation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nlibrary(tidymodels)\nlibrary(feasts)\nlibrary(tsibble)\nlibrary(glue)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(ggfx)\nlibrary(janitor)\nlibrary(rvest)\nlibrary(broom.mixed)\nlibrary(imputeTS)\nlibrary(readxl)\nlibrary(rstantools)\nlibrary(shapviz)\nlibrary(kernelshap)\nlibrary(finetune)\nlibrary(tictoc)\nlibrary(ggfoundry)\nlibrary(usedthese)\n\nconflict_scout()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_bw())\n\npal <- c(\n  \"#7E1134\", \"#EDD5C5\",\n  \"#057683\", \"#F3E2D8\",\n  \"black\", \"#7F7E7C\"\n)\n\npal_name <- \"Palette from Feature Image\"\n\ndisplay_palette(pal, pal_name)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/theme-1.png){width=100%}\n:::\n:::\n\n\nBanks review their mortgage interest rates in response to changes in the central bank rate. Fixed-rate mortgages are further [influenced by the value of gilts (government bonds)](https://www.moneysavingexpert.com/news/2022/10/mortgage-rates--could-dip--over-coming-weeks---what-you-need-to-/#gilt). Banks rely less on the wholesale market to fund mortgages when their balance sheet is well supported by savings made by households.\n\nSo, to forecast an average fixed 5-year (75% loan-to-value) mortgage rate (`mortgage`), I'll try using the central bank rate (`cbr`), the 5-year yield from British Government Securities (`gilt`), the ratio of retail deposits to the value of outstanding mortgages (`retail_ratio`) and the number of new mortgage approvals (`approvals`). All sourced from the Bank of England[^1].\n\n[^1]: Per [Legal \\| Bank of England](https://www.bankofengland.co.uk/legal?_gl=1*ecwt7o*_ga*MTEwNjczODEyMy4xNjMzNzEwMzA1*_ga_WJQM5Y97BL*MTYzMzcxMDMwNS4xLjEuMTYzMzcxMDMxMi4w), the information made available via the [Database](https://www.bankofengland.co.uk/boeapps/database/) is the copyright of the Governor and Company of the Bank, unless otherwise stated. Reproduction of data in the Database is subject to the terms of the [UK Open Government Licence](https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/), allowing and encouraging free and flexible data reuse.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_rate <- \\(y, z) {\n  url <-\n    str_c(\n      \"https://www.bankofengland.co.uk/boeapps/database/\",\n      \"fromshowcolumns.asp?Travel=NIxAZxSUx&FromSeries=1&ToSeries=50&\",\n      \"DAT=ALL&\",\n      \"FNY=Y&CSVF=TT&html.x=66&html.y=26&SeriesCodes=\", y,\n      \"&UsingCodes=Y&Filter=N&title=Quoted%20Rates&VPD=Y\"\n    )\n  \n  read_html(url) |>\n    html_element(\"#stats-table\") |>\n    html_table() |>\n    clean_names() |>\n    mutate(date = dmy(date)) |> \n    rename({{ z }} := 2)\n}\n\npwalk(\n  list(\n    x = c(\"cbr_df\", \"mor_df\", \"gil_df\", \"ret_df\", \"out_df\", \"app_df\"),\n    y = c(\"IUMBEDR\", \"IUMBV42\", \"IUMSNPY\", \"LPMVRJX\", \"LPMB3TA\", \"LPMB3VA\"),\n    z = c(\"cbr\", \"mortgage\", \"gilt\", \"retail\", \"outstanding\", \"approvals\")\n  ),\n  \\(x, y, z) assign(x, get_rate(y, {{ z }}), .GlobalEnv)\n)\n\nboe_list <- list(cbr_df, mor_df, gil_df, ret_df, out_df, app_df)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\njoined_df <- \n  reduce(boe_list, left_join, join_by(date)) |> \n  filter(date <= \"2023-04-30\") |>\n  mutate(retail_ratio = retail / outstanding) |>\n  select(-retail, -outstanding)\n```\n:::\n\n\nTo enable forecasting to year-end, forward-looking estimates are required for the predictors: `cbr`[^2], `gilt`[^3] and `approvals`[^4]. Imputation is used to linearly interpolate the interim months.\n\n[^2]: *econforecasting.com* (2023). Consensus Interest Rate Forecast Model. Retrieved from https://econforecasting.com/forecast-ukbankrate.\n\n[^3]: Estimated by [Trading Economics](https://tradingeconomics.com/united-kingdom/5-year-note-yield#forecast)\n\n[^4]: Estimated by [Trading Economics](https://tradingeconomics.com/united-kingdom/mortgage-approvals)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforward_df <- joined_df |>\n  rows_append(\n    tribble(\n      ~date, ~cbr, ~gilt, ~approvals, \n      ymd(\"2023-06-30\"), 4.57, 4.29, 49000, \n      ymd(\"2023-09-30\"), 4.90, NA, NA,\n      ymd(\"2023-12-31\"), 4.92, NA, NA,\n      ymd(\"2024-03-31\"), 4.64, NA, NA,\n      ymd(\"2024-06-30\"), 4.30, 4.92, 68000, \n    )\n  )\n\nfilled_df <-\n  tibble(date = seq(\n    joined_df$date |> nth(-2L) |> floor_date(\"month\"),\n    forward_df$date |> last() |> floor_date(\"month\"),\n    by = \"month\"\n  )) |>\n  mutate(date = rollforward(date)) |>\n  left_join(forward_df, join_by(date)) |>\n  mutate(across(-c(date, mortgage), na_interpolation)) \n\nforecast_df <-\n  bind_rows(actual = joined_df, forecast = filled_df, .id = \"id\") |>\n  arrange(date, id) |>\n  distinct(date, .keep_all = TRUE) |>\n  mutate(across(c(gilt, cbr),\n    list(\n      lag1 = lag,\n      lag2 = \\(x) lag(x, 2),\n      lag3 = \\(x) lag(x, 3),\n      lag12 = \\(x) lag(x, 12)\n    ),\n    .names = \"{.col}_{.fn}\"\n  )) |>\n  filter(date >= \"1995-01-01\", date <= \"2023-12-31\") |> \n  mutate(mortgage_cut = cut_number(mortgage, 10))\n```\n:::\n\n\nAll these historical data and forward-looking estimates may then be visualised to get an overall view.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_compare <- \\(x){\n  forecast_df |>\n    ggplot(aes(date, {{ x }}, colour = id)) +\n    geom_line() +\n    geom_vline(xintercept = ymd(\"2023-04-30\"), linetype = \"dashed\", colour = \"grey60\") +\n    scale_colour_manual(values = as.character(pal[c(5, 3)])) +\n    scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n    scale_y_continuous(labels = label_number(suffix = \"%\")) +\n    labs(x = NULL, colour = NULL) +\n    theme(\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank(),\n      legend.position = \"none\"\n    )\n}\n\np1 <- plot_compare(cbr) +\n  labs(title = \"Key Rates\", y = \"Central\\nBank Rate\") +\n  theme(legend.position = \"right\")\n\np2 <- plot_compare(gilt) +\n  labs(y = \"5-year\\nGilt\")\n\np3 <- plot_compare(retail_ratio) +\n  scale_y_continuous(labels = label_number()) +\n  labs(y = \"Retail\\nRatio\")\n\np4 <- plot_compare(approvals) +\n  scale_y_continuous(labels = label_number()) +\n  labs(y = \"Approvals\")\n\np5 <- plot_compare(mortgage) +\n  labs(y = \"Fixed 5-year\\nMortgage\") +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.ticks.x = element_line()\n  )\n\np1 / p2 / p3 / p4 / p5\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-1.png){width=100%}\n:::\n:::\n\n\nCross-correlation shows not only the strength of the relationship between the response (`mortgage`) and explanatory variables, but also highlights any lagged effects. Spikes outside the dashed lines indicate significant correlation beyond [\"white noise\"](https://otexts.com/fpp3/wn.html#wn).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_xcorr <- \\(x){\n  forecast_df |>\n    mutate(date = yearmonth(date)) |>\n    as_tsibble(index = date) |>\n    CCF({{ x }}, mortgage, lag_max = 40) |>\n    autoplot() +\n    scale_y_continuous(limits = c(-1, 1)) +\n    labs(y = NULL) +\n    theme(\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank()\n    )\n}\n\nc1 <- plot_xcorr(cbr) +\n  labs(title = \"CBR\", y = \"Cross Correlation\") +\n  theme_bw()\n\nc2 <- plot_xcorr(gilt) +\n  ggtitle(\"Gilt\")\n\nc3 <- plot_xcorr(retail_ratio) +\n  ggtitle(\"Retail Ratio\")\n\nc4 <- plot_xcorr(approvals) +\n  ggtitle(\"Approvals\")\n\nc1 + c2 + c3 + c4 + \n  plot_layout(nrow = 1) +\n  plot_annotation(title = \"5-year Fixed-rate Mortgage Correlated With:\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/correlation-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory_df <- forecast_df |>\n  filter(id == \"actual\")\n\nplot_lm <- \\(x) {\n  history_df |>\n    ggplot(aes({{ x }}, mortgage)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", colour = pal[3]) +\n    scale_x_continuous(labels = label_number(accuracy = 0.1, suffix = \"%\")) +\n    scale_y_continuous(labels = label_number(accuracy = 0.1, suffix = \"%\")) +\n    theme(\n      axis.text.y = element_blank(),\n      axis.ticks.y = element_blank()\n    ) +\n  labs(y = NULL)\n}\n\nl1 <- plot_lm(cbr) +\n  labs(y = \"Mortgage\") +\n  theme_bw()\n\nl2 <- plot_lm(gilt)\n\nl3 <- plot_lm(retail_ratio) +\n  scale_x_continuous(labels = label_number(accuracy = 0.1))\n\nl4 <- plot_lm(approvals) +\n  scale_x_continuous(\n    labels = label_number(\n      accuracy = 1,\n      scale_cut = cut_short_scale()\n    )\n  )\n\nl1 + l2 + l3 + l4 + \n  plot_annotation(title = \"Independent vs Potential Explanatory Variables\") +\n  plot_layout(nrow = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/relationships-1.png){width=100%}\n:::\n:::\n\n\nOne could try a time series model[^5], e.g. ARIMA. This would focus on patterns and pattern changes in the historical mortgage data, i.e. [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation), seasonality and trend. External regressors like `cbr` and `gilt`, could then be additionally used to reduce any remaining unexplained variance. The train/test split would be a time-based split. This kind of statistical technique works well when relationships and trends in the historical data are clear and fairly stable such as, for example, with electricity consumption.\n\n[^5]: See [Forecasting: Principles and Practice](https://otexts.com/fpp3/) with implementation via the [fable](https://tidyverts.github.io/tidy-forecasting-principles/index.html) package[@fable].\n\nA causal model feels more appropriate here. Use of linear or non-linear regression would place the principal focus on the external variables. A causal model also allows the train/test split to be sampled from across the full series rather than requiring a time-based split.\n\nThe [tidymodels](https://www.tidymodels.org) ecosystem, including workflowsets [@workflowsets], will facilitate the fitting and tuning of multiple models with differing recipes: Linear and [Bayesian](https://parsnip.tidymodels.org/reference/details_linear_reg_stan.html) regression models (the latter using [weakly informative prior distributions](https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html) via the rstanarm package [@rstanarm]) and the non-linear tree-based models Random Forest and XGBoost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\ndata_split <- history_df |>\n  initial_split(strata = mortgage_cut)\n\ntrain <- training(data_split)\ntest <- testing(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncore_recipe <-\n  train |> \n  recipe() |>\n  update_role(mortgage, new_role = \"outcome\") |>\n  update_role(id, date, mortgage_cut, new_role = \"id\") |> \n  update_role(-has_role(c(\"outcome\", \"id\")), new_role = \"predictor\") |> \n  step_date(date, features = \"decimal\")\n\nns_recipe <-\n  core_recipe |>\n  step_ns(date_decimal, deg_free = 6) |> \n  step_ns(cbr, deg_free = 5) |> \n  step_ns(gilt, approvals, deg_free = 4) |>\n  step_ns(retail_ratio, deg_free = 2) \n\nlm_model <- \n  linear_reg()\n\nrf_model <-\n  rand_forest(\n    mode = \"regression\",\n    trees = 756,\n    mtry = 5,\n    min_n = 2\n  )\n\nxgb_model <-\n  boost_tree(\n    mode = \"regression\",\n    learn_rate = 0.01,\n    trees = 1054,\n    tree_depth = 14,\n    mtry = 3,\n    min_n = 3,\n  )\n\nbayes_model <-\n  linear_reg() |>\n  set_engine(\"stan\", refresh = 1)\n\nmodel_set <-\n  workflow_set(\n    preproc = list(\n      ns = ns_recipe,\n      ns = ns_recipe,\n      core = core_recipe,\n      core = core_recipe\n    ),\n    models = list(\n      lm = lm_model,\n      bayes = bayes_model,\n      rf = rf_model,\n      xgb = xgb_model\n    ),\n    cross = FALSE\n  )\n\n# core_recipe |> prep() |> bake(new_data = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\n\nfolds <- train |> vfold_cv(strata = mortgage_cut)\n\n# --- Comment out for fit_resamples\n# params <- model_set |>\n#   extract_workflow(\"core_rf\") |>  # core_xgb\n#   parameters() |>\n#   update(mtry = finalize(mtry(), train))\n# ---------------------------------\n\ndoParallel::registerDoParallel()\n\ntic()\n\nset_results <- model_set |>\n  workflow_map(\"fit_resamples\",\n    # workflow_map(\"tune_bayes\", # Comment out for fit_resamples\n    resamples = folds,\n    # param_info = params, # Comment out for fit_resamples\n    metrics = metric_set(rmse),\n    initial = 10,\n    iter = 50,\n    control = control_bayes(save_workflow = TRUE, no_improve = 30),\n    seed = 1\n  )\n\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n30.721 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nset_results |>\n  rank_results() |>\n  slice_head(n = 1, by = wflow_id) |>\n  mutate(\n    tune = str_extract(.config, \"\\\\d?\\\\d$\"),\n    model = str_extract(wflow_id, \"(?<=_).*$\"),\n    wflow_id = str_c(wflow_id, \" \", tune) |> fct_reorder(.x = mean)\n  ) |>\n  ggplot(aes(wflow_id, mean,\n    ymin = mean - std_err,\n    ymax = mean + std_err, colour = model\n  )) +\n  geom_pointrange(position = position_dodge(width = 0.9)) +\n  geom_label(aes(label = tune)) +\n  scale_colour_manual(values = pal[c(1, 3, 5, 6)]) +\n  labs(\n    x = NULL, y = \"Mean RMSE\",\n    title = \"Workflow Ranking (Label = Tune Iteration)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/tune-1.png){width=100%}\n:::\n\n```{.r .cell-code}\n# --- Comment out for fit_resamples\n# set_results |>\n#   extract_workflow_set_result(\"core_rf\") |> # core_xgb\n#   unnest(.metrics) |>\n#   summarise(.estimate = mean(.estimate), .config = first(.config),\n#             .by = c(trees, mtry, min_n)) |> # tree_depth\n#   arrange(.estimate) |>\n#   slice_head(n = 1)\n# ---------------------------------\n```\n:::\n\n\nThe models may then be assessed against the test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\naug_train <- \\(x){\n  set.seed(2023)\n\n  set_results |>\n    filter(wflow_id == x) |>\n    fit_best(metric = \"rmse\") |>\n    augment(test)\n}\n\nwalk2(\n  c(\"xgb_res\", \"rf_res\", \"lm_res\", \"bayes_res\"),\n  c(\"core_xgb\", \"core_rf\", \"ns_lm\", \"ns_bayes\"),\n  \\(x, y) assign(x, aug_train(y), .GlobalEnv)\n)\n```\n:::\n\n\nThe residuals from the trained model used on the test data look quite reasonable aside a small curvature pattern in the linear models. The presence of a pattern could suggest a missing explanatory variable(s), or non-linearity if present only in linear models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfocus_df <- bind_rows(\n  rf = rf_res |> mutate(model = \"Random Forest\"),\n  xgb = xgb_res |> mutate(model = \"XGBoost\"),\n  lm = lm_res |> mutate(model = \"Linear\"),\n  bayes = bayes_res |> mutate(model = \"Bayes\"),\n  .id = \"id\"\n)\n\nlabel_df <- focus_df |>\n  nest(-c(id, model)) |>\n  mutate(metric = map(data, \\(x) rmse(x, mortgage, .pred))) |>\n  unnest(metric) |>\n  mutate(mortgage = 7.5, .pred = 3.5, .estimate = round(.estimate, 4))\n\nfocus_df |>\n  ggplot(aes(.pred, mortgage)) +\n  geom_point(alpha = 0.5, size = 0.5) +\n  geom_abline(alpha = 0.5) +\n  geom_smooth(se = FALSE, size = 0.5) +\n  geom_label(aes(label = glue(\"RMSE\\n{.estimate}\")),\n    data = label_df, size = 3, fill = pal[2]\n  ) +\n  coord_obs_pred() +\n  facet_wrap(~model) +\n  labs(\n    title = \"Observed vs Predicted Mortgage Rates\",\n    subtitle = \"Test Data\",\n    x = \"Predictions\", y = \"Observations\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/observed vs predicted-1.png){width=100%}\n:::\n\n```{.r .cell-code}\nfocus_df |>\n  mutate(residual = mortgage - .pred) |>\n  ggplot(aes(sample = residual, colour = id)) +\n  geom_qq() +\n  geom_qq_line() +\n  facet_wrap(~model) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_colour_manual(values = pal[c(1, 3, 5, 6)]) +\n  labs(title = \"QQ Plot of Residuals\", subtitle = \"Test Data\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/observed vs predicted-2.png){width=100%}\n:::\n\n```{.r .cell-code}\nfocus_df |>\n  mutate(residual = mortgage - .pred) |>\n  ggplot(aes(date, residual, colour = id)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", colour = pal[3], linewidth = 1) +\n  geom_point() +\n  geom_smooth() +\n  facet_wrap(~model) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_y_continuous(limits = c(-1, 1)) +\n  scale_colour_manual(values = pal[c(1, 3, 5, 6)]) +\n  labs(title = \"Residuals Over Time\", subtitle = \"Test Data\", x = NULL, y = \"Residuals\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/observed vs predicted-3.png){width=100%}\n:::\n:::\n\n\nIt makes sense to now train the tuned model(s) on *all* the historical data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_hist <- \\(y){\n  set.seed(2023)\n\n  model_set |>\n    extract_workflow(y) |>\n    fit(data = history_df)\n}\n\nwalk2(\n  c(\"xgb_fit\", \"rf_fit\", \"bayes_fit\", \"lm_fit\"),\n  c(\"core_xgb\", \"core_rf\", \"ns_bayes\", \"ns_lm\"),\n  \\(x, y) assign(x, fit_hist(y), .GlobalEnv)\n)\n\naug_fit <- \\(y) y |> augment(forecast_df)\n\nwalk2(\n  c(\"xgb_res\", \"rf_res\", \"bayes_res\", \"lm_res\"),\n  list(xgb_fit, rf_fit, bayes_fit, lm_fit),\n  \\(x, y) assign(x, aug_fit(y), .GlobalEnv)\n)\n```\n:::\n\n\nFeature importance unsurprisingly confirms higher `gilt` and `cbr` values equate to higher `mortgage` rates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\n\nmake_shap <- \\(y){\n  kernelshap(y,\n    X = forecast_df,\n    bg_X = train |> slice_sample(n = 50),\n    feature_names = train |> select(-c(id, mortgage, mortgage_cut)) |> names(),\n    parallel = TRUE\n  ) |>\n    shapviz()\n}\n\ntic()\n\nwalk2(\n  c(\"xgb_shap\", \"rf_shap\", \"bayes_shap\", \"lm_shap\"),\n  list(xgb_fit, rf_fit, bayes_fit, lm_fit),\n  \\(x, y) assign(x, make_shap(y), .GlobalEnv)\n)\n\ntoc()\n\nmshap <- c(bayes = bayes_shap, xgb = xgb_shap, lm = lm_shap, rf = rf_shap)\n\n# 119.9 sec elapsed\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsv_importance(mshap, \"beeswarm\", color_bar_title = NULL) +\n  plot_annotation(title = \"Feature Importance (Yellow = High Feature Value)\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/shap plots-1.png){width=100%}\n:::\n\n```{.r .cell-code}\nrow_id <- last(which(forecast_df$date == max(forecast_df$date)))\n\nsv_waterfall(mshap, row_id = row_id, max_display = 14) +\n  plot_annotation(\n    title = glue(\"{stamp('March 1, 2000')(max(forecast_df$date))} Waterfall\")\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/shap plots-2.png){width=100%}\n:::\n:::\n\n\nWhilst modelling with Bayesian (in contrast to linear) regression comes with the computational cost of Markov Chain Monte Carlo [(MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) simulation, the pay-off is the provision of a credible range of values for each of the parameters in addition to the coefficient point estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci <- tidy(bayes_fit, conf.int = TRUE, conf.level = 0.9)\n\nas.data.frame(extract_fit_engine(bayes_fit)) |>\n  pivot_longer(everything(), names_to = \"term\") |>\n  filter(term != \"sigma\") |>\n  ggplot(aes(value)) +\n  as_reference(geom_density(fill = \"white\"), id = \"density\") +\n  with_blend(\n    geom_rect(\n      aes(\n        x = NULL, y = NULL, xmin = conf.low, xmax = conf.high,\n        ymin = -Inf, ymax = Inf, fill = term\n      ),\n      data = ci, colour = \"grey50\", fill = pal[1],\n    ),\n    bg_layer = \"density\", blend_type = \"atop\"\n  ) +\n  geom_vline(aes(xintercept = estimate),\n    colour = \"white\",\n    linetype = \"dashed\", data = ci\n  ) +\n  geom_density(colour = \"grey50\") +\n  facet_wrap(~term, scales = \"free\") +\n  labs(\n    y = \"Density\", fill = \"Term\",\n    title = \"Bayesian Posterior Distributions\", x = \"Value\",\n    subtitle = \"Terms with 90% Plausible Interval\"\n  ) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/posterior-1.png){width=100%}\n:::\n:::\n\n\nThe plot below shows the final point estimate for each model as well as a 90% prediction interval for the Bayesian model using rstantools [@rstantools].\n\nThe forecasts are, at least initially, directionally similar. The test residuals and RMSE would favour the non-linear models, and XGBoost in particular, over the linear models for these data. The forecasts are further dependent upon the quality of the forward-looking externally-sourced estimates for the predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi <- predictive_interval(extract_fit_engine(bayes_fit),\n  newdata = ns_recipe |> prep() |> bake(new_data = forecast_df),\n  prob = 0.9\n) |>\n  as_tibble() |>\n  mutate(date = forecast_df$date)\n\nbind_rows(\n  xgb = xgb_res,\n  bayes = bayes_res,\n  rf = rf_res,\n  lm = lm_res,\n  .id = \"model\"\n) |>\n  left_join(pi, join_by(date)) |>\n  mutate(across(c(`5%`, `95%`), \\(x) if_else(model != \"bayes\", NA, x))) |>\n  ggplot(aes(date, .pred, ymin = `5%`, ymax = `95%`, colour = model, fill = model)) +\n  geom_ribbon(fill = pal[1], alpha = 0.3) +\n  geom_line() +\n  geom_vline(xintercept = ymd(\"2023-04-30\"), linetype = \"dashed\", colour = \"grey70\") +\n  scale_y_continuous(\n    labels = label_number(suffix = \"%\"),\n    breaks = c(2, 4, 6, 8, 10)\n  ) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +\n  scale_colour_manual(values = pal[c(1, 3, 5, 6)]) +\n  scale_fill_manual(values = pal[c(1, 3, 5, 6)]) +\n  labs(\n    title = \"Forecast of Average UK Household Mortgage Rates\",\n    subtitle = \"Fixed 5-year (75% Loan-to-Value)\",\n    x = NULL, y = \"Interest Rate\",\n    caption = \"Source Data: BoE\"\n  ) +\n  facet_wrap(~model) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/predictions-1.png){width=100%}\n:::\n:::\n\n\n## R Toolbox\n\nSummarising below the packages and functions used in this post enables me to separately create a [toolbox visualisation](/project/box) summarising the usage of packages and functions across all posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nused_here()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"usedthese table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Package </th>\n   <th style=\"text-align:left;\"> Function </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> base </td>\n   <td style=\"text-align:left;\"> as.character[1], as.data.frame[1], assign[5], c[27], library[21], list[7], max[2], names[1], readRDS[1], round[1], seq[1], set.seed[5], which[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> conflicted </td>\n   <td style=\"text-align:left;\"> conflict_prefer_all[1], conflict_scout[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> doParallel </td>\n   <td style=\"text-align:left;\"> registerDoParallel[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dplyr </td>\n   <td style=\"text-align:left;\"> across[3], arrange[1], bind_rows[3], distinct[1], filter[5], if_else[1], join_by[3], lag[3], last[2], left_join[2], mutate[18], nth[1], rename[1], rows_append[1], select[2], slice_head[1], slice_sample[1], tribble[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> feasts </td>\n   <td style=\"text-align:left;\"> CCF[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forcats </td>\n   <td style=\"text-align:left;\"> fct_reorder[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> generics </td>\n   <td style=\"text-align:left;\"> augment[2], fit[1], tidy[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ggfoundry </td>\n   <td style=\"text-align:left;\"> display_palette[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ggfx </td>\n   <td style=\"text-align:left;\"> as_reference[1], with_blend[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ggplot2 </td>\n   <td style=\"text-align:left;\"> aes[12], autoplot[1], cut_number[1], element_blank[6], element_line[1], element_text[4], facet_wrap[5], geom_abline[1], geom_density[2], geom_hline[1], geom_label[2], geom_line[2], geom_point[3], geom_pointrange[1], geom_qq[1], geom_qq_line[1], geom_rect[1], geom_ribbon[1], geom_smooth[3], geom_vline[3], ggplot[8], ggtitle[3], labs[16], position_dodge[1], scale_colour_manual[5], scale_fill_manual[1], scale_x_continuous[3], scale_x_date[3], scale_y_continuous[8], theme[9], theme_bw[3], theme_set[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> glue </td>\n   <td style=\"text-align:left;\"> glue[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hardhat </td>\n   <td style=\"text-align:left;\"> extract_fit_engine[2], extract_workflow[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> janitor </td>\n   <td style=\"text-align:left;\"> clean_names[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> kernelshap </td>\n   <td style=\"text-align:left;\"> kernelshap[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> lubridate </td>\n   <td style=\"text-align:left;\"> dmy[1], floor_date[2], rollforward[1], stamp[1], year[1], ymd[7] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> parsnip </td>\n   <td style=\"text-align:left;\"> boost_tree[1], linear_reg[2], rand_forest[1], set_engine[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> patchwork </td>\n   <td style=\"text-align:left;\"> plot_annotation[4], plot_layout[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> purrr </td>\n   <td style=\"text-align:left;\"> map[1], pwalk[1], reduce[1], walk2[4] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> recipes </td>\n   <td style=\"text-align:left;\"> bake[1], has_role[1], prep[1], recipe[1], step_date[1], step_ns[4], update_role[3] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> rsample </td>\n   <td style=\"text-align:left;\"> initial_split[1], testing[1], training[1], vfold_cv[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> rstantools </td>\n   <td style=\"text-align:left;\"> predictive_interval[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> rvest </td>\n   <td style=\"text-align:left;\"> html_element[1], html_table[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> scales </td>\n   <td style=\"text-align:left;\"> cut_short_scale[1], label_number[8] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> shapviz </td>\n   <td style=\"text-align:left;\"> shapviz[1], sv_importance[1], sv_waterfall[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stringr </td>\n   <td style=\"text-align:left;\"> str_c[2], str_extract[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tibble </td>\n   <td style=\"text-align:left;\"> as_tibble[1], tibble[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tictoc </td>\n   <td style=\"text-align:left;\"> tic[2], toc[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tidyr </td>\n   <td style=\"text-align:left;\"> nest[1], pivot_longer[1], unnest[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tidyselect </td>\n   <td style=\"text-align:left;\"> everything[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tsibble </td>\n   <td style=\"text-align:left;\"> as_tsibble[1], yearmonth[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tune </td>\n   <td style=\"text-align:left;\"> control_bayes[1], coord_obs_pred[1], fit_best[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> usedthese </td>\n   <td style=\"text-align:left;\"> used_here[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> workflowsets </td>\n   <td style=\"text-align:left;\"> rank_results[1], workflow_map[1], workflow_set[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> xml2 </td>\n   <td style=\"text-align:left;\"> read_html[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> yardstick </td>\n   <td style=\"text-align:left;\"> metric_set[1], rmse[1] </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}