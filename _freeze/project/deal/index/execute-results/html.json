{
  "hash": "ff8e10a08cca928a4126885b27c036eb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"A Frosty Deal?\"\ndate: \"2020-09-18\"\ncategories: [R, textual analysis, word embeddings, natural language processing]\ndescription: \"Quantitative textual analysis, word embeddings and analysing shifting trade-talk sentiment?\"\nbibliography: references.bib\n---\n\n\n![](feature.gif){fig-alt=\"Two frosty fists bump\"}\n\nBefore the post-Brexit trade negotiations concluded, what did quantitative textual analysis and word embeddings tell us about the shifting trade-talk sentiment?\n\nReading news articles on the will-they-won't-they post-Brexit trade negotiations with the EU sees days of optimism jarred by days of gloom. Do negative news articles, when one wants a positive outcome, leave a deeper impression?\n\nIs it possible to get a more objective view from [quantitative analysis of textual data](https://quanteda.io)? To do this, I'm going to look at hundreds of articles published in the Guardian newspaper over the course of the year to see how trade-talk sentiment changed week-to-week.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(conflicted)\nlibrary(tidyverse)\nconflict_prefer_all(\"dplyr\", quiet = TRUE)\nconflicts_prefer(lubridate::as_date)\nlibrary(paletteer)\nlibrary(guardianapi)\nlibrary(quanteda)\nlibrary(scales)\nlibrary(tictoc)\nlibrary(clock)\nlibrary(patchwork)\nlibrary(text2vec)\nlibrary(topicmodels)\nlibrary(slider)\nlibrary(glue)\nlibrary(ggfoundry)\nlibrary(usedthese)\n\nconflict_scout()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_bw())\n\npal_name <- \"wesanderson::Chevalier1\"\n\npal <- paletteer_d(pal_name)\n\ndisplay_palette(pal, pal_name)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/theme-1.png){width=100%}\n:::\n:::\n\n\nThe Withdrawal Agreement between the UK and the European Union was [signed on the 24th of January 2020](https://en.wikipedia.org/wiki/Brexit_withdrawal_agreement). Brexit-related newspaper articles will be imported from that date.\n\n::: callout-note\nSince publishing this article in September 2020, [an agreement was reached on December 24th 2020](https://www.bbc.com/news/uk-politics-55476625).\n:::\n\nThe Guardian newspaper asks for requests to span no more than 1 month at a time. Creating a set of monthly date ranges will enable the requests to be chunked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndates_df <- tibble(start_date = date_build(2020, 1:11, 25)) |> \n  mutate(end_date = add_months(start_date, 1) |> add_days(-1))\n\ndates_df\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|start_date |end_date   |\n|:----------|:----------|\n|2020-01-25 |2020-02-24 |\n|2020-02-25 |2020-03-24 |\n|2020-03-25 |2020-04-24 |\n|2020-04-25 |2020-05-24 |\n|2020-05-25 |2020-06-24 |\n|2020-06-25 |2020-07-24 |\n|2020-07-25 |2020-08-24 |\n|2020-08-25 |2020-09-24 |\n|2020-09-25 |2020-10-24 |\n|2020-10-25 |2020-11-24 |\n|2020-11-25 |2020-12-24 |\n\n</div>\n:::\n:::\n\n\n::: callout-important\nAccess to the Guardian's API via [guardianapi](https://github.com/cran/guardianapi)[@guardianapi] requires a key which may be requested [here](https://open-platform.theguardian.com/access/) and stored as `GU_API_KEY=` in the .Renviron file.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\n\nread_slowly <- slowly(gu_content)\n\narticle_df <-\n  pmap(dates_df, \\(start_date, end_date) {\n    read_slowly(\n      \"brexit\",\n      from_date = start_date,\n      to_date = end_date\n    )\n  }) |> \n  list_rbind()\n\ntoc()\n```\n:::\n\n\nThe data need a little cleaning, for example, to remove multi-topic articles, html tags and non-breaking spaces.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrade_df <-\n  article_df |>\n  filter(!str_detect(id, \"/live/\"), \n         section_id %in% c(\"world\", \"politics\", \"business\")) |>\n  mutate(\n    body = str_remove_all(body, \"<.*?>\") |> str_to_lower(),\n    body = str_remove_all(body, \"[^a-z0-9 .-]\"),\n    body = str_remove_all(body, \"nbsp\")\n  )\n```\n:::\n\n\nA corpus then gives me a collection of texts whereby each document is a newspaper article.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrade_corp <- trade_df |> \n  corpus(docid_field = \"short_url\", \n         text_field = \"body\", unique_docnames = FALSE)\n```\n:::\n\n\nAlthough only articles mentioning Brexit have been imported, some of these will not be related to trade negotiations with the EU. For example, there are on-going negotiations with many countries around the world. So, word embeddings[@text2vec] will help to narrow the focus to the specific context of the UK-EU trade deal.\n\nThe chief negotiator for the EU is Michel Barnier, so I'll quantitatively identify words in close proximity to \"Barnier\" in the context of these Brexit news articles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwindow <- 5\n\ntrade_fcm <-\n  trade_corp |>\n  tokens() |> \n  fcm(context = \"window\", window = window, \n      count = \"weighted\", weights = window:1)\n\nglove <- GlobalVectors$new(rank = 60, x_max = 10)\n\nset.seed(42)\n\nwv_main <- glove$fit_transform(trade_fcm, n_iter = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [11:53:21.069] epoch 1, loss 0.3797\nINFO  [11:53:22.349] epoch 2, loss 0.2565\nINFO  [11:53:23.648] epoch 3, loss 0.2290\nINFO  [11:53:24.907] epoch 4, loss 0.2086\nINFO  [11:53:26.164] epoch 5, loss 0.1919\nINFO  [11:53:27.414] epoch 6, loss 0.1792\nINFO  [11:53:28.669] epoch 7, loss 0.1695\nINFO  [11:53:29.919] epoch 8, loss 0.1620\nINFO  [11:53:31.172] epoch 9, loss 0.1558\nINFO  [11:53:32.425] epoch 10, loss 0.1507\n```\n\n\n:::\n\n```{.r .cell-code}\nwv_context <- glove$components\nword_vectors <- wv_main + t(wv_context)\n\nsearch_coord <- \n  word_vectors[\"barnier\", , drop = FALSE]\n\nword_vectors |> \n  sim2(search_coord, method = \"cosine\") |> \n  as_tibble(rownames = NA) |> \n  rownames_to_column(\"term\") |> \n  rename(similarity = 2) |> \n  slice_max(similarity, n = 10)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|term        | similarity|\n|:-----------|----------:|\n|barnier     |  1.0000000|\n|frost       |  0.8212195|\n|michel      |  0.8081586|\n|negotiator  |  0.7688649|\n|brussels    |  0.7139891|\n|negotiators |  0.7022531|\n|eus         |  0.6570144|\n|team        |  0.6551399|\n|tweeted     |  0.6414501|\n|chief       |  0.6102063|\n\n</div>\n:::\n:::\n\n\nWord embedding is a learned modelling technique placing words into a multi-dimensional vector space such that contextually-similar words may be found close by. Not surprisingly, one of the closest words contextually is \"Michel\". And as he is the chief negotiator for the EU, we find \"negotiator\" and \"brussels\" also in the top most contextually-similar words.\n\nThe word embeddings algorithm, through word co-occurrence, has identified the name of Michel Barnier's UK counterpart David Frost. So filtering articles for \"Barnier\", \"Frost\" and \"UK-EU\" should help narrow the focus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontext_df <- \n  trade_df |> \n  filter(str_detect(body, \"barnier|frost|uk-eu\")) \n\ncontext_corp <- \n  context_df |> \n  distinct() |> \n  corpus(docid_field = \"short_url\", text_field = \"body\")\n```\n:::\n\n\nQuanteda's[@quanteda] `kwic` function shows key phrases in context to ensure we're homing in on the required texts. Short URLs are included below so one can click on any to read the actual article as presented by The Guardian.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\ncontext_corp |>\n  tokens(\n    remove_punct = TRUE,\n    remove_symbols = TRUE,\n    remove_numbers = TRUE\n  ) |>\n  kwic(pattern = phrase(c(\"trade negotiation\", \"trade deal\", \"trade talks\")), \n       valuetype = \"regex\", window = 7) |>\n  as_tibble() |>\n  left_join(article_df, by = join_by(docname == short_url)) |> \n  slice_sample(n = 10) |> \n  select(docname, pre, keyword, post, headline)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|docname                             |pre                                            |keyword     |post                                           |headline                                                                   |\n|:-----------------------------------|:----------------------------------------------|:-----------|:----------------------------------------------|:--------------------------------------------------------------------------|\n|https://www.theguardian.com/p/en8ec |government as incompetent failing to secure a  |trade deal  |would give the labour leader another front     |Brexit withdrawal deal: what is No 10 playing at?                          |\n|https://www.theguardian.com/p/dag4n |the uk could not have the same                 |trade deal  |with the eu as canada he said                  |Brexit deal 'a different ball game' to Canada agreement, warns EU          |\n|https://www.theguardian.com/p/en752 |a linkage between subsidy rules in free        |trade deals |with common ground proving difficult to find   |Von der Leyen warns UK against breaking international law over Brexit deal |\n|https://www.theguardian.com/p/fptbj |had to choose between a relatively thin        |trade deal  |and no deal and to no great                    |Brexit talks followed common pattern but barrier-raising outcome is unique |\n|https://www.theguardian.com/p/dpzjp |related whats at stake in britains post-brexit |trade talks |the uk government wants to include eu-derived  |UK to publish draft treaty in effort to reboot Brexit process              |\n|https://www.theguardian.com/p/fmmga |its pretty clear when you do a                 |trade deal  |that you are a sovereign nation the            |EU leaders stress unity as they welcome Brexit trade talks extension       |\n|https://www.theguardian.com/p/f63be |for a no-deal outcome on january the           |trade talks |are over the eu has effectively ended          |What was the point of Johnson's Brexit statement? To save face             |\n|https://www.theguardian.com/p/f7tf4 |by michel barnier broke the impasse in         |trade talks |the fts on that story but naturally            |Thursday briefing: Iran and Russia 'interfere' as Obama lays into Trump    |\n|https://www.theguardian.com/p/fk5kt |companies await news of a potential uk-eu      |trade deal  |abf said our businesses have completed all     |Primark reports 'phenomenal' trading since lockdowns ended                 |\n|https://www.theguardian.com/p/evgxe |in talks trying to thrash out a                |trade deal  |before january but after the chief negotiators |Wednesday briefing: Tory revolt over Cummings piles pressure on PM         |\n\n</div>\n:::\n:::\n\n\nQuanteda provides a sentiment dictionary which, in addition to identifying positive and negative words, also finds negative-negatives and negative-positives such as, for example, \"not effective\". For each week's worth of articles, we can calculate the proportion of positive sentiments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\n\nsent_df <- \n  context_corp |> \n  tokens() |> \n  dfm() |> \n  dfm_lookup(data_dictionary_LSD2015) |> \n  convert(to = \"data.frame\") |>\n  left_join(context_df, by = join_by(doc_id == short_url)) |> \n  mutate(\n    pos = positive + neg_negative,\n    neg = negative + neg_positive,\n    web_date = date_ceiling(as_date(web_publication_date), \"week\"),\n    pct_pos = pos / (pos + neg)\n  )\n\nsent_df |> \n  select(Article = doc_id, \"Pos Score\" = pos, \"Neg Score\" = neg) |> \n  slice(1:10)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|Article                             | Pos Score| Neg Score|\n|:-----------------------------------|---------:|---------:|\n|https://www.theguardian.com/p/d6qhb |        40|        22|\n|https://www.theguardian.com/p/d9e9j |        27|        15|\n|https://www.theguardian.com/p/d6kzd |        51|        27|\n|https://www.theguardian.com/p/d79cn |        56|        48|\n|https://www.theguardian.com/p/d6t3c |        27|        26|\n|https://www.theguardian.com/p/d9vjq |        13|        23|\n|https://www.theguardian.com/p/d7n8b |        54|        34|\n|https://www.theguardian.com/p/d9xtf |        33|        13|\n|https://www.theguardian.com/p/dag4n |        37|        35|\n|https://www.theguardian.com/p/d7d9t |        22|        11|\n\n</div>\n:::\n\n```{.r .cell-code}\nsummary_df <- sent_df |> \n  summarise(pct_pos = mean(pct_pos), \n            n = n(),\n            .by = web_date)\n\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.616 sec elapsed\n```\n\n\n:::\n:::\n\n\nPlotting the changing proportion of positive sentiment over time did surprise me a little. The outcome was more balanced than I expected which perhaps confirms the deeper impression left on me by negative articles.\n\nThe upper plot shows a rolling 7-day mean with a narrowing ribbon representing a narrowing variation in sentiment.\n\nThe lower plot shows the volume of articles. As we drew closer to the crunch-point the volume picked up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwidth <- 7\n\nsent_df2 <- sent_df |>\n  mutate(web_date = as_date(web_publication_date)) |> \n  group_by(web_date) |>\n  summarise(pct_pos = sum(pos) / sum(neg + pos)) |> \n  mutate(\n    roll_mean = slide_dbl(pct_pos, mean, .before = 6),\n    roll_lq = slide_dbl(pct_pos, ~ quantile(.x, probs = 0.25), .before = 6),\n    roll_uq = slide_dbl(pct_pos, ~ quantile(.x, probs = 0.75), .before = 6)\n  )\n\np1 <- sent_df2 |>\n  ggplot(aes(web_date)) +\n  geom_line(aes(y = roll_mean), colour = pal[1]) +\n  geom_ribbon(aes(ymin = roll_lq, ymax = roll_uq), \n              alpha = 0.33, fill = pal[1]) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", \n             colour = pal[4], linewidth = 1) +\n  scale_y_continuous(labels = label_percent(accuracy = 1)) +\n  labs(\n    title = \"Changing Sentiment Towards a UK-EU Trade Deal\",\n    subtitle = glue(\"Rolling {width} days Since the Withdrawal Agreement\"),\n    x = NULL, y = \"Positive Sentiment\"\n  )\n\np2 <- summary_df |> \n  ggplot(aes(web_date, n)) +\n  geom_line(colour = pal[1]) +\n  labs(x = \"Weeks\", y = \"Article Count\",\n       caption = \"Source: Guardian Newspaper\")\n\np1 / p2 + \n  plot_layout(heights = c(2, 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-1.png){width=100%}\n:::\n:::\n\n\n## R Toolbox\n\nSummarising below the packages and functions used in this post enables me to separately create a [toolbox visualisation](/project/box) summarising the usage of packages and functions across all posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nused_here()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"usedthese table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Package </th>\n   <th style=\"text-align:left;\"> Function </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Matrix </td>\n   <td style=\"text-align:left;\"> t[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> base </td>\n   <td style=\"text-align:left;\"> c[3], library[15], mean[1], set.seed[2], sum[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> clock </td>\n   <td style=\"text-align:left;\"> add_days[1], add_months[1], date_build[1], date_ceiling[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> conflicted </td>\n   <td style=\"text-align:left;\"> conflict_prefer_all[1], conflict_scout[1], conflicts_prefer[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> dplyr </td>\n   <td style=\"text-align:left;\"> distinct[1], filter[2], group_by[1], join_by[2], left_join[2], mutate[5], n[1], rename[1], select[2], slice[1], slice_max[1], slice_sample[1], summarise[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ggfoundry </td>\n   <td style=\"text-align:left;\"> display_palette[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ggplot2 </td>\n   <td style=\"text-align:left;\"> aes[4], geom_hline[1], geom_line[2], geom_ribbon[1], ggplot[2], labs[2], scale_y_continuous[1], theme_bw[1], theme_set[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> glue </td>\n   <td style=\"text-align:left;\"> glue[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> lubridate </td>\n   <td style=\"text-align:left;\"> as_date[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> methods </td>\n   <td style=\"text-align:left;\"> new[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mlapi </td>\n   <td style=\"text-align:left;\"> fit_transform[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> paletteer </td>\n   <td style=\"text-align:left;\"> paletteer_d[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> patchwork </td>\n   <td style=\"text-align:left;\"> plot_layout[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> purrr </td>\n   <td style=\"text-align:left;\"> list_rbind[1], pmap[1], slowly[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> quanteda </td>\n   <td style=\"text-align:left;\"> convert[1], corpus[2], dfm[1], dfm_lookup[1], fcm[1], kwic[1], phrase[1], tokens[3] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> scales </td>\n   <td style=\"text-align:left;\"> label_percent[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> slider </td>\n   <td style=\"text-align:left;\"> slide_dbl[3] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stats </td>\n   <td style=\"text-align:left;\"> quantile[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stringr </td>\n   <td style=\"text-align:left;\"> str_detect[2], str_remove_all[3], str_to_lower[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> text2vec </td>\n   <td style=\"text-align:left;\"> sim2[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tibble </td>\n   <td style=\"text-align:left;\"> as_tibble[2], rownames_to_column[1], tibble[1] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> tictoc </td>\n   <td style=\"text-align:left;\"> tic[2], toc[2] </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> usedthese </td>\n   <td style=\"text-align:left;\"> used_here[1] </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}