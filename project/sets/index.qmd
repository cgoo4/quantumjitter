---
title: "Where Clouds Cross"
date: "2017-11-14"
categories: [R, sets, web scraping, regex]
description: "Visualising the dozens of overlapping sets formed by categories of cloud services"
bibliography: references.bib
---

![](feature.gif){fig-alt="A silhouette of the Houses of Parliament with the text \"G9\" instead of Big Ben's clock. Three semi-transparent clouds move across the sky mimicing a Venn diagram."}

When visualising a small number of overlapping sets, [Venn diagrams](https://en.wikipedia.org/wiki/Venn_diagram) work well. But what if there are more. Here's a [tidyverse](https://www.tidyverse.org) approach to the exploration of sets and their intersections.

In [Let's Jitter](/project/jitter) I looked at a relatively simple set of cloud-service-related sales data. [G-Cloud data](https://www.digitalmarketplace.service.gov.uk/g-cloud/search) offers a much richer source with many thousands of services documented by several thousand suppliers and hosted across myriad web pages. These services straddle many categories. I'll use these data to explore the sets and where they cross.

```{r}
#| label: libraries

library(conflicted)
library(tidyverse)
conflict_prefer_all("dplyr", quiet = TRUE)
conflicts_prefer(tidyr::unite)
library(rvest)
library(furrr)
library(wesanderson)
library(tictoc)
library(ggupset)
library(ggVennDiagram)
library(glue)
library(usedthese)

conflict_scout()

plan(multisession, workers = 10)
```

```{r}
#| label: theme
#| fig-height: 2
#| dev.args: { bg: "transparent" }

theme_set(theme_bw())

(cols <- wes_palette("Royal2"))
```

I'm going to focus on the Cloud Hosting lot. Suppliers document the services they want to offer to Public Sector buyers. Each supplier is free to assign each of their services to one or more service categories. It would be interesting to see how these categories overlap when looking at the aggregated data.

I'll begin by harvesting the URL for each category's search results. And I'll also capture the number of search pages for each category. This will enable me to later control how R iterates through the web pages to extract the required data.

```{r}
#| label: categories

path <- 
  str_c("https://www.applytosupply.digitalmarketplace", 
        ".service.gov.uk/g-cloud/search?lot=cloud-")

lot_urls <-
  c(
    str_c(path, "hosting"),
    str_c(path, "software"),
    str_c(path, "support")
  )

cat_urls <- future_map(lot_urls, \(x) {
  nodes <- x |>
    read_html() |>
    html_elements(".app-lot-filter__last-list li a")

  tibble(
    url = nodes |>
      html_attr("href"),

    pages = nodes |>
      html_text()
  )
}) |>
  list_rbind() |> 
  mutate(
    pages = parse_number(as.character(pages)),
    pages = if_else(pages %% 30 > 0, pages %/% 30 + 1, pages %/% 30),
    lot = str_extract(url, "(?<=cloud-)[\\w]+"),
    url = str_remove(url, ".*(?=&)")
  )

version <- lot_urls[[1]] |> 
  read_html() |> 
  html_elements(".app-search-result:first-child") |> 
  html_text() |> 
  str_extract("G-Cloud \\d\\d")
```

So now I'm all set to parallel process through the data at two levels. At category level. And within each category, I'll iterate through the multiple pages of search results, harvesting 100 service IDs per page.

I'll also auto-abbreviate the category names so I'll have the option of more concise names for less-cluttered plotting later on.

::: callout-tip
Hover over the numbered code annotation symbols for REGEX explanations.
:::

```{r}
#| label: service ids
#| cache: true

tic()

data_df <-
  pmap(cat_urls, \(url, pages, lot) {
    
    cat("\n", url, " | ", pages, " | ", lot)
      
    future_map(1:pages, possibly(\(pages) {
        refs <- str_c(
          "https://www.applytosupply.digitalmarketplace", 
          ".service.gov.uk/g-cloud/search?page=",
          pages, url, "&lot=cloud-", lot
          ) |>
          read_html() |>
          html_elements("#js-dm-live-search-results .govuk-link") |>
          html_attr("href") 
    
        tibble(
            lot = str_c("Cloud ", str_to_title(lot)),
            service_id = str_extract(refs, "[[:digit:]]{15}"), # <1>
            cat = str_remove(url, "&serviceCategories=") |>
              str_replace_all("\\Q+\\E", " ") |> # <2>
              str_remove("%28[[:print:]]+%29")
          )
      }), .progress = TRUE) |> bind_rows()
    }
  ) |> 
  bind_rows() |>
  select(lot:cat) |>
  mutate(
    cat = str_trim(cat) |> str_to_title(),
    abbr = str_remove(cat, "and") |> abbreviate(3) |> str_to_upper()
  ) |> 
  drop_na(service_id)

toc()
```

1.  `[[:digit:]]{15}` finds the 15-digit service ID in the scraped link.
2.  `\\Q+\\E` finds the *literal* `+` character rather than interpreting it as a *one or more* quantifier (i.e. if the `\\Q` and `\\E` were not specified)

Now that I have a nice tidy [tibble](https://tibble.tidyverse.org) [@tibble], I can start to think about visualisations.

I like Venn diagrams. But to create one I'll first need to do a little prep as ggVennDiagram [@ggVennDiagram] requires separate character vectors for each set.

```{r}
#| label: vectors

host_df <- data_df |>
  filter(lot == "Cloud Hosting") |>
  group_by(abbr)

keys <- host_df |> 
  group_keys() |> 
  pull(abbr)

all_cats <- host_df |> 
  group_split() |>
  map("service_id") |> 
  set_names(keys)
```

Venn diagrams work best with a small number of sets. So we'll select four categories.

```{r}
#| label: venn

four_cats <- all_cats[c("CAAH", "PAAS", "OBS", "IND")]

four_cats |> 
  ggVennDiagram(label = "count", label_alpha = 0) +
  scale_fill_gradient(low = cols[5], high = cols[3]) +
  scale_colour_manual(values = cols[c(rep(4, 4))]) +
  labs(
    x = "Category Combinations", y = NULL, fill = "# Services",
    title = "The Most Frequent Category Combinations",
    subtitle = glue("Focusing on Four {version} Service Categories"),
    caption = "Source: digitalmarketplace.service.gov.uk\n"
  )
```

Let's suppose I want to find out which Service IDs lie in a particular intersection. Perhaps I want to go back to the web site with those IDs to search for, and read up on, those particular services. I could use purrr's `reduce` to achieve this. For example, let's extract the IDs at the heart of the Venn which intersect all categories.

```{r}
#| label: four cats

four_cats |> reduce(intersect)
```

And if we wanted the IDs intersecting the "OBS" and "IND" categories?

```{r}
#| label: obs-ind

list(
  four_cats$OBS,
  four_cats$IND
) |>
  reduce(intersect)
```

Sometimes though we need something a little more scalable than a Venn diagram. The ggupset package provides a good solution. Before we try more than four sets though, I'll first use the same four categories so we may compare the visualisation to the Venn.

```{r}
#| label: frequent

set_df <- data_df |>
  filter(abbr %in% c("CAAH", "PAAS", "OBS", "IND")) |>
  mutate(category = list(cat), .by = service_id) |>
  distinct(service_id, category) |>
  mutate(n = n(), .by = category)

set_df |>
  ggplot(aes(category)) +
  geom_bar(fill = cols[1]) +
  geom_label(aes(y = n, label = n), vjust = -0.1, size = 3, fill = cols[5]) +
  scale_x_upset() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +
  theme(panel.border = element_blank()) +
  labs(
    x = "Category Combinations", y = NULL,
    title = "The Most Frequent Category Combinations",
    subtitle = glue("Focusing on Four {version} Service Categories"),
    caption = "Source: digitalmarketplace.service.gov.uk"
  )
```

Now let's take a look at the intersections across all the categories. And let's suppose that our particular interest is all services which appear in one, and only one, category.

```{r}
#| label: singles
#| warning: false
#| message: false

set_df <- data_df |>
  filter(n() == 1, lot == "Cloud Hosting", .by = service_id) |>
  mutate(category = list(cat), .by = service_id) |>
  distinct(service_id, category) |>
  mutate(n = n(), .by = category)

set_df |>
  ggplot(aes(category)) +
  geom_bar(fill = cols[2]) +
  geom_label(aes(y = n, label = n), vjust = -0.1, size = 3, fill = cols[3]) +
  scale_x_upset(n_sets = 10) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +
  theme(panel.border = element_blank()) +
  labs(
    x = "Category Combinations", y = NULL,
    title = "10 Most Frequent Single-Category Services",
    subtitle = "Focused on Service Categories in the Cloud Hosting Lot",
    caption = "Source: digitalmarketplace.service.gov.uk"
  )
```

Suppose we want to extract the intersection data for the top intersections across all sets. I could use functions from the tidyr package to achieve this.

```{r}
#| label: intersection data

cat_mix <- data_df |>
  filter(lot == "Cloud Hosting") |>
  mutate(x = cat) |>
  pivot_wider(service_id, names_from = cat, values_from = x, values_fill = "^") |>
  unite(col = intersect, -service_id, sep = "/") |>
  count(intersect) |>
  mutate(
    intersect = str_replace_all(intersect, "(?:\\Q/^\\E|\\Q^/\\E)", ""),
    intersect = str_replace_all(intersect, "/", " | ")
  ) |>
  arrange(desc(n)) |>
  slice(1:21)

cat_mix |>
  rename(
    "Intersecting Categories" = intersect,
    "Services Count" = n
  )
```

And I can compare this table to the equivalent ggupset [@ggupset]visualisation.

```{r}
#| label: upset top
#| fig-height: 7
#| warning: false

set_df <- data_df |>
  filter(lot == "Cloud Hosting") |>
  mutate(category = list(cat), .by = service_id) |>
  distinct(service_id, category) |>
  mutate(n = n(), .by = category)

set_df |>
  ggplot(aes(category)) +
  geom_bar(fill = cols[5]) +
  geom_label(aes(y = n, label = n), vjust = -0.1, size = 3, fill = cols[4]) +
  scale_x_upset(n_sets = 22, n_intersections = 21) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.15))) +
  theme(panel.border = element_blank()) +
  labs(
    x = "Category Combinations", y = NULL,
    title = "Top Intersections Across all Sets",
    subtitle = "Focused on Service Categories in the Cloud Hosting Lot",
    caption = "Source: digitalmarketplace.service.gov.uk"
  )
```

And if I want to extract all the service IDs for the top 5 intersections, I could use dplyr [@dplyr] and tidyr [@tidyr] verbs to achieve this too.

I won't print them all out though!

```{r}
#| label: top 5

top5_int <- data_df |>
  filter(lot == "Cloud Hosting") |>
  select(service_id, abbr) |>
  mutate(x = abbr) |>
  pivot_wider(names_from = abbr, values_from = x, values_fill = "^") |>
  unite(col = intersect, -service_id, sep = "/") |>
  mutate(
    intersect = str_replace_all(intersect, "(?:\\Q/^\\E|\\Q^/\\E)", ""),
    intersect = str_replace(intersect, "/", " | ")
  ) |>
  mutate(count = n_distinct(service_id), .by = intersect) |>
  arrange(desc(count), intersect, service_id) |>
  add_count(intersect, wt = count, name = "temp") |>
  mutate(temp = dense_rank(desc(temp))) |>
  filter(temp %in% 1:5) |>
  distinct(service_id)

top5_int |>
  summarise(service_ids = n_distinct(service_id))
```

## R Toolbox

Summarising below the packages and functions used in this post enables me to separately create a [toolbox visualisation](/project/box) summarising the usage of packages and functions across all posts.

```{r}
#| label: toolbox

used_here()
```
